<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLM | noexcs</title><meta name="author" content="noexcs"><meta name="copyright" content="noexcs"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="论文阅读总结[TOC] Attention Is All You Need.《Attention Is All You Need》是一篇在自然语言处理领域具有里程碑意义的论文，由Google的研究人员在2017年提出。这篇论文的主要贡献是引入了Transformer模型，这是一种基于注意力机制的深度学习架构，它在多个任务上取得了显著的性能提升。 主要内容 注意力机制：论文强调了注意力机制在序列建模">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM">
<meta property="og:url" content="http://noexcs.github.io/llm/index.html">
<meta property="og:site_name" content="noexcs">
<meta property="og:description" content="论文阅读总结[TOC] Attention Is All You Need.《Attention Is All You Need》是一篇在自然语言处理领域具有里程碑意义的论文，由Google的研究人员在2017年提出。这篇论文的主要贡献是引入了Transformer模型，这是一种基于注意力机制的深度学习架构，它在多个任务上取得了显著的性能提升。 主要内容 注意力机制：论文强调了注意力机制在序列建模">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://avatars.githubusercontent.com/u/56532264?v=4">
<meta property="article:published_time" content="2024-11-22T05:11:57.000Z">
<meta property="article:modified_time" content="2024-11-22T08:51:12.407Z">
<meta property="article:author" content="noexcs">
<meta property="article:tag" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://avatars.githubusercontent.com/u/56532264?v=4"><link rel="shortcut icon" href="https://avatars.githubusercontent.com/u/56532264?v=4"><link rel="canonical" href="http://noexcs.github.io/llm/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-color: #DBEBEE;"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://avatars.githubusercontent.com/u/56532264?v=4" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">1</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">noexcs</span></a><a class="nav-page-title" href="/"><span class="site-name">LLM</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">LLM</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-categories"><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/default/">default</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h1 id="论文阅读总结"><a href="#论文阅读总结" class="headerlink" title="论文阅读总结"></a>论文阅读总结</h1><p>[TOC]</p>
<h2 id="Attention-Is-All-You-Need"><a href="#Attention-Is-All-You-Need" class="headerlink" title="Attention Is All You Need."></a>Attention Is All You Need.</h2><p>《Attention Is All You Need》是一篇在自然语言处理领域具有里程碑意义的论文，由Google的研究人员在2017年提出。这篇论文的主要贡献是引入了Transformer模型，这是一种基于注意力机制的深度学习架构，它在多个任务上取得了显著的性能提升。</p>
<h3 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h3><ol>
<li><p><strong>注意力机制</strong>：论文强调了注意力机制在序列建模中的重要性。传统的序列模型，如循环神经网络（RNN）和长短期记忆网络（LSTM），在处理长序列时会遇到梯度消失或梯度爆炸的问题。注意力机制允许模型在处理序列时动态地聚焦于重要的部分，从而提高了模型的性能。</p>
</li>
<li><p><strong>Transformer架构</strong>：Transformer模型完全基于注意力机制，摒弃了传统的循环结构。它由编码器和解码器组成，每个编码器和解码器都由多个相同的层堆叠而成。每层包含两个主要部分：多头自注意力机制（Multi-Head Self-Attention）和前馈神经网络（Feed-Forward Neural Network）。</p>
</li>
<li><p><strong>多头自注意力</strong>：这是Transformer的核心创新之一。多头自注意力机制允许模型在不同的表示子空间中同时学习信息，从而更好地捕捉序列中的复杂关系。</p>
</li>
<li><p><strong>位置编码</strong>：由于Transformer模型没有循环结构，它需要一种方法来引入序列中元素的位置信息。论文中提出了使用位置编码的方法，将位置信息嵌入到输入序列中。</p>
</li>
</ol>
<h3 id="影响和应用"><a href="#影响和应用" class="headerlink" title="影响和应用"></a>影响和应用</h3><p>Transformer模型的提出对自然语言处理领域产生了深远的影响。它不仅在机器翻译任务上取得了当时的最佳性能，还为后续的许多模型和应用奠定了基础，包括BERT、GPT系列等预训练语言模型。这些模型在各种自然语言处理任务中都取得了显著的成果，如文本分类、问答系统、文本生成等。</p>
<p><img src="/.%5Cllm%5Ctransformer.png" alt="attention structure"></p>
<h3 id="其他文章："><a href="#其他文章：" class="headerlink" title="其他文章："></a>其他文章：</h3><ol>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a">Illustrated: Self-Attention. A step-by-step guide to self-attention… | by Raimi Karim | Towards Data Science</a></li>
</ol>
<h3 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h3><ol>
<li>为什么要除以根号d</li>
<li>为什么要用layer norm不用batch normlization</li>
</ol>
<h2 id="PagedAttention"><a href="#PagedAttention" class="headerlink" title="PagedAttention"></a>PagedAttention</h2><p>PagedAttention 是一种专门针对大型语言模型（LLMs）设计的优化技术，旨在改善内存管理和提高推理速度。这项技术受到了操作系统中虚拟内存和分页技术的启发，旨在解决大规模语言模型在推理过程中面临的内存管理和计算效率问题。</p>
<h3 id="PagedAttention-的定义"><a href="#PagedAttention-的定义" class="headerlink" title="PagedAttention 的定义"></a>PagedAttention 的定义</h3><p>PagedAttention 通过将模型的键值缓存（KV cache）分块（blocks）来优化内存管理，每块包含一定数量的token对应的键（key）和值（value）张量。这种方法类似于操作系统中对物理内存的分页管理，它允许在非连续的内存空间中存储连续的键和值张量。通过这种方式，PagedAttention 能够更灵活地管理内存资源，减少内存浪费，并实现KV缓存的跨请求共享。</p>
<h3 id="PagedAttention-的功能"><a href="#PagedAttention-的功能" class="headerlink" title="PagedAttention 的功能"></a>PagedAttention 的功能</h3><ol>
<li><strong>内存管理优化</strong>：PagedAttention 将每个序列的KV缓存分成多个块，这些块在内存中不需要连续存放，这使得内存分配更加灵活，能够有效减少内存碎片化问题。</li>
<li><strong>高效计算</strong>：PagedAttention 通过分页的方式处理大规模数据，减少计算量和推理时间，从而提高模型的处理速度。</li>
<li><strong>并发处理</strong>：通过“先来先服务（FCFS），后来先抢占，GPU不够就先swap到CPU上”的调度策略，PagedAttention 能够在一个推理阶段处理尽可能多的请求，解决高并发场景下的推理吞吐问题。</li>
<li><strong>CUDA 优化</strong>：PagedAttention 的实现充分利用了CUDA的并行计算能力，特别是在GPU上实现了高效的矩阵运算和注意力计算，进一步提升了模型的性能。</li>
</ol>
<h3 id="PagedAttention-的应用场景"><a href="#PagedAttention-的应用场景" class="headerlink" title="PagedAttention 的应用场景"></a>PagedAttention 的应用场景</h3><p>PagedAttention 主要应用于大规模语言模型的推理服务中，尤其适用于需要处理大量文本数据、高并发请求的应用场景。它不仅提高了推理速度，还降低了部署和运行的成本。例如，在基于LLM的应用中，如通用聊天机器人、代码生成器等领域，PagedAttention 的引入可以帮助提升用户体验，加快响应速度。</p>
<h3 id="PagedAttention-的技术细节"><a href="#PagedAttention-的技术细节" class="headerlink" title="PagedAttention 的技术细节"></a>PagedAttention 的技术细节</h3><p>在PagedAttention 中，不同序列可以通过将它们的逻辑块映射到同一个物理块的方式来共享块。为了确保安全共享，PagedAttention 实现了写时复制（Copy-on-Write）机制，并对物理块的引用计数进行跟踪。此外，PagedAttention 的实现还涉及到对CUDA&#x2F;HIP图的支持，以便快速执行模型，并支持多种解码算法。</p>
<p>综上所述，PagedAttention 是一种创新的注意力算法，通过高效管理内存和优化计算，极大地提升了大规模语言模型的性能和实用性。它代表了当前LLM推理优化领域的前沿进展之一。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>PagedAttention是一种创新的注意力机制，通过分页机制解决了传统注意力机制在处理长序列时的内存和计算效率问题。它具有内存效率高、计算效率高、灵活性强等特点，特别适用于需要处理长序列数据的NLP任务。通过PagedAttention，模型能够在有限的硬件资源下处理更长的序列，从而提高了模型的适用性和性能。</p>
<h2 id="GroupAttention"><a href="#GroupAttention" class="headerlink" title="GroupAttention"></a>GroupAttention</h2><h2 id="FlashAttention"><a href="#FlashAttention" class="headerlink" title="FlashAttention"></a>FlashAttention</h2><p>“FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”是由斯坦福大学和Google的研究人员共同发表的一篇论文。这篇论文提出了一种新的注意力机制，名为FlashAttention，旨在解决传统注意力机制在计算和内存使用上的效率问题。FlashAttention通过引入IO（输入&#x2F;输出）感知的优化策略，实现了快速且内存高效的注意力计算。</p>
<h3 id="主要内容-1"><a href="#主要内容-1" class="headerlink" title="主要内容"></a>主要内容</h3><ol>
<li><strong>IO感知优化</strong>：FlashAttention的核心创新在于其IO感知的优化策略。传统的注意力机制在计算过程中需要大量的内存和计算资源，尤其是在处理长序列时。FlashAttention通过优化数据在内存中的存取方式，减少了不必要的IO操作，从而提高了计算效率。</li>
<li><strong>内存效率</strong>：FlashAttention通过分块（chunking）和重用中间结果等技术，显著减少了内存使用量，使得模型能够在更有限的内存资源下处理更长的序列。</li>
<li><strong>计算效率</strong>：通过减少IO操作和内存访问的次数，FlashAttention在保持计算精度的同时，大幅提升了计算速度，使得注意力机制的计算更加高效。</li>
</ol>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>论文的结论是，FlashAttention作为一种新的注意力机制，通过IO感知的优化策略，实现了快速且内存高效的注意力计算。这种方法不仅提高了计算效率，还降低了内存使用量，使得模型能够在更广泛的硬件资源下运行，特别是在处理长序列时表现出色。</p>
<h3 id="影响和应用-1"><a href="#影响和应用-1" class="headerlink" title="影响和应用"></a>影响和应用</h3><p>FlashAttention的提出对自然语言处理和深度学习领域产生了重要影响，特别是在以下几个方面：</p>
<ol>
<li><strong>模型训练</strong>：FlashAttention使得大型模型在训练过程中能够更高效地使用内存和计算资源，加速了模型的训练过程。</li>
<li><strong>长序列处理</strong>：在处理长序列数据时，FlashAttention能够显著减少内存使用量，使得模型能够处理更长的文本或其他序列数据。</li>
<li><strong>硬件适应性</strong>：FlashAttention的优化策略使得模型能够更好地适应不同类型的硬件资源，包括内存受限的设备。</li>
</ol>
<p>总的来说，“FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”这篇论文提出了一种创新的注意力机制，通过IO感知的优化策略，显著提高了注意力计算的效率和内存使用效率。这一方法为解决深度学习模型在处理长序列数据时的计算和内存瓶颈问题提供了新的解决方案，推动了自然语言处理和深度学习技术的发展。</p>
<h2 id="Scaling-Down-to-Scale-Up-A-Guide-to-Parameter-Efficient-Fine-Tuning"><a href="#Scaling-Down-to-Scale-Up-A-Guide-to-Parameter-Efficient-Fine-Tuning" class="headerlink" title="Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning"></a>Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning</h2><p>本文系统地概述了40多篇论文中参数高效微调方法的比较2019年2月至2023年2月。这些方法旨在重新解决仅通过以下方式微调大型语言模型训练一小部分参数。我们提供了一个涵盖广泛的分类提出了一种详细的方法与现实生活中的特定关注点进行比较效率和微调数十亿规模语言模型。</p>
<h3 id="主要内容-2"><a href="#主要内容-2" class="headerlink" title="主要内容"></a>主要内容</h3><ol>
<li><p>给现在的各种微调方法分成了三大类：</p>
<ol>
<li><strong>Addition-based</strong></li>
<li><strong>Selection-based</strong></li>
<li><strong>Reparameterization-bsaed</strong></li>
</ol>
</li>
</ol>
<h3 id="Additive-methods"><a href="#Additive-methods" class="headerlink" title="Additive methods"></a><img src="/.%5Cllm%5Cimage-20240725223537925.png" alt="image-20240725223537925"><strong>Additive methods</strong></h3><p>加性方法背后的主要思想是用额外的方法来增强现有的预训练模型参数或层，只训练新的添加参数。截至目前，这是最大的并广泛探索了PEFT方法的范畴。在这个类别中，有两个大子类别出现了：类似适配器的方法和软提示。</p>
<ol>
<li><p>Adapter:</p>
<p>Adapters (Houlsby et al., 2019) are a type of additive parameter-efficient fine-tuning method that involves <strong>introducing small fully connected networks after Transformer sub-layers.</strong></p>
<ul>
<li><p>就是在一个Transformer层的子层后添加一个小的全连接层。因此在推理开销上一般都会增加。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transformer_block_with_adapter</span>(<span class="params">x</span>):</span><br><span class="line">	residual = x</span><br><span class="line">	x = SelfAttention(x)</span><br><span class="line">	x = FFN(x) <span class="comment"># adapter</span></span><br><span class="line">	x = LN(x + residual)</span><br><span class="line">	residual = x</span><br><span class="line">	x = FFN(x) <span class="comment"># transformer FFN</span></span><br><span class="line">	x = FFN(x) <span class="comment"># adapter</span></span><br><span class="line">	x = LN(x + residual)</span><br><span class="line">	<span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
</li>
<li><p>还有一种AdaMix：AdaMix (Wang et al., 2022) improves the performance of adapters by <strong>utilizing multiple adapters in a mixture-of-experts (MoE) fashion</strong></p>
</li>
</ul>
</li>
<li><p>Soft Prompts：</p>
<p>Language model prompting aims to control the behavior of a language model by modifying the input text, which typically consists of a task description accompanied by a few in-context examples. However, these methods are difficult to optimize and are inherently limited in the number of training examples by the maximum model input length. To address these drawbacks, the concept of “soft” prompts was introduced, where a part of the model’s input embeddings is fine-tuned via gradient descent. <strong>This pivots the problem of finding prompts in a discrete space to a continuous optimization problem.</strong> Soft prompts can be trained for the input layer only or for all layers. Recent advancements explore how soft prompts could be pre-trained or prompts for different tasks utilized to reduce the computation required for fine-tuning a soft prompt for a new task.</p>
<ul>
<li><p>Prompt Tuning：</p>
<p>Prompt tuning (Lester et al., 2021) proposes to <strong>prepend the model input embeddings with a trainable tensor P</strong> ∈ R^l×h . This tensor is commonly referred to as “soft prompt” and it is optimized directly through gradient descent</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">soft_prompted_model</span>(<span class="params">input_ids</span>):</span><br><span class="line">	x = Embed(input_ids)</span><br><span class="line">	x = concat([soft_prompt, x], dim=seq)</span><br><span class="line">	<span class="keyword">return</span> model(x)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Prefix-Tuning：</p>
<p>Li and Liang (2021) independently develop the idea of soft prompts with a distinctive flavor: instead of adding a soft prompt to the model input, trainable parameters are prepended to the hidden states of all layers. The same prefix Pθ ∈ Rl×h is prepended to all of the hidden states.</p>
<p>They <strong>observe that directly optimizing the soft prompt leads to instabilities during training</strong>. Instead, soft prompts are <strong>parametrized through a feed-forward network Pθ &#x3D; FFN(Pˆ θ)</strong>. During training, <strong>Pˆ θ and the parameters of the FFN are optimized</strong>. After, <strong>only Pθ is needed for inference, and the FFN can be discarded</strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transformer_block_for_prefix_tuning</span>(<span class="params">x</span>):</span><br><span class="line">	soft_prompt = FFN(soft_prompt)</span><br><span class="line">	x = concat([soft_prompt, x], dim=seq)</span><br><span class="line">	<span class="keyword">return</span> transformer_block(x)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>other:</p>
<p>Additive methods are a diverse category of parameter-efficient finetuning techniques that extends beyond adapters and soft prompts. For example, LeTS (Fu et al., 2021), LST (Sung et al., 2022), and (IA)3 (Liu et al., 2022) introduce novel ways to add parameters that improve adapters or soft prompts in terms of memory, computation, or accuracy.</p>
<ul>
<li><p>（IA)3：</p>
<p>Liu et al. (2022) propose a new parameter-efficient method to multi-task fine-tune T-few. (IA)3 learns new parameters lv, lk, lff which rescale key, value, and hidden FFN activations. Specifically：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transformer_block_with_ia3</span>(<span class="params">x</span>):</span><br><span class="line">	residual = x</span><br><span class="line">	x = ia3_self_attention(x)</span><br><span class="line">	x = LN(x + residual)</span><br><span class="line">	residual = x</span><br><span class="line">	x = x @ W_1 <span class="comment"># FFN in</span></span><br><span class="line">	x = l_ff * gelu(x) <span class="comment"># (IA)3 scaling</span></span><br><span class="line">	x = x @ W_2 <span class="comment"># FFN out</span></span><br><span class="line">	x = LN(x + residual)</span><br><span class="line">	<span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ia3_self_attention</span>(<span class="params">x</span>):</span><br><span class="line">	k, q, v = x @ W_k, x @ W_q, x @ W_v</span><br><span class="line">	k = l_k * k</span><br><span class="line">	v = l_v * v</span><br><span class="line">	<span class="keyword">return</span> softmax(q @ k.T) @ V</span><br></pre></td></tr></table></figure>

<p>Training only these three vectors lv, lk, lff for each transformer block leads to high parameter efficiency. For T0-3B, it only updates about 0.02% of model parameters and outperforms other methods, including Compacter (Section 11.4) with similar parameter count and LoRa (Section 10.2) with 16 times more trainable parameters. Unlike adapters-tuned models, (IA)3 -tuned models <strong>exhibit minimal overhead</strong>. Vectors lv and lk can be integrated into the corresponding linear layers, and the only overhead comes from lff. </p>
</li>
<li></li>
</ul>
</li>
</ol>
<h3 id="Selective-methods"><a href="#Selective-methods" class="headerlink" title="Selective methods"></a><strong>Selective methods</strong></h3><p>（没完全看懂这段）：可以说是选择性PEFT的最早例子仅对网络的几个顶层进行微调。现代方法是通常基于层的类型或内部结构，如仅训练模型偏差或仅特定行。选择性方法的极端版本是稀疏更新方法可以完全改变模型的结构，并单独选择参数。然而，稀疏的参数更新带来了多种工程和效率挑战，其中一些挑战在最近的参数重构研究中得到了解决和NxM稀疏性。然而，非受限的非结构化的稀疏性仍然对于现代硬件来说不切实际。</p>
<h3 id="Reparametrization-based-methods"><a href="#Reparametrization-based-methods" class="headerlink" title="Reparametrization-based methods"></a><strong>Reparametrization-based methods</strong></h3><p>基于重新参数化的参数高效微调方法利用低秩表示以最小化可训练参数的数量。神经网络具有低维表示的概念在经验和理论分析中得到了广泛的探索深度学习。</p>
<ul>
<li>LoRA</li>
</ul>
<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p>BERT（Bidirectional Encoder Representations from Transformers）是由Google AI在2018年提出的一篇论文，标题为《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》。BERT模型在自然语言处理（NLP）领域引起了巨大的关注，因为它在多个NLP任务上取得了突破性的性能提升。</p>
<h3 id="主要内容-3"><a href="#主要内容-3" class="headerlink" title="主要内容"></a>主要内容</h3><ol>
<li><p><strong>预训练和微调框架</strong>：BERT的核心思想是通过大规模文本数据进行预训练，学习语言的通用表示，然后在特定任务上进行微调。这种两阶段的框架使得BERT能够适应各种不同的NLP任务。</p>
</li>
<li><p><strong>双向Transformer编码器</strong>：BERT采用了Transformer模型的编码器部分，通过双向上下文来学习单词的表示。这与之前的模型（如ELMo和GPT）不同，它们通常只考虑单向上下文（左到右或右到左）。</p>
</li>
<li><p><strong>预训练任务</strong>：BERT使用了两个预训练任务来学习语言表示：</p>
<ul>
<li><strong>掩码语言模型（Masked Language Model, MLM）</strong>：随机遮蔽输入序列中的一些单词，然后让模型预测这些被遮蔽的单词。这迫使模型学习单词的上下文表示。</li>
<li><strong>下一句预测（Next Sentence Prediction, NSP）</strong>：给定两个句子，模型需要判断第二个句子是否是第一个句子的下一句。这有助于模型理解句子之间的关系。</li>
</ul>
</li>
<li><p><strong>微调</strong>：在预训练完成后，BERT可以通过在特定任务上进行微调来适应不同的NLP任务，如文本分类、问答、命名实体识别等。微调通常只需要在预训练模型的基础上添加一个任务特定的输出层，并使用较少的任务特定数据进行训练。</p>
</li>
</ol>
<h3 id="影响和应用-2"><a href="#影响和应用-2" class="headerlink" title="影响和应用"></a>影响和应用</h3><p>BERT的提出对NLP领域产生了深远的影响。它不仅在多个基准测试上取得了当时的最佳性能，还为后续的许多模型和应用奠定了基础，如RoBERTa、ALBERT、DistilBERT等。这些模型在各种NLP任务中都取得了显著的成果，推动了NLP技术的广泛应用。</p>
<h3 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h3><p>《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》这篇论文通过引入BERT模型，展示了预训练和微调框架在NLP任务中的强大能力。这一创新不仅推动了NLP领域的发展，也为其他领域的序列建模问题提供了新的思路和方法。</p>
<h2 id="BART"><a href="#BART" class="headerlink" title="BART"></a>BART</h2><h2 id="ViT"><a href="#ViT" class="headerlink" title="ViT"></a>ViT</h2><p>Vision Transformer（ViT）是一种基于Transformer架构的图像处理模型，由Alexey Dosovitskiy等人于2020年在论文《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》中提出。ViT模型将Transformer的注意力机制应用于图像识别任务，取得了与传统卷积神经网络（CNN）相媲美的性能。</p>
<h3 id="主要内容-4"><a href="#主要内容-4" class="headerlink" title="主要内容"></a>主要内容</h3><ol>
<li><p><strong>图像分块和线性嵌入</strong>：ViT的核心思想是将图像分割成固定大小的块（例如16x16像素），然后将每个块展平成一维向量，并通过线性嵌入层将其转换为序列数据。这样，图像就被转换成了类似于自然语言处理中的单词序列。</p>
</li>
<li><p><strong>位置编码</strong>：由于Transformer模型没有循环结构，它需要一种方法来引入序列中元素的位置信息。ViT在输入序列中添加了位置编码，以保留图像块的空间信息。</p>
</li>
<li><p><strong>Transformer编码器</strong>：ViT使用了标准的Transformer编码器结构，包括多头自注意力机制和前馈神经网络。这些编码器层堆叠在一起，以学习图像块之间的复杂关系。</p>
</li>
<li><p><strong>分类头</strong>：在Transformer编码器的输出上，ViT添加了一个简单的分类头，用于图像识别任务。分类头通常是一个线性层，后面跟着一个softmax函数，用于输出类别概率。</p>
</li>
</ol>
<h3 id="影响和应用-3"><a href="#影响和应用-3" class="headerlink" title="影响和应用"></a>影响和应用</h3><p>ViT的提出对计算机视觉领域产生了深远的影响。它展示了Transformer架构在图像识别任务中的强大能力，尤其是在大规模数据集上进行预训练时。ViT模型在多个图像识别基准测试上取得了与传统CNN相媲美的性能，甚至在某些情况下超过了CNN。</p>
<h3 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h3><p>《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》这篇论文通过引入Vision Transformer模型，展示了Transformer架构在图像识别任务中的应用潜力。这一创新不仅推动了计算机视觉领域的发展，也为其他领域的序列建模问题提供了新的思路和方法。</p>
<h2 id="A-Survey-of-Large-Language-Models"><a href="#A-Survey-of-Large-Language-Models" class="headerlink" title="A Survey of Large Language Models"></a>A Survey of Large Language Models</h2><p>这是一篇关于LLM的综述性文章，它从LLM的历史演进方面、LLM背景、GPT系列模型技术迭代、LLM相关资源（公共可用模型检查点和通用预训练语料集）、预训练、模型架构、模型训练、模型微调、对齐微调、模型量化、模型使用、基准测试与评估方法等许多方面讲述了LLM。</p>
<h3 id="主要内容-5"><a href="#主要内容-5" class="headerlink" title="主要内容"></a>主要内容</h3><ol>
<li>引言：介绍了LLM的主要发展阶段</li>
<li>概览：LLM背景、几个定律（law）、关键技术、技术迭代等</li>
<li>LLM相关资源：公共可用模型检查点和APIs、通用预训练语料库、通用微调语料库、对齐数据集、LLM开发库（Transformers、JAX、Megetron-LM等）</li>
<li>预训练：<ol>
<li>数据预处理需要哪些步骤</li>
<li>使用什么样的Tokenization、Architecture、Normalization method、Positional embedding、Activation Functions、Attention（FlashAttention、Full Attention等）、Pretraining-Task</li>
<li>模型训练阶段：如何设置相关超参数、使用多GPU并行训练有哪些方法、有哪些高效训练的方法（ZeRO、FSDP，mixed Precision Training等）。</li>
</ol>
</li>
</ol>
<h2 id="ALiBi"><a href="#ALiBi" class="headerlink" title="ALiBi"></a>ALiBi</h2><p>ALiBi（Attention with Linear Biases）是一种在Transformer模型中引入线性偏置的技术，旨在改进模型的上下文长度和训练效率。ALiBi方法由Ofir Press等人提出，并在论文《Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation》中进行了详细介绍。</p>
<h3 id="主要内容-6"><a href="#主要内容-6" class="headerlink" title="主要内容"></a>主要内容</h3><ol>
<li><p><strong>线性偏置的引入</strong>：ALiBi的核心思想是在Transformer模型的注意力机制中引入线性偏置。这种偏置是基于查询和键之间的相对位置，而不是绝对位置。具体来说，ALiBi在注意力分数的计算中添加了一个线性递减的偏置项，这有助于模型更好地捕捉长距离依赖关系。</p>
</li>
<li><p><strong>上下文长度扩展</strong>：通过引入线性偏置，ALiBi使得Transformer模型能够在测试时处理比训练时更长的输入序列。这种能力被称为“输入长度外推”（input length extrapolation），它允许模型在实际应用中处理更长的文本序列，而无需重新训练。</p>
</li>
<li><p><strong>训练效率提升</strong>：ALiBi方法还提高了模型的训练效率。由于线性偏置的引入，模型在训练时可以更有效地学习长距离依赖关系，从而减少了所需的训练步数和计算资源。</p>
</li>
</ol>
<h3 id="影响和应用-4"><a href="#影响和应用-4" class="headerlink" title="影响和应用"></a>影响和应用</h3><p>ALiBi的提出对自然语言处理领域产生了积极的影响。它不仅改进了Transformer模型的上下文长度处理能力，还提高了训练效率，使得模型在实际应用中更加实用。ALiBi方法已经被应用于多个NLP任务，如文本生成、机器翻译和问答系统，取得了显著的性能提升。</p>
<h3 id="结论-3"><a href="#结论-3" class="headerlink" title="结论"></a>结论</h3><p>《Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation》这篇论文通过引入ALiBi方法，展示了线性偏置在Transformer模型中的应用潜力。这一创新不仅推动了NLP领域的发展，也为其他领域的序列建模问题提供了新的思路和方法。</p>
<h2 id="RoPE"><a href="#RoPE" class="headerlink" title="RoPE"></a>RoPE</h2><p>RoPE（Rotary Position Embedding）是一种在Transformer模型中引入旋转位置嵌入的技术，旨在改进模型的上下文长度和性能。RoPE方法由Jianlin Su等人提出，并在论文《RoFormer: Enhanced Transformer with Rotary Position Embedding》中进行了详细介绍。</p>
<h3 id="主要内容-7"><a href="#主要内容-7" class="headerlink" title="主要内容"></a>主要内容</h3><ol>
<li><p><strong>旋转位置嵌入的引入</strong>：RoPE的核心思想是通过旋转矩阵来编码位置信息。具体来说，RoPE在Transformer模型的注意力机制中引入了一种新的位置嵌入方法，该方法使用旋转矩阵来表示序列中元素的位置关系。这种旋转矩阵的引入使得模型能够更好地捕捉长距离依赖关系。</p>
</li>
<li><p><strong>上下文长度扩展</strong>：通过引入旋转位置嵌入，RoPE使得Transformer模型能够在处理长序列时保持较高的性能。这种能力有助于模型在实际应用中处理更长的文本序列，而无需重新训练。</p>
</li>
<li><p><strong>性能提升</strong>：RoPE方法还提高了模型的性能。由于旋转位置嵌入的引入，模型在多个NLP任务上取得了显著的性能提升，如文本生成、机器翻译和问答系统。</p>
</li>
<li><p>Finally, experiments on both English and Chinese benchmark datasets <strong>demonstrate that our method encourages faster convergence in pre-training</strong>. The experimental results also show that our proposed RoFormer can achieve <strong>better performance on long texts task</strong>.</p>
</li>
</ol>
<p>它不仅改进了Transformer模型的上下文长度处理能力，还提高了模型在多个NLP任务上的性能。RoPE方法已经被应用于多个NLP模型，如Llama。</p>
<img src="\llm\image-20240907202903119.png" alt="image-20240907202903119" style="zoom:50%;" />

<p>其中 \theta 是可调的参数，也是提供Long-term decay性质（离得越远的token关系应该越小）的参数。</p>
<ul>
<li>Long-term decay: Following Vaswani et al. [2017], we set θi &#x3D; 10000−2i&#x2F;d. One can prove that this setting provides a long-term decay property (refer to Section (3.4.3) for more details), which means the inner-product will decay when the relative position increase. This property coincides with the intuition that a pair of tokens with a long relative distance should have less connection.</li>
</ul>
<h2 id="GPT-3（Language-Models-are-Few-Shot-Learners）"><a href="#GPT-3（Language-Models-are-Few-Shot-Learners）" class="headerlink" title="GPT-3（Language Models are Few-Shot Learners）"></a>GPT-3（Language Models are Few-Shot Learners）</h2><p>“Language Models are Few-Shot Learners”是由OpenAI团队发表的一篇论文，主要介绍了GPT-3（Generative Pre-trained Transformer 3）模型的研究成果。这篇论文的核心观点是，大型语言模型如GPT-3具有在极少样本（few-shot）甚至零样本（zero-shot）的情况下进行有效学习的能力。以下是该论文的主要内容和结论：</p>
<h3 id="主要内容-8"><a href="#主要内容-8" class="headerlink" title="主要内容"></a>主要内容</h3><ol>
<li><strong>模型规模</strong>：论文介绍了GPT-3的规模，它拥有1750亿个参数，是当时最大的语言模型之一。</li>
<li><strong>学习能力</strong>：GPT-3展示了在不需要大量标注数据的情况下，通过极少的样本（few-shot）、单个样本（one-shot）或甚至没有样本（zero-shot）的情况下进行学习的能力。</li>
<li><strong>任务适应性</strong>：GPT-3能够在多种自然语言处理任务上进行适应，包括文本生成、翻译、问答、摘要等，显示出强大的通用性。</li>
<li><strong>性能评估</strong>：论文通过一系列基准测试和实际应用案例，评估了GPT-3在不同任务上的性能，并与其他模型进行了比较。</li>
</ol>
<h3 id="结论-4"><a href="#结论-4" class="headerlink" title="结论"></a>结论</h3><p>论文的结论是，GPT-3作为一种大型预训练语言模型，能够在极少样本的情况下进行有效学习，并在多种任务上展现出优异的性能。这一发现挑战了传统机器学习需要大量标注数据的模式，为自然语言处理领域带来了新的可能性。同时，GPT-3的成功也引发了对模型规模、计算资源消耗、模型可解释性和伦理问题等方面的深入讨论。</p>
<h3 id="影响和应用-5"><a href="#影响和应用-5" class="headerlink" title="影响和应用"></a>影响和应用</h3><p>GPT-3的出现对自然语言处理领域产生了深远的影响，其应用范围广泛，包括但不限于：</p>
<ol>
<li><strong>内容生成</strong>：GPT-3能够生成高质量的文本内容，包括文章、故事、诗歌等。</li>
<li><strong>编程辅助</strong>：GPT-3可以帮助生成代码，辅助编程工作。</li>
<li><strong>语言翻译</strong>：GPT-3在多种语言之间的翻译任务上表现出色。</li>
<li><strong>教育辅导</strong>：GPT-3可以用于生成教育材料，辅助学习和教学。</li>
</ol>
<p>总的来说，“Language Models are Few-Shot Learners”这篇论文展示了GPT-3在自然语言处理任务上的强大能力，推动了大型预训练模型的发展，并为未来的研究方向提供了新的视角。</p>
<h2 id="CoT"><a href="#CoT" class="headerlink" title="CoT"></a>CoT</h2><h3 id="主要内容-9"><a href="#主要内容-9" class="headerlink" title="主要内容"></a>主要内容</h3><p>论文“Chain of Thought Prompting Elicits Reasoning in Large Language Models”主要探讨了如何通过一种称为“思维链提示”（Chain-of-Thought Prompting）的技术来提高大型语言模型（如GPT-3）在需要复杂推理的任务上的性能。思维链提示的核心思想是引导模型生成一系列逻辑上连贯的中间推理步骤，从而逐步解决问题的各个部分，最终得出答案。这种方法模拟了人类解决问题的自然过程，通过展示推理路径，增强了模型的推理能力。</p>
<h3 id="影响和应用-6"><a href="#影响和应用-6" class="headerlink" title="影响和应用"></a>影响和应用</h3><p>思维链提示技术对自然语言处理领域产生了深远的影响，特别是在以下几个方面：</p>
<ol>
<li><strong>提高模型性能</strong>：在多种需要复杂推理的任务上，如数学问题解决、逻辑谜题和常识推理等，思维链提示显著提高了模型的性能。</li>
<li><strong>增强可解释性</strong>：通过展示模型的推理过程，思维链提示增强了模型的可解释性，使得用户和研究者能够更好地理解模型的决策过程。</li>
<li><strong>推动研究方向</strong>：这种方法激发了更多关于如何进一步优化和扩展模型推理能力的研究，推动了自然语言处理领域的发展。</li>
</ol>
<h3 id="结论-5"><a href="#结论-5" class="headerlink" title="结论"></a>结论</h3><p>论文的结论是，思维链提示是一种有效的方法，能够显著提高大型语言模型在复杂推理任务上的表现。通过模拟人类的推理过程，这种方法不仅能够帮助模型给出正确的答案，还能够展示出其推理过程，这对于理解和解释模型的决策过程非常有用。此外，思维链提示还为自然语言处理领域带来了新的研究方向和应用可能性，展示了大型语言模型在复杂任务上的巨大潜力。</p>
<h2 id="RAG"><a href="#RAG" class="headerlink" title="RAG"></a>RAG</h2><p>“Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”这篇论文提出了一种新的方法，用于增强生成模型在知识密集型自然语言处理任务（如问答、事实验证和长篇内容生成）中的性能。这种方法结合了检索（retrieval）和生成（generation）两个步骤，以提高模型对特定领域知识的利用效率。</p>
<h3 id="主要内容-10"><a href="#主要内容-10" class="headerlink" title="主要内容"></a>主要内容</h3><ol>
<li><strong>检索增强生成</strong>：论文提出的方法核心是检索增强生成（Retrieval-Augmented Generation，简称RAG）。RAG模型由两个主要部分组成：一个检索器（retriever）和一个生成器（generator）。检索器负责从大型知识库中检索相关文档或段落，生成器则利用这些检索到的信息来生成更准确和丰富的文本。</li>
<li><strong>端到端训练</strong>：RAG模型可以进行端到端的训练，这意味着检索器和生成器可以同时优化，以提高整体性能。</li>
<li><strong>灵活性和可解释性</strong>：RAG模型提供了更高的灵活性和可解释性，因为它明确地利用了外部知识源，使得模型的决策过程更加透明。</li>
</ol>
<h3 id="结论-6"><a href="#结论-6" class="headerlink" title="结论"></a>结论</h3><p>论文的结论是，通过结合检索和生成两个步骤，RAG模型在知识密集型自然语言处理任务上取得了显著的性能提升。这种方法不仅提高了生成文本的准确性和相关性，还增强了模型的可解释性和灵活性。</p>
<h3 id="影响和应用-7"><a href="#影响和应用-7" class="headerlink" title="影响和应用"></a>影响和应用</h3><p>RAG模型的提出对自然语言处理领域产生了重要影响，特别是在以下几个方面：</p>
<ol>
<li><strong>问答系统</strong>：RAG模型在开放域问答任务上表现出色，能够提供更准确和详细的答案。</li>
<li><strong>内容生成</strong>：在需要特定领域知识的文本生成任务中，RAG模型能够生成更丰富和准确的内容。</li>
<li><strong>事实验证</strong>：RAG模型在事实验证任务中能够更好地利用外部知识源，提高验证的准确性。</li>
</ol>
<p>总的来说，“Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”这篇论文提出了一种创新的方法，通过结合检索和生成两个步骤，显著提高了生成模型在知识密集型自然语言处理任务上的性能。这一方法为解决复杂的信息检索和文本生成问题提供了新的思路，推动了自然语言处理技术的发展。</p>
<h2 id="Prefix-Tuning"><a href="#Prefix-Tuning" class="headerlink" title="Prefix-Tuning"></a>Prefix-Tuning</h2><p>“Prefix-Tuning: Optimizing Continuous Prompts for Generation”这篇论文提出了一种新的方法，名为Prefix-Tuning，用于优化生成模型中的连续提示（continuous prompts）。Prefix-Tuning旨在通过在输入序列前添加可训练的前缀（prefix）来引导模型生成更高质量的文本，同时减少对大量标注数据的依赖。</p>
<h3 id="主要内容-11"><a href="#主要内容-11" class="headerlink" title="主要内容"></a>主要内容</h3><ol>
<li><strong>前缀优化</strong>：Prefix-Tuning的核心思想是在输入序列前添加一个可训练的前缀，这个前缀作为模型的引导，帮助模型生成更符合特定任务或风格的文本。这些前缀是连续的向量，可以通过微调来优化。</li>
<li><strong>减少数据依赖</strong>：通过使用前缀优化，模型可以在不需要大量标注数据的情况下进行微调，从而减少了数据依赖。</li>
<li><strong>任务适应性</strong>：Prefix-Tuning可以应用于多种生成任务，包括文本生成、对话系统和机器翻译等，显示出强大的任务适应性。</li>
</ol>
<h3 id="结论-7"><a href="#结论-7" class="headerlink" title="结论"></a>结论</h3><p>论文的结论是，Prefix-Tuning作为一种有效的生成模型优化方法，能够在减少数据依赖的同时，提高生成文本的质量和相关性。这种方法通过引入可训练的前缀，为模型提供了更灵活的引导，使得模型能够更好地适应不同的生成任务。</p>
<h3 id="影响和应用-8"><a href="#影响和应用-8" class="headerlink" title="影响和应用"></a>影响和应用</h3><p>Prefix-Tuning的提出对自然语言处理和生成模型领域产生了重要影响，特别是在以下几个方面：</p>
<ol>
<li><strong>文本生成</strong>：Prefix-Tuning在文本生成任务中表现出色，能够生成更符合特定风格或主题的文本。</li>
<li><strong>对话系统</strong>：在对话系统中，Prefix-Tuning可以帮助模型生成更连贯和自然的对话内容。</li>
<li><strong>机器翻译</strong>：Prefix-Tuning在机器翻译任务中也显示出潜力，能够提高翻译的准确性和流畅性。</li>
</ol>
<p>总的来说，“Prefix-Tuning: Optimizing Continuous Prompts for Generation”这篇论文提出了一种创新的生成模型优化方法，通过引入可训练的前缀，显著提高了生成文本的质量和相关性，同时减少了数据依赖。这一方法为自然语言处理和生成模型领域提供了新的研究方向和应用可能性。</p>
<h2 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h2><p>“LoRA”是一种在自然语言处理领域中用于优化和调整大型语言模型的技术。论文标题为“LoRA: Low-Rank Adaptation of Large Language Models”，由微软的研究人员发表。这篇论文主要探讨了如何在保持模型性能的同时，减少大型语言模型在特定任务上微调时的参数数量和计算资源消耗。</p>
<h3 id="主要内容-12"><a href="#主要内容-12" class="headerlink" title="主要内容"></a>主要内容</h3><ol>
<li><strong>低秩适应</strong>：LoRA的核心思想是通过低秩分解（low-rank decomposition）来调整大型语言模型的权重矩阵。具体来说，它将模型的权重矩阵分解为两个较小的矩阵，从而减少了需要调整的参数数量。</li>
<li><strong>参数效率</strong>：通过使用LoRA，研究人员能够在不显著影响模型性能的情况下，大幅减少微调过程中所需的参数数量，从而提高了参数效率。</li>
<li><strong>计算效率</strong>：LoRA还减少了微调过程中的计算资源消耗，使得在资源受限的环境下也能有效地进行模型微调。</li>
</ol>
<h3 id="结论-8"><a href="#结论-8" class="headerlink" title="结论"></a>结论</h3><p>论文的结论是，LoRA作为一种有效的模型微调技术，能够在保持模型性能的同时，显著减少参数数量和计算资源消耗。这种方法对于优化大型语言模型在特定任务上的适应性具有重要意义，尤其是在资源受限的环境中。</p>
<h3 id="影响和应用-9"><a href="#影响和应用-9" class="headerlink" title="影响和应用"></a>影响和应用</h3><p>LoRA的出现对自然语言处理领域产生了积极的影响，特别是在以下几个方面：</p>
<ol>
<li><strong>模型部署</strong>：LoRA使得大型语言模型在特定任务上的微调更加高效，有助于模型的实际部署和应用。</li>
<li><strong>资源节约</strong>：通过减少参数数量和计算资源消耗，LoRA有助于节约训练和微调过程中的资源，降低了成本。</li>
<li><strong>研究推广</strong>：LoRA的技术推广为更多研究者和开发者提供了在资源有限的情况下使用大型语言模型的可能性。</li>
</ol>
<p>总的来说，“LoRA: Low-Rank Adaptation of Large Language Models”这篇论文展示了低秩适应技术在优化大型语言模型微调过程中的有效性，为自然语言处理领域的模型优化和资源节约提供了新的思路和方法。</p>
<h2 id="Megatron-LM"><a href="#Megatron-LM" class="headerlink" title="Megatron-LM"></a>Megatron-LM</h2><p>“Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism”是由NVIDIA的研究人员发表的一篇论文。这篇论文主要介绍了Megatron-LM，这是一个用于训练具有数十亿参数的大型语言模型的模型并行化框架。Megatron-LM通过高效的模型并行化技术，使得在有限的计算资源下能够训练更大规模的语言模型。</p>
<h3 id="主要内容-13"><a href="#主要内容-13" class="headerlink" title="主要内容"></a>主要内容</h3><ol>
<li><p><strong>模型并行化</strong>：Megatron-LM的核心技术是模型并行化，它通过将模型的参数分布到多个GPU上，实现了在单个计算节点上无法完成的超大模型训练。就是把一个层从中间切开的方式，通常叫做张量并行（Tensor Parallel），在一个张量的内部切开做并行。但是论文题目讲的是Model Parallelism（模型并行），所以有时模型并行指的是这篇文章的张量并行。</p>
<ol>
<li>他指出其他的工作用起来没有自己的简单，但是他这种简单牺牲了通用性，也就是只能用在transformer模型中。</li>
</ol>
</li>
<li><p><strong>高效通信</strong>：论文中提出了一种高效的通信策略，减少了模型并行化过程中的通信开销，从而提高了训练效率。</p>
</li>
<li><p><strong>扩展性</strong>：Megatron-LM展示了良好的扩展性，能够在增加GPU数量的情况下，线性地提高训练速度。</p>
</li>
<li><p>总体上看，就是把模型整个从中间切开，分到不同的GPU上</p>
<ol>
<li><p>视频：32:38</p>
</li>
<li><p>代价</p>
<ol>
<li>每一层都要做all reduce</li>
<li>所有GPU都要同步，无法异步</li>
</ol>
</li>
<li><p>好处</p>
<ol>
<li>每个GPU只有部分模型，减少每个GPU的显存需求</li>
</ol>
</li>
</ol>
</li>
</ol>
<h3 id="MLP部分并行"><a href="#MLP部分并行" class="headerlink" title="MLP部分并行"></a>MLP部分并行</h3><p>$$<br>Y&#x3D;\text{GeLU}(XA)B\<br>Z &#x3D; \text{Dropout}(YB)<br>$$</p>
<ul>
<li>视频：19:12</li>
</ul>
<h3 id="多头注意力部分并行"><a href="#多头注意力部分并行" class="headerlink" title="多头注意力部分并行"></a>多头注意力部分并行</h3><ul>
<li>由于每个GPU都含有完整的一份数据X，所以让每个GPU分别负责一个头，或者说负责 n_head&#x2F;n_GPU 个头</li>
<li>每个GPU产生头之后，需要按列Concat，再乘以权重W，这就相当于把数据按列切分，相应的W需要按行切分分到每个GPU上，这样每个GPU都输出一个形状相同，但值只是最终结果的一部分，最后再把所有GPU上的结果加起来就是正确的结果了。所以通讯的部分只有最后一次。</li>
</ul>
<h3 id="输入输出层"><a href="#输入输出层" class="headerlink" title="输入输出层"></a>输入输出层</h3><ul>
<li><p>输入层</p>
<ul>
<li><p>就是token数字索引转word embedding层，<a href="#GPT.__init__"><code>wte=nn.Embedding(config.vocab_size, config.n_embd)</code></a></p>
</li>
<li><p>把词典按GPU个数划分，每个GPU只含有一部分词典，在一个GPU上如果一个token找不到对应的embedding，就设为0。</p>
</li>
<li><p>每个GPU的输入层在使用自己的词典后，把所有GPU的输出结果累加即可。</p>
</li>
</ul>
</li>
<li><p>输出层</p>
<ul>
<li><p>视频：30:03</p>
</li>
<li><p>就是最后的Linear层</p>
<p>NanoGPT中的<a href="#GPT.__init__"><code>self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False</code>)</a></p>
<p>（B, T, K）– lm_head –&gt; (B, T, V)</p>
</li>
<li><p>因为lm_head层也是划分到所有GPU上的，所以最终的结果 （B, T, V）也是每个GPU上各有一部分。</p>
</li>
<li><p>在得到 (B, T, V)之后，实际上已经走完模型了，随后就是通过这个logits得到下一个预测的token，要做的就是softmax</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, idx, max_new_tokens, temperature=<span class="number">1.0</span>, top_k=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete</span></span><br><span class="line"><span class="string">    the sequence max_new_tokens times, feeding the predictions back into the model each time.</span></span><br><span class="line"><span class="string">    Most likely you&#x27;ll want to make sure to be in model.eval() mode of operation for this.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">        <span class="comment"># if the sequence context is growing too long we must crop it at block_size</span></span><br><span class="line">        idx_cond = idx <span class="keyword">if</span> idx.size(<span class="number">1</span>) &lt;= <span class="variable language_">self</span>.block_size <span class="keyword">else</span> idx[:, -<span class="variable language_">self</span>.block_size:]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># forward the model to get the logits for the index in the sequence</span></span><br><span class="line">        logits, _ = <span class="variable language_">self</span>(idx_cond)  <span class="comment"># (B, T, vocab_size)</span></span><br><span class="line">        <span class="comment"># pluck the logits at the final step and scale by desired temperature</span></span><br><span class="line">        logits = logits[:, -<span class="number">1</span>, :] / temperature  <span class="comment"># (B, 1, vocab_size)</span></span><br><span class="line">        <span class="comment"># optionally crop the logits to only the top k options</span></span><br><span class="line">        <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            v, _ = torch.topk(logits, top_k)</span><br><span class="line">            logits[logits &lt; v[:, [-<span class="number">1</span>]]] = -<span class="built_in">float</span>(<span class="string">&#x27;Inf&#x27;</span>)</span><br><span class="line">        <span class="comment"># apply softmax to convert logits to (normalized) probabilities</span></span><br><span class="line">        probs = F.softmax(logits, dim=-<span class="number">1</span>)  <span class="comment"># (B, 1, vocab_size)</span></span><br><span class="line">        <span class="comment"># sample from the distribution</span></span><br><span class="line">        idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>)  <span class="comment"># (B, 1, 1)</span></span><br><span class="line">        <span class="comment"># append sampled index to the running sequence and continue</span></span><br><span class="line">        idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><img src="\llm\image-20240822143852892.png" alt="image-20240822143852892" style="zoom:33%;" /></li>
</ul>
</li>
</ul>
<p><video src="F:\Videos\limu\Megatron-LM.mp4"></video></p>
<ul>
<li>MLP并行：19:12</li>
<li>注意力层：23:14</li>
<li>输入输出层：28:18</li>
</ul>
<h2 id="ZeRO"><a href="#ZeRO" class="headerlink" title="ZeRO"></a>ZeRO</h2><p>“ZeRO: Memory Optimizations Toward Training Trillion Parameter Models”是由微软的研究人员发表的一篇论文。这篇论文提出了一种名为ZeRO（Zero Redundancy Optimizer）的内存优化技术，旨在解决训练具有数万亿参数的超大规模深度学习模型时面临的内存瓶颈问题。</p>
<p>如何将数据并行应用到超大模型上</p>
<h3 id="主要内容-14"><a href="#主要内容-14" class="headerlink" title="主要内容"></a>主要内容</h3><ol>
<li><strong>内存优化</strong>：ZeRO的核心思想是通过消除内存冗余来优化内存使用。在传统的数据并行训练中，每个GPU都需要存储模型的完整参数副本，这导致了大量的内存浪费。ZeRO通过将模型参数、优化器状态和梯度分散到多个GPU上，显著减少了每个GPU的内存需求。</li>
<li><strong>扩展性</strong>：ZeRO技术使得在有限的GPU内存资源下，能够训练具有数万亿参数的超大规模模型，极大地提高了模型的扩展性。</li>
<li><strong>计算效率</strong>：通过减少内存冗余，ZeRO不仅优化了内存使用，还提高了计算效率，使得训练过程更加高效。</li>
<li>就是通过增加带宽来减小显存占用</li>
</ol>
<p><img src="/.%5Cllm%5Cimage-20240822183637274.png" alt="image-20240822183637274"></p>
<p>K &#x3D; 3 * 4 &#x3D; 12</p>
<p>参见[混合精度训练](#混合精度训练（Mixed-Precision Training）)</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center">(单位：byte)</th>
<th align="center">ZeRO优化</th>
<th align="center">优化后大小（Nd：Number of device）</th>
</tr>
</thead>
<tbody><tr>
<td align="center">W</td>
<td align="center">权重（用于前向）</td>
<td align="center">fp16</td>
<td align="center">2φ</td>
<td align="center">ZeRO-3</td>
<td align="center">2φ&#x2F;Nd</td>
</tr>
<tr>
<td align="center">activations或dW</td>
<td align="center">激活值，反向传播后成为梯度</td>
<td align="center">fp16</td>
<td align="center">2φ</td>
<td align="center">ZeRO-2</td>
<td align="center">2φ&#x2F;Nd</td>
</tr>
<tr>
<td align="center">W</td>
<td align="center">权重fp32副本（用于更新）</td>
<td align="center">fp32</td>
<td align="center">4φ</td>
<td align="center">ZeRO-1</td>
<td align="center">4φ&#x2F;Nd</td>
</tr>
<tr>
<td align="center">Momentum</td>
<td align="center">Adam优化器状态</td>
<td align="center">fp32</td>
<td align="center">4φ</td>
<td align="center">ZeRO-1</td>
<td align="center">4φ&#x2F;Nd</td>
</tr>
<tr>
<td align="center">Variance</td>
<td align="center">Adam优化器状态</td>
<td align="center">fp32</td>
<td align="center">4φ</td>
<td align="center">ZeRO-1</td>
<td align="center">4φ&#x2F;Nd</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">16φ</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody></table>
<p>时间：22:55</p>
<p><video src="F:\Videos\limu\ZeRo.mp4"></video></p>
<h2 id="MoE"><a href="#MoE" class="headerlink" title="MoE"></a>MoE</h2><p>“OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER”是由Google Brain团队发表的一篇论文。这篇论文提出了一种新的神经网络架构，名为稀疏门控混合专家层（Sparsely-Gated Mixture-of-Experts Layer，简称MoE），旨在解决传统神经网络在扩展到极大规模时面临的挑战。</p>
<h3 id="主要内容-15"><a href="#主要内容-15" class="headerlink" title="主要内容"></a>主要内容</h3><ol>
<li><strong>混合专家层</strong>：MoE层由多个专家（experts）组成，每个专家是一个独立的神经网络。在MoE层中，输入数据会被分配给不同的专家进行处理，每个专家专注于处理特定类型的数据。</li>
<li><strong>稀疏门控</strong>：为了提高效率，论文引入了一个稀疏门控机制，该机制决定哪些专家应该处理当前的输入数据。这个门控机制通过学习来选择最相关的专家，从而实现稀疏激活，即只有少数专家会被激活来处理当前的输入。</li>
<li><strong>扩展性</strong>：通过使用MoE层，模型可以在不增加计算负担的情况下扩展到极大的规模，因为只有少数专家会被激活来处理每个输入。</li>
</ol>
<h3 id="结论-9"><a href="#结论-9" class="headerlink" title="结论"></a>结论</h3><p>论文的结论是，通过引入稀疏门控混合专家层，可以有效地扩展神经网络的规模，同时保持计算效率。这种方法使得模型能够在处理复杂任务时利用更多的专家知识，而不会增加过多的计算成本。</p>
<h3 id="影响和应用-10"><a href="#影响和应用-10" class="headerlink" title="影响和应用"></a>影响和应用</h3><p>MoE层的提出对深度学习和神经网络领域产生了重要影响，特别是在以下几个方面：</p>
<ol>
<li><strong>模型规模</strong>：MoE层使得训练和部署具有大量参数的神经网络成为可能，推动了模型规模的进一步扩展。</li>
<li><strong>任务适应性</strong>：通过使用多个专家，MoE层能够更好地适应不同的任务和数据类型，提高了模型的灵活性和性能。</li>
<li><strong>计算效率</strong>：稀疏门控机制确保了在处理每个输入时只有少数专家被激活，从而提高了计算效率，减少了资源消耗。</li>
</ol>
<p>总的来说，“OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER”这篇论文提出了一种创新的神经网络架构，通过稀疏门控混合专家层，有效地解决了传统神经网络在扩展到极大规模时面临的挑战。这一方法为深度学习和神经网络领域的模型扩展和计算效率提供了新的思路和解决方案。</p>
<h2 id="Clip"><a href="#Clip" class="headerlink" title="Clip"></a>Clip</h2><h2 id="LongLoRA"><a href="#LongLoRA" class="headerlink" title="LongLoRA"></a>LongLoRA</h2><p>arXiv:2309.12307v3</p>
<p>提出了一种高效微调方法，通过这种方法可以扩展预训练模型的上下文窗口大小。</p>
<p>特点：</p>
<ul>
<li>with limited computation cost.</li>
<li>extends models’ context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2.</li>
<li>can be implemented with only two lines of code in training.</li>
</ul>
<p>方法：</p>
<ul>
<li><p>shifted sparse attention (S2 -Attn)</p>
<p> although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention (S2 -Attn) effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference.</p>
</li>
<li><p>improved LoRA</p>
<p>find that LoRA for context extension works well under the premise of trainable embedding and normalization</p>
</li>
</ul>
<h1 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h1><blockquote>
<p><a target="_blank" rel="noopener" href="https://tiktokenizer.vercel.app/">https://tiktokenizer.vercel.app/</a></p>
</blockquote>
<h2 id="minbpe"><a href="#minbpe" class="headerlink" title="minbpe"></a>minbpe</h2><p><a target="_blank" rel="noopener" href="https://github.com/karpathy/minbpe">karpathy&#x2F;minbpe: Minimal, clean code for the Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. (github.com)</a></p>
<p>Minimal, clean code for the (byte-level) Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. The BPE algorithm is “byte-level” because it runs on UTF-8 encoded strings.</p>
<p>This algorithm was popularized for LLMs by the <a target="_blank" rel="noopener" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2 paper</a> and the associated GPT-2 <a target="_blank" rel="noopener" href="https://github.com/openai/gpt-2">code release</a> from OpenAI. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.07909">Sennrich et al. 2015</a> is cited as the original reference for the use of BPE in NLP applications. Today, all modern LLMs (e.g. GPT, Llama, Mistral) use this algorithm to train their tokenizers.</p>
<p>There are two Tokenizers in this repository, both of which can perform the 3 primary functions of a Tokenizer: 1) train the tokenizer vocabulary and merges on a given text, 2) encode from text to tokens, 3) decode from tokens to text. The files of the repo are as follows:</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://github.com/karpathy/minbpe/blob/master/minbpe/base.py">minbpe&#x2F;base.py</a>: Implements the <code>Tokenizer</code> class, which is the base class. It contains the <code>train</code>, <code>encode</code>, and <code>decode</code> stubs, save&#x2F;load functionality, and there are also a few common utility functions. This class is not meant to be used directly, but rather to be inherited from.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/karpathy/minbpe/blob/master/minbpe/basic.py">minbpe&#x2F;basic.py</a>: Implements the <code>BasicTokenizer</code>, the simplest implementation of the BPE algorithm that runs directly on text.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/karpathy/minbpe/blob/master/minbpe/regex.py">minbpe&#x2F;regex.py</a>: Implements the <code>RegexTokenizer</code> that further splits the input text by a regex pattern, which is a preprocessing stage that splits up the input text by categories (think: letters, numbers, punctuation) before tokenization. This ensures that no merges will happen across category boundaries. This was introduced in the GPT-2 paper and continues to be in use as of GPT-4. This class also handles special tokens, if any.</li>
<li><a target="_blank" rel="noopener" href="https://github.com/karpathy/minbpe/blob/master/minbpe/gpt4.py">minbpe&#x2F;gpt4.py</a>: Implements the <code>GPT4Tokenizer</code>. This class is a light wrapper around the <code>RegexTokenizer</code> (2, above) that exactly reproduces the tokenization of GPT-4 in the <a target="_blank" rel="noopener" href="https://github.com/openai/tiktoken">tiktoken</a> library. The wrapping handles some details around recovering the exact merges in the tokenizer, and the handling of some unfortunate (and likely historical?) 1-byte token permutations.</li>
</ol>
<p>Finally, the script <a target="_blank" rel="noopener" href="https://github.com/karpathy/minbpe/blob/master/train.py">train.py</a> trains the two major tokenizers on the input text <a target="_blank" rel="noopener" href="https://github.com/karpathy/minbpe/blob/master/tests/taylorswift.txt">tests&#x2F;taylorswift.txt</a> (this is the Wikipedia entry for her kek) and saves the vocab to disk for visualization. This script runs in about 25 seconds on my (M1) MacBook.</p>
<p>All of the files above are very short and thoroughly commented, and also contain a usage example on the bottom of the file.</p>
<h2 id="一些问题（来自Andrej-Karpathy"><a href="#一些问题（来自Andrej-Karpathy" class="headerlink" title="一些问题（来自Andrej Karpathy)"></a>一些问题（来自Andrej Karpathy)</h2><p>视频1:51:54</p>
<ol>
<li><p>为什么LLM不能拼写单词&#x2F;无法处理简单的字符串问题，例如反转</p>
<ul>
<li><p>例如问LLM：How many letters “l” are there in the word “.DefaultCellStyle”</p>
<p>可能得不到正确的答案，DeepSeek&#x2F;通义都错了</p>
</li>
<li><p>或者问（不让用工具）：Reverse the string: “.DefaultCellStyle” Print it as a comma-separated list of characters，也得不到正确的答案，DeepSeek&#x2F;通义都错了</p>
</li>
<li><p>如果你这样问：Reverse the string: “.DefaultCellStyle”. Take the following approach.First print out everysingle character seaprated by spaces. Then as a step 2 print out all the characters backwards,again separated by spaces.</p>
<p>就可能得到正确的答案，DeepSeek&#x2F;通义还是错</p>
</li>
</ul>
<p>原因可能就是单词<code>.DefaultCellStyle</code>被当作了一个token</p>
</li>
<li><p>why is LLM worse at non-English languages (e.g. Japanese)? </p>
<p>除了非英语的训练数据少以外，tokenization也是原因</p>
</li>
<li><p>Why is LLM bad at simple arithmetic? </p>
</li>
<li><p>why did GPT-2 have more than necessary trouble coding in Python?</p>
</li>
<li><p>Why did rhy LLM abruptly halt when it sees the string “<code>&lt;lendoftextl&gt;</code>“? Tokenization</p>
</li>
<li><p>What is this weird warning I get about a “trailing whitespace”? Tokenization</p>
<p>因为有些tokenization方法使得每个单词都是以空格开头，你手动加了一个空格，那LLM补全的话，又要加一个空格，两个空格又不符合格式规范，就会使用不带空格的token，这个token在语义上可能不如带空格的</p>
</li>
<li><p>Why the LLM break if I ask it about”SolidGoldMagikarp”? Tokenization.</p>
</li>
<li><p>Why shouldIprefer touseYAML overJSONwithLLMs?Tokenization.</p>
</li>
<li><p>Why is LLM not actually end-to-end language modeling? Tokenization.</p>
</li>
<li><p>What is the real root of suffering? Tokenization.</p>
</li>
</ol>
<h2 id="BPE（Byte-Pair-Encoding）"><a href="#BPE（Byte-Pair-Encoding）" class="headerlink" title="BPE（Byte-Pair Encoding）"></a>BPE（Byte-Pair Encoding）</h2><p>BPE 算法的工作流程：</p>
<ol>
<li><p><strong>初始化</strong>: 开始时，每个字符都被视为一个独立的 token。</p>
</li>
<li><p><strong>统计</strong>: 计算所有相邻<strong>字符对</strong>（即 byte pairs）在训练文本中的出现频率。</p>
</li>
<li><p><strong>合并</strong>: 找到出现频率最高的字符对，并将其合并成一个新的 token。</p>
</li>
<li><p><strong>更新</strong>: 使用新 token 替换原来的字符对，并重新计算所有字符对的频率。</p>
</li>
<li><p><strong>重复</strong>: 重复步骤 3 和 4 直到达到预设的词汇表大小或达到预定的迭代次数。</p>
</li>
</ol>
<p>因此，每完成一次这样的迭代，原本的字符对就被替换成了一个新的 token，这意味着 token 的种类减少了至少一种。随着迭代的继续，词汇表会逐渐变得更小，最终达到所需的大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_stats</span>(<span class="params">ids</span>):</span><br><span class="line">    counts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> pair <span class="keyword">in</span> <span class="built_in">zip</span>(ids, ids[<span class="number">1</span>:]):</span><br><span class="line">        counts[pair] = counts.get(pair, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> counts</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">merge</span>(<span class="params">ids, pair, idx</span>):</span><br><span class="line">    newids = []</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(ids):</span><br><span class="line">        <span class="keyword">if</span> i &lt; <span class="built_in">len</span>(ids) - <span class="number">1</span> <span class="keyword">and</span> ids[i] == pair[<span class="number">0</span>] <span class="keyword">and</span> ids[i+<span class="number">1</span>] == pair[<span class="number">1</span>]:</span><br><span class="line">            newids.append(idx)</span><br><span class="line">            i += <span class="number">2</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            newids.append(ids[i])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> newids</span><br><span class="line"></span><br><span class="line"><span class="comment"># -------------</span></span><br><span class="line">vocab_size = <span class="number">276</span> <span class="comment"># the desired final vocabulary size</span></span><br><span class="line">num_merges = vocab_size - <span class="number">256</span></span><br><span class="line">ids = <span class="built_in">list</span>(tokens) <span class="comment"># copy so we don&#x27;t destroy the original list</span></span><br><span class="line">merges = &#123;&#125; <span class="comment"># (int, int) -&gt; int</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_merges):</span><br><span class="line">    stats = get_stats(ids)</span><br><span class="line">    pair = <span class="built_in">max</span>(stats, key=stats.get)</span><br><span class="line">    idx = <span class="number">256</span> + i</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;merging <span class="subst">&#123;pair&#125;</span> into a new token <span class="subst">&#123;idx&#125;</span>&quot;</span>)</span><br><span class="line">    ids = merge(ids, pair, idx)</span><br></pre></td></tr></table></figure>

<p>时间：28分</p>
<p><video src="F:\Videos\Andrej Karpathy\Let's build the GPT Tokenizer.mp4"></video></p>
<p>但是，在LLM中，一般还会在使用BPE算法前对文本预处理，比如GPT2会先把文本按单词分开，然后再用BPE，防止单词末尾的字母和标点符号组成token pair。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">gt2pat = re.<span class="built_in">compile</span>(<span class="string">r&quot;&quot;&quot;&#x27;s|&#x27;t|&#x27;re|&#x27;ve|&#x27;m|&#x27;ll|&#x27;d| ?\p&#123;L&#125;+| ?\p&#123;N&#125;+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(re.findall(gt2pat, <span class="string">&quot;Hello&#x27;ve world123 how&#x27;s are   you!!!?&quot;</span>))</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://github.com/openai/gpt-2/blob/9b63575ef42771a015060c964af2c3da4cf7c8ab/src/encoder.py#L53">https://github.com/openai/gpt-2/src/encoder.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;Byte pair encoding utilities&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"></span><br><span class="line"><span class="meta">@lru_cache()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bytes_to_unicode</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Returns list of utf-8 byte and a corresponding list of unicode strings.</span></span><br><span class="line"><span class="string">    The reversible bpe codes work on unicode strings.</span></span><br><span class="line"><span class="string">    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.</span></span><br><span class="line"><span class="string">    When you&#x27;re at something like a 10B token dataset you end up needing around 5K for decent coverage.</span></span><br><span class="line"><span class="string">    This is a signficant percentage of your normal, say, 32K bpe vocab.</span></span><br><span class="line"><span class="string">    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.</span></span><br><span class="line"><span class="string">    And avoids mapping to whitespace/control characters the bpe code barfs on.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    bs = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">ord</span>(<span class="string">&quot;!&quot;</span>), <span class="built_in">ord</span>(<span class="string">&quot;~&quot;</span>)+<span class="number">1</span>))+<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">ord</span>(<span class="string">&quot;¡&quot;</span>), <span class="built_in">ord</span>(<span class="string">&quot;¬&quot;</span>)+<span class="number">1</span>))+<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">ord</span>(<span class="string">&quot;®&quot;</span>), <span class="built_in">ord</span>(<span class="string">&quot;ÿ&quot;</span>)+<span class="number">1</span>))</span><br><span class="line">    cs = bs[:]</span><br><span class="line">    n = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>**<span class="number">8</span>):</span><br><span class="line">        <span class="keyword">if</span> b <span class="keyword">not</span> <span class="keyword">in</span> bs:</span><br><span class="line">            bs.append(b)</span><br><span class="line">            cs.append(<span class="number">2</span>**<span class="number">8</span>+n)</span><br><span class="line">            n += <span class="number">1</span></span><br><span class="line">    cs = [<span class="built_in">chr</span>(n) <span class="keyword">for</span> n <span class="keyword">in</span> cs]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(<span class="built_in">zip</span>(bs, cs))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_pairs</span>(<span class="params">word</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Return set of symbol pairs in a word.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Word is represented as tuple of symbols (symbols being variable-length strings).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    pairs = <span class="built_in">set</span>()</span><br><span class="line">    prev_char = word[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> word[<span class="number">1</span>:]:</span><br><span class="line">        pairs.add((prev_char, char))</span><br><span class="line">        prev_char = char</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, bpe_merges, errors=<span class="string">&#x27;replace&#x27;</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.encoder = encoder</span><br><span class="line">        <span class="variable language_">self</span>.decoder = &#123;v:k <span class="keyword">for</span> k,v <span class="keyword">in</span> <span class="variable language_">self</span>.encoder.items()&#125;</span><br><span class="line">        <span class="variable language_">self</span>.errors = errors <span class="comment"># how to handle errors in decoding</span></span><br><span class="line">        <span class="variable language_">self</span>.byte_encoder = bytes_to_unicode()</span><br><span class="line">        <span class="variable language_">self</span>.byte_decoder = &#123;v:k <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="variable language_">self</span>.byte_encoder.items()&#125;</span><br><span class="line">        <span class="variable language_">self</span>.bpe_ranks = <span class="built_in">dict</span>(<span class="built_in">zip</span>(bpe_merges, <span class="built_in">range</span>(<span class="built_in">len</span>(bpe_merges))))</span><br><span class="line">        <span class="variable language_">self</span>.cache = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions</span></span><br><span class="line">        <span class="variable language_">self</span>.pat = re.<span class="built_in">compile</span>(<span class="string">r&quot;&quot;&quot;&#x27;s|&#x27;t|&#x27;re|&#x27;ve|&#x27;m|&#x27;ll|&#x27;d| ?\p&#123;L&#125;+| ?\p&#123;N&#125;+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">bpe</span>(<span class="params">self, token</span>):</span><br><span class="line">        <span class="keyword">if</span> token <span class="keyword">in</span> <span class="variable language_">self</span>.cache:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.cache[token]</span><br><span class="line">        word = <span class="built_in">tuple</span>(token)</span><br><span class="line">        pairs = get_pairs(word)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> pairs:</span><br><span class="line">            <span class="keyword">return</span> token</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            bigram = <span class="built_in">min</span>(pairs, key = <span class="keyword">lambda</span> pair: <span class="variable language_">self</span>.bpe_ranks.get(pair, <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)))</span><br><span class="line">            <span class="keyword">if</span> bigram <span class="keyword">not</span> <span class="keyword">in</span> <span class="variable language_">self</span>.bpe_ranks:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            first, second = bigram</span><br><span class="line">            new_word = []</span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(word):</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    j = word.index(first, i)</span><br><span class="line">                    new_word.extend(word[i:j])</span><br><span class="line">                    i = j</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    new_word.extend(word[i:])</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> word[i] == first <span class="keyword">and</span> i &lt; <span class="built_in">len</span>(word)-<span class="number">1</span> <span class="keyword">and</span> word[i+<span class="number">1</span>] == second:</span><br><span class="line">                    new_word.append(first+second)</span><br><span class="line">                    i += <span class="number">2</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    new_word.append(word[i])</span><br><span class="line">                    i += <span class="number">1</span></span><br><span class="line">            new_word = <span class="built_in">tuple</span>(new_word)</span><br><span class="line">            word = new_word</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(word) == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                pairs = get_pairs(word)</span><br><span class="line">        word = <span class="string">&#x27; &#x27;</span>.join(word)</span><br><span class="line">        <span class="variable language_">self</span>.cache[token] = word</span><br><span class="line">        <span class="keyword">return</span> word</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, text</span>):</span><br><span class="line">        bpe_tokens = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> re.findall(<span class="variable language_">self</span>.pat, text):</span><br><span class="line">            token = <span class="string">&#x27;&#x27;</span>.join(<span class="variable language_">self</span>.byte_encoder[b] <span class="keyword">for</span> b <span class="keyword">in</span> token.encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">            bpe_tokens.extend(<span class="variable language_">self</span>.encoder[bpe_token] <span class="keyword">for</span> bpe_token <span class="keyword">in</span> <span class="variable language_">self</span>.bpe(token).split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">        <span class="keyword">return</span> bpe_tokens</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        text = <span class="string">&#x27;&#x27;</span>.join([<span class="variable language_">self</span>.decoder[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokens])</span><br><span class="line">        text = <span class="built_in">bytearray</span>([<span class="variable language_">self</span>.byte_decoder[c] <span class="keyword">for</span> c <span class="keyword">in</span> text]).decode(<span class="string">&#x27;utf-8&#x27;</span>, errors=<span class="variable language_">self</span>.errors)</span><br><span class="line">        <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_encoder</span>(<span class="params">model_name, models_dir</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(models_dir, model_name, <span class="string">&#x27;encoder.json&#x27;</span>), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        encoder = json.load(f)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(models_dir, model_name, <span class="string">&#x27;vocab.bpe&#x27;</span>), <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        bpe_data = f.read()</span><br><span class="line">    bpe_merges = [<span class="built_in">tuple</span>(merge_str.split()) <span class="keyword">for</span> merge_str <span class="keyword">in</span> bpe_data.split(<span class="string">&#x27;\n&#x27;</span>)[<span class="number">1</span>:-<span class="number">1</span>]]</span><br><span class="line">    <span class="keyword">return</span> Encoder(</span><br><span class="line">        encoder=encoder,</span><br><span class="line">        bpe_merges=bpe_merges,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>



<h2 id="tiktoken库"><a href="#tiktoken库" class="headerlink" title="tiktoken库"></a>tiktoken库</h2><p>一个OpenAi的token库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line"><span class="comment">#GPT-2 (does not merge spaces)</span></span><br><span class="line">enc = tiktoken.get_encoding(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(enc.encode(<span class="string">&quot;    hello world!!!&quot;</span>))</span><br><span class="line"><span class="comment"># GPT-4（merges spaces)</span></span><br><span class="line">enc = tiktoken.get_encoding(<span class="string">&quot;cl100k_base&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(enc.encode(<span class="string">&quot;    hello world!!!&quot;</span>))</span><br></pre></td></tr></table></figure>



<p><a target="_blank" rel="noopener" href="https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py">tiktoken&#x2F;tiktoken_ext&#x2F;openai_public.py at main · openai&#x2F;tiktoken (github.com)</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tiktoken.load <span class="keyword">import</span> data_gym_to_mergeable_bpe_ranks, load_tiktoken_bpe</span><br><span class="line"></span><br><span class="line">ENDOFTEXT = <span class="string">&quot;&lt;|endoftext|&gt;&quot;</span></span><br><span class="line">FIM_PREFIX = <span class="string">&quot;&lt;|fim_prefix|&gt;&quot;</span></span><br><span class="line">FIM_MIDDLE = <span class="string">&quot;&lt;|fim_middle|&gt;&quot;</span></span><br><span class="line">FIM_SUFFIX = <span class="string">&quot;&lt;|fim_suffix|&gt;&quot;</span></span><br><span class="line">ENDOFPROMPT = <span class="string">&quot;&lt;|endofprompt|&gt;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gpt2</span>():</span><br><span class="line">    mergeable_ranks = data_gym_to_mergeable_bpe_ranks(</span><br><span class="line">        vocab_bpe_file=<span class="string">&quot;https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/vocab.bpe&quot;</span>,</span><br><span class="line">        encoder_json_file=<span class="string">&quot;https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/encoder.json&quot;</span>,</span><br><span class="line">        vocab_bpe_hash=<span class="string">&quot;1ce1664773c50f3e0cc8842619a93edc4624525b728b188a9e0be33b7726adc5&quot;</span>,</span><br><span class="line">        encoder_json_hash=<span class="string">&quot;196139668be63f3b5d6574427317ae82f612a97c5d1cdaf36ed2256dbf636783&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;gpt2&quot;</span>,</span><br><span class="line">        <span class="string">&quot;explicit_n_vocab&quot;</span>: <span class="number">50257</span>,</span><br><span class="line">        <span class="comment"># The pattern in the original GPT-2 release is:</span></span><br><span class="line">        <span class="comment"># r&quot;&quot;&quot;&#x27;s|&#x27;t|&#x27;re|&#x27;ve|&#x27;m|&#x27;ll|&#x27;d| ?[\p&#123;L&#125;]+| ?[\p&#123;N&#125;]+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># This is equivalent, but executes faster:</span></span><br><span class="line">        <span class="string">&quot;pat_str&quot;</span>: <span class="string">r&quot;&quot;&quot;&#x27;(?:[sdmt]|ll|ve|re)| ?\p&#123;L&#125;+| ?\p&#123;N&#125;+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;mergeable_ranks&quot;</span>: mergeable_ranks,</span><br><span class="line">        <span class="string">&quot;special_tokens&quot;</span>: &#123;ENDOFTEXT: <span class="number">50256</span>&#125;,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">r50k_base</span>():</span><br><span class="line">    mergeable_ranks = load_tiktoken_bpe(</span><br><span class="line">        <span class="string">&quot;https://openaipublic.blob.core.windows.net/encodings/r50k_base.tiktoken&quot;</span>,</span><br><span class="line">        expected_hash=<span class="string">&quot;306cd27f03c1a714eca7108e03d66b7dc042abe8c258b44c199a7ed9838dd930&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;r50k_base&quot;</span>,</span><br><span class="line">        <span class="string">&quot;explicit_n_vocab&quot;</span>: <span class="number">50257</span>,</span><br><span class="line">        <span class="string">&quot;pat_str&quot;</span>: <span class="string">r&quot;&quot;&quot;&#x27;(?:[sdmt]|ll|ve|re)| ?\p&#123;L&#125;+| ?\p&#123;N&#125;+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;mergeable_ranks&quot;</span>: mergeable_ranks,</span><br><span class="line">        <span class="string">&quot;special_tokens&quot;</span>: &#123;ENDOFTEXT: <span class="number">50256</span>&#125;,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p50k_base</span>():</span><br><span class="line">    mergeable_ranks = load_tiktoken_bpe(</span><br><span class="line">        <span class="string">&quot;https://openaipublic.blob.core.windows.net/encodings/p50k_base.tiktoken&quot;</span>,</span><br><span class="line">        expected_hash=<span class="string">&quot;94b5ca7dff4d00767bc256fdd1b27e5b17361d7b8a5f968547f9f23eb70d2069&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;p50k_base&quot;</span>,</span><br><span class="line">        <span class="string">&quot;explicit_n_vocab&quot;</span>: <span class="number">50281</span>,</span><br><span class="line">        <span class="string">&quot;pat_str&quot;</span>: <span class="string">r&quot;&quot;&quot;&#x27;(?:[sdmt]|ll|ve|re)| ?\p&#123;L&#125;+| ?\p&#123;N&#125;+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;mergeable_ranks&quot;</span>: mergeable_ranks,</span><br><span class="line">        <span class="string">&quot;special_tokens&quot;</span>: &#123;ENDOFTEXT: <span class="number">50256</span>&#125;,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">p50k_edit</span>():</span><br><span class="line">    mergeable_ranks = load_tiktoken_bpe(</span><br><span class="line">        <span class="string">&quot;https://openaipublic.blob.core.windows.net/encodings/p50k_base.tiktoken&quot;</span>,</span><br><span class="line">        expected_hash=<span class="string">&quot;94b5ca7dff4d00767bc256fdd1b27e5b17361d7b8a5f968547f9f23eb70d2069&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    special_tokens = &#123;ENDOFTEXT: <span class="number">50256</span>, FIM_PREFIX: <span class="number">50281</span>, FIM_MIDDLE: <span class="number">50282</span>, FIM_SUFFIX: <span class="number">50283</span>&#125;</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;p50k_edit&quot;</span>,</span><br><span class="line">        <span class="string">&quot;pat_str&quot;</span>: <span class="string">r&quot;&quot;&quot;&#x27;(?:[sdmt]|ll|ve|re)| ?\p&#123;L&#125;+| ?\p&#123;N&#125;+| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+|\s+(?!\S)|\s+&quot;&quot;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;mergeable_ranks&quot;</span>: mergeable_ranks,</span><br><span class="line">        <span class="string">&quot;special_tokens&quot;</span>: special_tokens,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cl100k_base</span>():</span><br><span class="line">    mergeable_ranks = load_tiktoken_bpe(</span><br><span class="line">        <span class="string">&quot;https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken&quot;</span>,</span><br><span class="line">        expected_hash=<span class="string">&quot;223921b76ee99bde995b7ff738513eef100fb51d18c93597a113bcffe865b2a7&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    special_tokens = &#123;</span><br><span class="line">        ENDOFTEXT: <span class="number">100257</span>,</span><br><span class="line">        FIM_PREFIX: <span class="number">100258</span>,</span><br><span class="line">        FIM_MIDDLE: <span class="number">100259</span>,</span><br><span class="line">        FIM_SUFFIX: <span class="number">100260</span>,</span><br><span class="line">        ENDOFPROMPT: <span class="number">100276</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;cl100k_base&quot;</span>,</span><br><span class="line">        <span class="string">&quot;pat_str&quot;</span>: <span class="string">r&quot;&quot;&quot;&#x27;(?i:[sdmt]|ll|ve|re)|[^\r\n\p&#123;L&#125;\p&#123;N&#125;]?+\p&#123;L&#125;+|\p&#123;N&#125;&#123;1,3&#125;| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+&quot;&quot;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;mergeable_ranks&quot;</span>: mergeable_ranks,</span><br><span class="line">        <span class="string">&quot;special_tokens&quot;</span>: special_tokens,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">o200k_base</span>():</span><br><span class="line">    mergeable_ranks = load_tiktoken_bpe(</span><br><span class="line">        <span class="string">&quot;https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken&quot;</span>,</span><br><span class="line">        expected_hash=<span class="string">&quot;446a9538cb6c348e3516120d7c08b09f57c36495e2acfffe59a5bf8b0cfb1a2d&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    special_tokens = &#123;</span><br><span class="line">        ENDOFTEXT: <span class="number">199999</span>,</span><br><span class="line">        ENDOFPROMPT: <span class="number">200018</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># This regex could be made more efficient</span></span><br><span class="line">    pat_str = <span class="string">&quot;|&quot;</span>.join(</span><br><span class="line">        [</span><br><span class="line">            <span class="string">r&quot;&quot;&quot;[^\r\n\p&#123;L&#125;\p&#123;N&#125;]?[\p&#123;Lu&#125;\p&#123;Lt&#125;\p&#123;Lm&#125;\p&#123;Lo&#125;\p&#123;M&#125;]*[\p&#123;Ll&#125;\p&#123;Lm&#125;\p&#123;Lo&#125;\p&#123;M&#125;]+(?i:&#x27;s|&#x27;t|&#x27;re|&#x27;ve|&#x27;m|&#x27;ll|&#x27;d)?&quot;&quot;&quot;</span>,</span><br><span class="line">            <span class="string">r&quot;&quot;&quot;[^\r\n\p&#123;L&#125;\p&#123;N&#125;]?[\p&#123;Lu&#125;\p&#123;Lt&#125;\p&#123;Lm&#125;\p&#123;Lo&#125;\p&#123;M&#125;]+[\p&#123;Ll&#125;\p&#123;Lm&#125;\p&#123;Lo&#125;\p&#123;M&#125;]*(?i:&#x27;s|&#x27;t|&#x27;re|&#x27;ve|&#x27;m|&#x27;ll|&#x27;d)?&quot;&quot;&quot;</span>,</span><br><span class="line">            <span class="string">r&quot;&quot;&quot;\p&#123;N&#125;&#123;1,3&#125;&quot;&quot;&quot;</span>,</span><br><span class="line">            <span class="string">r&quot;&quot;&quot; ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+[\r\n/]*&quot;&quot;&quot;</span>,</span><br><span class="line">            <span class="string">r&quot;&quot;&quot;\s*[\r\n]+&quot;&quot;&quot;</span>,</span><br><span class="line">            <span class="string">r&quot;&quot;&quot;\s+(?!\S)&quot;&quot;&quot;</span>,</span><br><span class="line">            <span class="string">r&quot;&quot;&quot;\s+&quot;&quot;&quot;</span>,</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;o200k_base&quot;</span>,</span><br><span class="line">        <span class="string">&quot;pat_str&quot;</span>: pat_str,</span><br><span class="line">        <span class="string">&quot;mergeable_ranks&quot;</span>: mergeable_ranks,</span><br><span class="line">        <span class="string">&quot;special_tokens&quot;</span>: special_tokens,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ENCODING_CONSTRUCTORS = &#123;</span><br><span class="line">    <span class="string">&quot;gpt2&quot;</span>: gpt2,</span><br><span class="line">    <span class="string">&quot;r50k_base&quot;</span>: r50k_base,</span><br><span class="line">    <span class="string">&quot;p50k_base&quot;</span>: p50k_base,</span><br><span class="line">    <span class="string">&quot;p50k_edit&quot;</span>: p50k_edit,</span><br><span class="line">    <span class="string">&quot;cl100k_base&quot;</span>: cl100k_base,</span><br><span class="line">    <span class="string">&quot;o200k_base&quot;</span>: o200k_base,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a>WordPiece</h2><p>WordPiece 是一种用于自然语言处理（NLP）的子词（subword）分词方法。它最初由日本电气公司（NEC）的 Schuster 和 Nakajima 在 2012 年提出，并在 Google 的机器翻译系统中得到了广泛应用。后来，WordPiece 被整合到 BERT（Bidirectional Encoder Representations from Transformers）等模型中，成为处理文本数据的重要工具。</p>
<h3 id="主要特点"><a href="#主要特点" class="headerlink" title="主要特点"></a>主要特点</h3><ol>
<li><p><strong>子词分词</strong>：</p>
<ul>
<li>WordPiece 将词汇表中的词分解为更小的子词单元。例如，单词 “unhappiness” 可能会被分解为 “un-“, “happi”, 和 “ness”。</li>
<li><strong>这种方法有助于处理未登录词（out-of-vocabulary, OOV）问题，因为即使遇到新词，模型也可以通过已知的子词单元来理解其含义。</strong></li>
</ul>
</li>
<li><p><strong>基于频率的训练</strong>：</p>
<ul>
<li>WordPiece 的词汇表是通过对训练数据进行统计分析来构建的。具体来说，它会根据子词单元的频率来决定哪些子词应该被包含在词汇表中。</li>
<li>这种方法类似于 BPE（Byte Pair Encoding），但 WordPiece 在选择子词时考虑了语言模型的概率。</li>
</ul>
</li>
<li><p><strong>灵活性</strong>：</p>
<ul>
<li>WordPiece 可以根据不同的任务和数据集进行调整。例如，BERT 模型使用 WordPiece 来处理英文文本，而其他语言模型可能会使用不同的分词方法。</li>
</ul>
</li>
<li><p><strong>高效性</strong>：</p>
<ul>
<li>由于 WordPiece 将词分解为子词，模型可以更高效地处理文本数据，尤其是在处理长尾词汇时。</li>
</ul>
</li>
<li><p><strong>步骤</strong></p>
<ol>
<li>Initialize the word unit inventory with all the characters in the text.</li>
<li>Build a language model on the training data using the inventory from 1.</li>
<li>Generate a new word unit by combining two units out of the current word inventory to increment the word unit inventory by one. Choose the new word unit out of all <strong>the possible ones that increases the likelihood on the training data the most</strong> when added to the model.</li>
<li>Goto 2 until a predefined limit of word units is reached or the likelihood increase falls below a certain threshold.</li>
</ol>
<p>In contrast to BPE, <strong>WordPiece does not choose the most frequent symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary</strong>. </p>
<p>E.g. <code>&quot;u&quot;</code>, followed by <code>&quot;g&quot;</code> would have only been merged if the probability of <code>&quot;ug&quot;</code> divided by <code>&quot;u&quot;</code>, <code>&quot;g&quot;</code> would have been greater than for any other symbol pair.</p>
</li>
</ol>
<h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><ul>
<li><strong>机器翻译</strong>：WordPiece 在 Google 的机器翻译系统中被广泛使用，因为它能够有效地处理多语言数据。</li>
<li><strong>预训练语言模型</strong>：BERT、DistilBERT 等模型使用 WordPiece 来处理输入文本，从而提高模型的泛化能力。</li>
<li><strong>文本分类</strong>：在文本分类任务中，WordPiece 可以帮助模型更好地理解复杂的词汇结构。</li>
</ul>
<h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>假设我们有一个句子：”I love natural language processing.”</p>
<p>使用 WordPiece 分词后，可能会得到如下结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&quot;I&quot;, &quot;love&quot;, &quot;natural&quot;, &quot;language&quot;, &quot;processing&quot;, &quot;.&quot;]</span><br></pre></td></tr></table></figure>

<p>如果词汇表中没有 “processing” 这个词，它可能会被分解为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&quot;I&quot;, &quot;love&quot;, &quot;natural&quot;, &quot;language&quot;, &quot;process&quot;, &quot;##ing&quot;, &quot;.&quot;]</span><br></pre></td></tr></table></figure>

<p>其中 “##ing” 表示这是一个子词单元，且它是一个词的后缀。</p>
<h2 id="SentencePiece"><a href="#SentencePiece" class="headerlink" title="SentencePiece"></a>SentencePiece</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.06226">https://arxiv.org/abs/1808.06226</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece">google&#x2F;sentencepiece: Unsupervised text tokenizer for Neural Network-based text generation. (github.com)</a></p>
<p>SentencePiece is a re-implementation of <strong>sub-word units</strong>, an effective way to alleviate the open vocabulary problems in neural machine translation. SentencePiece supports two segmentation algorithms, <strong>byte-pair-encoding (BPE)</strong> [<a target="_blank" rel="noopener" href="http://www.aclweb.org/anthology/P16-1162">Sennrich et al.</a>] and <strong>unigram language model</strong> [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.10959">Kudo.</a>]. Here are the high level differences from other implementations.</p>
<p>简而言之，是个Python库，是个工具。</p>
<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><h3 id="为何不采⽤one-hot向量"><a href="#为何不采⽤one-hot向量" class="headerlink" title="为何不采⽤one-hot向量"></a>为何不采⽤one-hot向量</h3><p>虽然one-hot词向量构造起来很容易，但通常并不是⼀个好选择。⼀个主要的原因是，one-hot词 向量⽆法准确表达不同词之间的相似度，如我们常常使⽤的余弦相似度。由于任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度难以通过onehot向量准确地体现出来。</p>
<p>Word2Vec 提出了两种不同的模型架构来实现词向量的生成：</p>
<ol>
<li>**Continuous Bag of Words (CBOW)**：在这个模型中，目标词的向量是从它周围的上下文词的向量平均得来的。换句话说，CBOW 模型试图根据一个词周围的词来预测这个词本身。</li>
<li><strong>Skip-gram</strong>：与 CBOW 不同，Skip-gram 模型试图根据一个目标词来预测它的上下文词。也就是说，它使用中心词的信息来推测周围词的信息。</li>
</ol>
<h3 id="Word2Vec-的优点"><a href="#Word2Vec-的优点" class="headerlink" title="Word2Vec 的优点"></a>Word2Vec 的优点</h3><ul>
<li><strong>语义相似性</strong>：Word2Vec 学习到的词向量能够捕捉词之间的语义相似性。例如，“king”和“queen”的向量之间的距离比“king”和“table”的距离更接近。</li>
<li><strong>向量运算</strong>：Word2Vec 支持向量运算，例如著名的例子 “king - man + woman ≈ queen”。</li>
<li><strong>高效性</strong>：Word2Vec 采用了一种称为负采样的技术，可以有效地在大规模数据集上训练。</li>
</ul>
<h3 id="Word2Vec-的局限性"><a href="#Word2Vec-的局限性" class="headerlink" title="Word2Vec 的局限性"></a>Word2Vec 的局限性</h3><p>尽管 Word2Vec 在很多方面表现优秀，但它也有一些局限性，比如：</p>
<ul>
<li><strong>一词多义</strong>：Word2Vec 为每个词提供了一个固定向量，无法很好地处理一词多义的问题。</li>
<li><strong>短语和多词表达</strong>：对于复杂的短语或固定搭配，Word2Vec 的效果可能不如针对短语专门训练的模型。</li>
</ul>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h2 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h2><h1 id="主流架构"><a href="#主流架构" class="headerlink" title="主流架构"></a>主流架构</h1><h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><h3 id="T5"><a href="#T5" class="headerlink" title="T5"></a>T5</h3><h3 id="BART-1"><a href="#BART-1" class="headerlink" title="BART"></a>BART</h3><h2 id="Encoder-Only"><a href="#Encoder-Only" class="headerlink" title="Encoder-Only"></a>Encoder-Only</h2><h3 id="BERT-1"><a href="#BERT-1" class="headerlink" title="BERT"></a>BERT</h3><p>BERT PyTorch开源个人实现：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://github.com/codertimo/BERT-pytorch">codertimo&#x2F;BERT-pytorch: Google AI 2018 BERT pytorch implementation (github.com)</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/coaxsoft/pytorch_bert">coaxsoft&#x2F;pytorch_bert: Tutorial for how to build BERT from scratch (github.com)</a></li>
</ol>
<p>教程：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://coaxsoft.com/blog/building-bert-with-pytorch-from-scratch">Building BERT with PyTorch from scratch (coaxsoft.com)</a></li>
<li><a target="_blank" rel="noopener" href="https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial">How to Code BERT Using PyTorch - Tutorial With Examples (neptune.ai)</a></li>
</ol>
<h2 id="Decoder-Only"><a href="#Decoder-Only" class="headerlink" title="Decoder-Only"></a>Decoder-Only</h2><h3 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h3><h3 id="Llama"><a href="#Llama" class="headerlink" title="Llama"></a>Llama</h3><h2 id="Prefix-Decoder"><a href="#Prefix-Decoder" class="headerlink" title="Prefix Decoder"></a>Prefix Decoder</h2><h3 id="ChatGLM"><a href="#ChatGLM" class="headerlink" title="ChatGLM"></a>ChatGLM</h3><h3 id="UNILM"><a href="#UNILM" class="headerlink" title="UNILM"></a>UNILM</h3><h1 id="NanoGPT项目"><a href="#NanoGPT项目" class="headerlink" title="NanoGPT项目"></a>NanoGPT项目</h1><h2 id="model"><a href="#model" class="headerlink" title="model"></a>model</h2><h3 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Full definition of a GPT Language Model, all of it in this single file.</span></span><br><span class="line"><span class="string">References:</span></span><br><span class="line"><span class="string">1) the official GPT-2 TensorFlow implementation released by OpenAI:</span></span><br><span class="line"><span class="string">https://github.com/openai/gpt-2/blob/master/src/model.py</span></span><br><span class="line"><span class="string">2) huggingface/transformers PyTorch implementation:</span></span><br><span class="line"><span class="string">https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure>

<h3 id="gelu"><a href="#gelu" class="headerlink" title="gelu"></a>gelu</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.jit.script</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fused_gelu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).</span></span><br><span class="line"><span class="string">    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * x * (<span class="number">1.0</span> + torch.tanh(math.sqrt(<span class="number">2.0</span> / math.pi) * (x + <span class="number">0.044715</span> * torch.<span class="built_in">pow</span>(x, <span class="number">3.0</span>))))</span><br></pre></td></tr></table></figure>

<p>$$<br>\text{GELU}(x)&#x3D;0.5<em>x</em>(1+\text{Tanh}(\sqrt{2&#x2F;\pi}<em>(x+0.044715</em>x^3)))<br>$$</p>
<p><img src="/.%5Cllm%5CGELU.png" alt="../_images/GELU.png"></p>
<p><code>@torch.jit.script</code>:<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.jit.script.html">torch.jit.script — PyTorch 2.4 documentation</a></p>
<p><code>@torch.jit.script</code> 是 PyTorch 中的一个装饰器，用于将 Python 函数或类方法编译成 TorchScript。TorchScript 是一种中间表示形式，可以在不依赖 Python 解释器的情况下执行，从而提高性能并实现模型部署的灵活性。</p>
<p>以下是 <code>@torch.jit.script</code> 的一些关键点：</p>
<ol>
<li><strong>性能优化</strong>：通过将 Python 代码编译成 TorchScript，可以消除 Python 解释器的开销，从而提高执行速度。</li>
<li><strong>部署灵活性</strong>：TorchScript 可以在没有 Python 依赖的环境中运行，便于在生产环境中部署模型。</li>
<li><strong>类型检查</strong>：TorchScript 在编译时进行类型检查，有助于捕获一些在运行时才会发现的错误。</li>
</ol>
<h3 id="CausalSelfAttention"><a href="#CausalSelfAttention" class="headerlink" title="CausalSelfAttention"></a>CausalSelfAttention</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CausalSelfAttention</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> config.n_embd % config.n_head == <span class="number">0</span></span><br><span class="line">        <span class="comment"># key, query, value projections for all heads, but in a batch</span></span><br><span class="line">        <span class="variable language_">self</span>.c_attn = nn.Linear(config.n_embd, <span class="number">3</span> * config.n_embd)</span><br><span class="line">        <span class="comment"># output projection</span></span><br><span class="line">        <span class="variable language_">self</span>.c_proj = nn.Linear(config.n_embd, config.n_embd)</span><br><span class="line">        <span class="comment"># regularization</span></span><br><span class="line">        <span class="variable language_">self</span>.attn_dropout = nn.Dropout(config.dropout)</span><br><span class="line">        <span class="variable language_">self</span>.resid_dropout = nn.Dropout(config.dropout)</span><br><span class="line">        <span class="comment"># causal mask to ensure that attention is only applied to the left in the input sequence</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;bias&quot;</span>, torch.tril(torch.ones(config.block_size, config.block_size))</span><br><span class="line">                             .view(<span class="number">1</span>, <span class="number">1</span>, config.block_size, config.block_size))</span><br><span class="line">        <span class="variable language_">self</span>.n_head = config.n_head</span><br><span class="line">        <span class="variable language_">self</span>.n_embd = config.n_embd</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, T, C = x.size()  <span class="comment"># batch size, sequence length, embedding dimensionality (n_embd)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># calculate query, key, values for all heads in batch and move head forward to be the batch dim</span></span><br><span class="line">        q, k, v = <span class="variable language_">self</span>.c_attn(x).split(<span class="variable language_">self</span>.n_embd, dim=<span class="number">2</span>)</span><br><span class="line">        k = k.view(B, T, <span class="variable language_">self</span>.n_head, C // <span class="variable language_">self</span>.n_head).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># (B, nh, T, hs)</span></span><br><span class="line">        q = q.view(B, T, <span class="variable language_">self</span>.n_head, C // <span class="variable language_">self</span>.n_head).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># (B, nh, T, hs)</span></span><br><span class="line">        v = v.view(B, T, <span class="variable language_">self</span>.n_head, C // <span class="variable language_">self</span>.n_head).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># (B, nh, T, hs)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)</span></span><br><span class="line">        att = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * (<span class="number">1.0</span> / math.sqrt(k.size(-<span class="number">1</span>)))</span><br><span class="line">        att = att.masked_fill(<span class="variable language_">self</span>.bias[:, :, :T, :T] == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))</span><br><span class="line">        att = F.softmax(att, dim=-<span class="number">1</span>)</span><br><span class="line">        att = <span class="variable language_">self</span>.attn_dropout(att)</span><br><span class="line">        y = att @ v  <span class="comment"># (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)</span></span><br><span class="line">        y = y.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(B, T, C)  <span class="comment"># re-assemble all head outputs side by side</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># output projection</span></span><br><span class="line">        y = <span class="variable language_">self</span>.resid_dropout(<span class="variable language_">self</span>.c_proj(y))</span><br><span class="line">        <span class="keyword">return</span> y  <span class="comment"># (B, T, C)</span></span><br></pre></td></tr></table></figure>

<h4 id="MultiHead-Attention"><a href="#MultiHead-Attention" class="headerlink" title="MultiHead Attention"></a>MultiHead Attention</h4><p>$$<br>\text{MultiHead}(Q, K, V) &#x3D; \text{Concat}(\text{head}<em>1,\dots,\text{head}<em>h)W^O\<br>\text{head}<em>i &#x3D; \text{Attention}(QW_i^Q,KW_i^K,VW_i^V)\<br>W_i^{Q,K}\in\R^{d</em>{model}\times d_k}\<br>W_i^V\in\R^{d</em>{model}\times d_v}\<br>W^O\in\R^{h d_v \times d</em>{model}}\<br>d_k&#x3D;d_v&#x3D;d_{model}&#x2F;h<br>$$</p>
<p>跳转到 42:12; 55:50</p>
<video id="myVideo" src="F:\Videos\Andrej Karpathy\Let's build GPT from scratch, in code, spelled out..mp4" controls data-start="10">
</video>


<h4 id="Andrej-Karpathy-解释"><a href="#Andrej-Karpathy-解释" class="headerlink" title="Andrej Karpathy 解释"></a>Andrej Karpathy 解释</h4><ul>
<li><p>第<code>t</code>位的token需要与前<code>t-1</code>个token做“交流”</p>
<p>一种交流方法是与前<code>t-1</code>个token做平均：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#We want x[b,t]= mean_&#123;i&lt;=t&#125; x[b,i]</span></span><br><span class="line">xbow = torch.zeros <span class="comment"># (B,T,C)</span></span><br><span class="line"><span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(B):</span><br><span class="line">	<span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">		xprev =x[b,:t+<span class="number">1</span>]<span class="comment">#（t,C)</span></span><br><span class="line">		xbow[b,t]= torch.mean(xprev,θ)</span><br></pre></td></tr></table></figure>

<p>其中，<code>xprev[t]</code>就是<code>x[0]</code>到<code>x[t]</code>的平均。</p>
</li>
<li><p>但是之前那样算效率低，使用矩阵乘法可以更快。</p>
<ul>
<li><p>首先可以使用全是1的矩阵，算出每一列的总和（token中每一个channel的和）</p>
<img src="\llm\image-20240905112007978.png" alt="image-20240905112007978" style="zoom: 50%;" />
</li>
<li><p>然后，将全一矩阵变成下三角的矩阵，就可以算出每个token与之前token在每一个channel上的和</p>
<img src="\llm\image-20240905112222983.png" alt="image-20240905112222983" style="zoom:50%;" />
</li>
<li><p>最后，再将下三角的矩阵的每一行除以自己每一行的和，就可以算出每个token与自己之前所有token的均值。</p>
<p><code>torch.sum(a, 1, keepdim=True)</code>会算出<code>a</code>每一行的和，然后<code>a / torch.sum(a, 1, keepdim=True)</code> 会使用广播机制，最后做按元素的除法（element-wise）。</p>
<img src="\llm\image-20240905112500588.png" alt="image-20240905112500588" style="zoom:50%;" />
</li>
<li><p>最后，这个矩阵中的每个数字就代表一种权重，把权重不按平均二是按与当前token的点积决定，使用softmax换成概率，就变成的self-attention。只是一般的Self-Attention公式没有包含掩码处理，这里针对于LLM直接就用上了掩码。</p>
</li>
</ul>
</li>
</ul>
<p><code>q, k, v = self.c_attn(x).split(self.n_embd, dim=2)</code>解释：</p>
<p>x: (B, T, C)</p>
<p>c_attn: (C, 3 * C)</p>
<p>c_attn(x):  (B, T, 3C)</p>
<p>总而言之，复制三份分别乘以3个不同的（C, C ）大小的矩阵，可以简化为直接乘以一个（C, 3 * C）大小的矩阵然后再分开，而且QKV对应的部分不会相互影响。</p>
<img src="\llm\image-20240820232826059.png" alt="image-20240820232826059" style="zoom:50%;" />

<p>不仅合并QKV三个，还可以QKV对应的几个头的矩阵（）</p>
<img src="\llm\image-20240821000323484.png" alt="image-20240821000323484" style="zoom:50%;" />


<h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.c_fc = nn.Linear(config.n_embd, <span class="number">4</span> * config.n_embd)</span><br><span class="line">        <span class="variable language_">self</span>.c_proj = nn.Linear(<span class="number">4</span> * config.n_embd, config.n_embd)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(config.dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.c_fc(x)</span><br><span class="line">        x = fused_gelu(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.c_proj(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.dropout(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="Block"><a href="#Block" class="headerlink" title="Block"></a>Block</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.ln_1 = nn.LayerNorm(config.n_embd)</span><br><span class="line">        <span class="variable language_">self</span>.attn = CausalSelfAttention(config)</span><br><span class="line">        <span class="variable language_">self</span>.ln_2 = nn.LayerNorm(config.n_embd) </span><br><span class="line">        <span class="variable language_">self</span>.mlp = MLP(config)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):   <span class="comment"># (B, T, C)</span></span><br><span class="line">        x = x + <span class="variable language_">self</span>.attn(<span class="variable language_">self</span>.ln_1(x))</span><br><span class="line">        x = x + <span class="variable language_">self</span>.mlp(<span class="variable language_">self</span>.ln_2(x))</span><br><span class="line">        <span class="keyword">return</span> x   <span class="comment"># (B, T, C)</span></span><br></pre></td></tr></table></figure>

<p>现在的做法都是先做 Layer norm</p>
<h3 id="GPTConfig"><a href="#GPTConfig" class="headerlink" title="GPTConfig"></a>GPTConfig</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GPTConfig</span>:</span><br><span class="line">    block_size: <span class="built_in">int</span> = <span class="number">1024</span></span><br><span class="line">    vocab_size: <span class="built_in">int</span> = <span class="number">50257</span></span><br><span class="line">    n_layer: <span class="built_in">int</span> = <span class="number">12</span></span><br><span class="line">    n_head: <span class="built_in">int</span> = <span class="number">12</span></span><br><span class="line">    n_embd: <span class="built_in">int</span> = <span class="number">768</span></span><br><span class="line">    dropout: <span class="built_in">float</span> = <span class="number">0.1</span></span><br></pre></td></tr></table></figure>

<h3 id="GPT-1"><a href="#GPT-1" class="headerlink" title="GPT"></a>GPT</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPT</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, idx, targets=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">crop_block_size</span>(<span class="params">self, block_size</span>):</span><br><span class="line">        <span class="comment"># model surgery to decrease the block size if necessary</span></span><br><span class="line">        <span class="comment"># e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)</span></span><br><span class="line">        <span class="comment"># but want to use a smaller block size for some smaller, simpler model</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_pretrained</span>(<span class="params">cls, model_type</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self, weight_decay, learning_rate, betas</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This long function is unfortunately doing something very simple and is being very defensive:</span></span><br><span class="line"><span class="string">        We are separating out all parameters of the model into two buckets: those that will experience</span></span><br><span class="line"><span class="string">        weight decay for regularization and those that won&#x27;t (biases, and layernorm/embedding weights).</span></span><br><span class="line"><span class="string">        We are then returning the PyTorch optimizer object.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, idx, max_new_tokens, temperature=<span class="number">1.0</span>, top_k=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<h3 id="GPT-init"><a href="#GPT-init" class="headerlink" title="GPT.__init__"></a>GPT.__init__</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPT</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> config.vocab_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">assert</span> config.block_size <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.block_size = config.block_size</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.transformer = nn.ModuleDict(<span class="built_in">dict</span>(</span><br><span class="line">            <span class="comment"># word token embedding</span></span><br><span class="line">            wte=nn.Embedding(config.vocab_size, config.n_embd),</span><br><span class="line">            <span class="comment"># word position embedding</span></span><br><span class="line">            wpe=nn.Embedding(config.block_size, config.n_embd),</span><br><span class="line">            drop=nn.Dropout(config.dropout),</span><br><span class="line">            h=nn.ModuleList([Block(config) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(config.n_layer)]),</span><br><span class="line">            ln_f=nn.LayerNorm(config.n_embd),</span><br><span class="line">        ))</span><br><span class="line">        <span class="variable language_">self</span>.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># report number of parameters (note we don&#x27;t count the decoder parameters in lm_head)</span></span><br><span class="line">        n_params = <span class="built_in">sum</span>(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> <span class="variable language_">self</span>.transformer.parameters())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;number of parameters: %.2fM&quot;</span> % (n_params / <span class="number">1e6</span>,))</span><br></pre></td></tr></table></figure>



<h3 id="GPT-forward"><a href="#GPT-forward" class="headerlink" title="GPT.forward"></a>GPT.forward</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPT</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, idx, targets=<span class="literal">None</span></span>):</span><br><span class="line">        device = idx.device</span><br><span class="line">        b, t = idx.size()</span><br><span class="line">        <span class="keyword">assert</span> t &lt;= <span class="variable language_">self</span>.block_size, <span class="string">f&quot;Cannot forward sequence of length <span class="subst">&#123;t&#125;</span>, block size is only <span class="subst">&#123;self.block_size&#125;</span>&quot;</span></span><br><span class="line">        pos = torch.arange(<span class="number">0</span>, t, dtype=torch.long, device=device).unsqueeze(<span class="number">0</span>)  <span class="comment"># shape (1, t)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward the GPT model itself</span></span><br><span class="line">        tok_emb = <span class="variable language_">self</span>.transformer.wte(idx)  <span class="comment"># token embeddings of shape (b, t, n_embd)</span></span><br><span class="line">        pos_emb = <span class="variable language_">self</span>.transformer.wpe(pos)  <span class="comment"># position embeddings of shape (1, t, n_embd)</span></span><br><span class="line">        x = <span class="variable language_">self</span>.transformer.drop(tok_emb + pos_emb)  <span class="comment"># (B, T, C)</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.transformer.h:</span><br><span class="line">            x = block(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.transformer.ln_f(x)  <span class="comment"># (B, T, C)</span></span><br><span class="line">        logits = <span class="variable language_">self</span>.lm_head(x)  <span class="comment"># (B, T, vocab_size)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># if we are given some desired targets also calculate the loss</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss = F.cross_entropy(logits.view(-<span class="number">1</span>, logits.size(-<span class="number">1</span>)), targets.view(-<span class="number">1</span>), ignore_index=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits, loss</span><br></pre></td></tr></table></figure>





<h3 id="GPT-generate"><a href="#GPT-generate" class="headerlink" title="GPT.generate"></a>GPT.generate</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPT</span>(nn.Module):</span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, idx, max_new_tokens, temperature=<span class="number">1.0</span>, top_k=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete</span></span><br><span class="line"><span class="string">        the sequence max_new_tokens times, feeding the predictions back into the model each time.</span></span><br><span class="line"><span class="string">        Most likely you&#x27;ll want to make sure to be in model.eval() mode of operation for this.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">            <span class="comment"># if the sequence context is growing too long we must crop it at block_size</span></span><br><span class="line">            idx_cond = idx <span class="keyword">if</span> idx.size(<span class="number">1</span>) &lt;= <span class="variable language_">self</span>.block_size <span class="keyword">else</span> idx[:, -<span class="variable language_">self</span>.block_size:]</span><br><span class="line">            <span class="comment"># forward the model to get the logits for the index in the sequence</span></span><br><span class="line">            logits, _ = <span class="variable language_">self</span>(idx_cond)  <span class="comment"># (B, T, vocab_size)</span></span><br><span class="line">            <span class="comment"># pluck the logits at the final step and scale by desired temperature</span></span><br><span class="line">            logits = logits[:, -<span class="number">1</span>, :] / temperature  <span class="comment"># (B, 1, vocab_size)</span></span><br><span class="line">            <span class="comment"># optionally crop the logits to only the top k options</span></span><br><span class="line">            <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                v, _ = torch.topk(logits, top_k)</span><br><span class="line">                logits[logits &lt; v[:, [-<span class="number">1</span>]]] = -<span class="built_in">float</span>(<span class="string">&#x27;Inf&#x27;</span>)</span><br><span class="line">            <span class="comment"># apply softmax to convert logits to (normalized) probabilities</span></span><br><span class="line">            probs = F.softmax(logits, dim=-<span class="number">1</span>)  <span class="comment"># (B, 1, vocab_size)</span></span><br><span class="line">            <span class="comment"># sample from the distribution</span></span><br><span class="line">            idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>)  <span class="comment"># (B, 1, 1)</span></span><br><span class="line">            <span class="comment"># append sampled index to the running sequence and continue</span></span><br><span class="line">            idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> idx</span><br></pre></td></tr></table></figure>

<h4 id="temperature"><a href="#temperature" class="headerlink" title="temperature"></a>temperature</h4><p>在自然语言生成任务中，<code>temperature</code> 参数<strong>用于控制生成文本的随机性</strong>。这个参数通过调整模型输出的 logits（未归一化的概率分布）来影响最终生成的文本。通过调整 <code>temperature</code> 参数，可以灵活地控制生成文本的随机性。较高的 <code>temperature</code> 会增加随机性，使生成的文本更加多样化；较低的 <code>temperature</code> 会减少随机性，使生成的文本更加确定和一致。</p>
<p>具体来说，除以 <code>temperature</code> 可以实现以下效果：</p>
<ol>
<li><p>增加随机性（高 temperature）</p>
<p>当 <code>temperature</code> 设置为一个大于 1 的值时，除以 <code>temperature</code> 会使 logits 的值变小。这会导致 softmax 函数输出的概率分布更加平滑，即各个词的概率差异变小。因此，采样时选择每个词的概率更加接近，生成的文本更具随机性。</p>
</li>
<li><p>减少随机性（低 temperature）</p>
<p>当 <code>temperature</code> 设置为一个小于 1 的值时，除以 <code>temperature</code> 会使 logits 的值变大。这会导致 softmax 函数输出的概率分布更加尖锐，即各个词的概率差异变大。因此，采样时选择概率最高的词的概率更大，生成的文本更具确定性。</p>
</li>
<li><p>保持原始分布（temperature &#x3D; 1）</p>
<p>当 <code>temperature</code> 设置为 1 时，除以 <code>temperature</code> 不会改变 logits 的值。因此，softmax 函数输出的概率分布保持不变，生成的文本保持原始的随机性。</p>
</li>
</ol>
<p><strong>数学解释</strong></p>
<p>假设模型输出的 logits 为 ( z )，softmax 函数的输出为 ( p )，则有：<br>$$<br> p_i &#x3D; \frac{e^{z_i &#x2F; \text{temperature}}}{\sum_j e^{z_j &#x2F; \text{temperature}}}<br>$$<br>当 <code>temperature</code> 增加时，<br>$$<br>z_i &#x2F; \text{temperature}<br>$$<br> 变小，指数函数的值变小，导致 p_i 更加平滑。反之，当 <code>temperature</code> 减小时，<br>$$<br>z_i &#x2F; \text{temperature}<br>$$<br>  变大，指数函数的值变大，导致  p_i  更加尖锐。</p>
<p><strong>代码示例</strong></p>
<p>在代码中，除以 <code>temperature</code> 的操作如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logits = logits[:, -<span class="number">1</span>, :] / temperature</span><br></pre></td></tr></table></figure>

<h4 id="TopK"><a href="#TopK" class="headerlink" title="TopK"></a>TopK</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    v, _ = torch.topk(logits, top_k)</span><br><span class="line">    logits[logits &lt; v[:, [-<span class="number">1</span>]]] = -<span class="built_in">float</span>(<span class="string">&#x27;Inf&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>如果指定了 <code>top_k</code>，则只保留 logits 中最大的 <code>top_k</code> 个值，其余值设为负无穷，以限制生成时考虑的词数。</li>
</ul>
<p>在生成式语言模型（如GPT、BERT等）中，<strong>top-k采样</strong>（top-k sampling）是一种常用的解码策略，用于控制生成文本的多样性和质量。top-k参数的作用是限制模型在生成下一个词时考虑的候选词的数量，从而避免生成低概率、不合理的词。</p>
<h5 id="为什么使用top-k参数？"><a href="#为什么使用top-k参数？" class="headerlink" title="为什么使用top-k参数？"></a>为什么使用top-k参数？</h5><ol>
<li><p><strong>控制生成文本的多样性</strong>：</p>
<ul>
<li>在生成文本时，模型会为每个位置生成一个概率分布，表示每个词被选中的概率。<strong>如果没有限制，模型可能会选择概率非常低的词，导致生成的文本不连贯或不合理</strong>。</li>
<li>top-k采样通过只考虑概率最高的k个词，限制了生成词的范围，从而提高了生成文本的连贯性和合理性。</li>
</ul>
</li>
<li><p><strong>避免低概率词的生成</strong>：</p>
<ul>
<li>在某些情况下，模型可能会为某些词分配非常低的概率，这些词通常是不合理的或不符合上下文的。top-k采样可以有效地过滤掉这些低概率词，避免它们被选中。</li>
</ul>
</li>
<li><p><strong>提高生成文本的质量</strong>：</p>
<ul>
<li>通过限制候选词的数量，top-k采样可以减少生成文本中的噪声，提高生成文本的整体质量。</li>
</ul>
</li>
</ol>
<h4 id="TopP"><a href="#TopP" class="headerlink" title="TopP"></a>TopP</h4><p><strong>top-p采样</strong>（top-p sampling），也称为<strong>nucleus采样</strong>（nucleus sampling）。top-p采样通过动态选择概率最高的词，直到这些词的概率累积达到一个预设的阈值（通常是0.9或0.95），从而控制生成文本的多样性和质量。</p>
<h5 id="为什么使用top-p参数？"><a href="#为什么使用top-p参数？" class="headerlink" title="为什么使用top-p参数？"></a>为什么使用top-p参数？</h5><ol>
<li><p><strong>动态调整候选词的数量</strong>：</p>
<ul>
<li>top-k采样固定了候选词的数量，而top-p采样则根据概率分布动态调整候选词的数量。这意味着在某些情况下，模型可能会考虑更多的词，而在其他情况下则考虑更少的词。</li>
</ul>
</li>
<li><p><strong>提高生成文本的多样性</strong>：</p>
<ul>
<li>top-p采样允许模型在某些情况下选择更多的词，从而提高生成文本的多样性。这对于生成更具创意和多样性的文本非常有用。</li>
</ul>
</li>
<li><p><strong>避免低概率词的生成</strong>：</p>
<ul>
<li>与top-k采样类似，top-p采样也可以避免生成低概率、不合理的词，从而提高生成文本的质量。</li>
</ul>
</li>
</ol>
<h5 id="top-p采样的工作原理"><a href="#top-p采样的工作原理" class="headerlink" title="top-p采样的工作原理"></a>top-p采样的工作原理</h5><ol>
<li><p><strong>计算概率分布</strong>：</p>
<ul>
<li>模型在生成每个词时，会计算所有可能词的概率分布。</li>
</ul>
</li>
<li><p><strong>排序并累积概率</strong>：</p>
<ul>
<li>将所有可能的词按概率从高到低排序，并计算累积概率。</li>
</ul>
</li>
<li><p><strong>选择累积概率达到p的词</strong>：</p>
<ul>
<li>选择累积概率达到预设阈值p的词作为候选词。例如，如果p&#x3D;0.9，那么选择累积概率达到0.9的词。</li>
</ul>
</li>
<li><p><strong>重新归一化</strong>：</p>
<ul>
<li>将这些候选词的概率重新归一化，使得它们的概率和为1。</li>
</ul>
</li>
<li><p><strong>采样</strong>：</p>
<ul>
<li>从这些候选词中，根据重新归一化后的概率分布进行采样，选择下一个生成的词。</li>
</ul>
</li>
</ol>
<h5 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h5><p>假设模型在生成下一个词时，计算出以下概率分布：</p>
<p>$$<br>\text{词1: 0.4}, \quad \text{词2: 0.3}, \quad \text{词3: 0.2}, \quad \text{词4: 0.05}, \quad \text{词5: 0.03}, \quad \text{词6: 0.02}<br>$$<br>如果设置top-p参数为0.9，那么模型会选择累积概率达到0.9的词：</p>
<p>$$<br>\text{词1: 0.4}, \quad \text{词2: 0.3}, \quad \text{词3: 0.2}<br>$$<br>累积概率为：</p>
<p>$$<br>0.4 + 0.3 + 0.2 &#x3D; 0.9<br>$$<br>然后，重新归一化这三个词的概率：</p>
<p>$$<br>\text{词1: 0.44}, \quad \text{词2: 0.33}, \quad \text{词3: 0.22}<br>$$<br>最后，从这三个词中根据新的概率分布进行采样，选择下一个生成的词。</p>
<h3 id="GPT-crop-block-size"><a href="#GPT-crop-block-size" class="headerlink" title="GPT.crop_block_size"></a>GPT.crop_block_size</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPT</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">crop_block_size</span>(<span class="params">self, block_size</span>):</span><br><span class="line">        <span class="comment"># model surgery to decrease the block size if necessary</span></span><br><span class="line">        <span class="comment"># e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)</span></span><br><span class="line">        <span class="comment"># but want to use a smaller block size for some smaller, simpler model</span></span><br><span class="line">        <span class="keyword">assert</span> block_size &lt;= <span class="variable language_">self</span>.block_size</span><br><span class="line">        <span class="variable language_">self</span>.block_size = block_size</span><br><span class="line">        <span class="variable language_">self</span>.transformer.wpe.weight = nn.Parameter(<span class="variable language_">self</span>.transformer.wpe.weight[:block_size])</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>.transformer.h:</span><br><span class="line">            block.attn.bias = block.attn.bias[:, :, :block_size, :block_size]</span><br></pre></td></tr></table></figure>

<p>控制一句话中token的个数，进入模型的数据token个数不能大于block_size，大于block_size会被截断只保留最后block_size大小的token数量。</p>
<p>block_size还会用在word position embedding层</p>
<h3 id="GPT-from-pretrained"><a href="#GPT-from-pretrained" class="headerlink" title="GPT.from_pretrained"></a>GPT.from_pretrained</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPT</span>(nn.Module):</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_pretrained</span>(<span class="params">cls, model_type</span>):</span><br><span class="line">        <span class="keyword">assert</span> model_type <span class="keyword">in</span> &#123;<span class="string">&#x27;gpt2&#x27;</span>, <span class="string">&#x27;gpt2-medium&#x27;</span>, <span class="string">&#x27;gpt2-large&#x27;</span>, <span class="string">&#x27;gpt2-xl&#x27;</span>&#125;</span><br><span class="line">        <span class="keyword">from</span> transformers <span class="keyword">import</span> GPT2LMHeadModel</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;loading weights from pretrained gpt: %s&quot;</span> % model_type)</span><br><span class="line"></span><br><span class="line">        layer_config = &#123;</span><br><span class="line">            <span class="string">&#x27;gpt2&#x27;</span>: <span class="built_in">dict</span>(n_layer=<span class="number">12</span>, n_head=<span class="number">12</span>, n_embd=<span class="number">768</span>),  <span class="comment"># 124M params</span></span><br><span class="line">            <span class="string">&#x27;gpt2-medium&#x27;</span>: <span class="built_in">dict</span>(n_layer=<span class="number">24</span>, n_head=<span class="number">16</span>, n_embd=<span class="number">1024</span>),  <span class="comment"># 350M params</span></span><br><span class="line">            <span class="string">&#x27;gpt2-large&#x27;</span>: <span class="built_in">dict</span>(n_layer=<span class="number">36</span>, n_head=<span class="number">20</span>, n_embd=<span class="number">1280</span>),  <span class="comment"># 774M params</span></span><br><span class="line">            <span class="string">&#x27;gpt2-xl&#x27;</span>: <span class="built_in">dict</span>(n_layer=<span class="number">48</span>, n_head=<span class="number">25</span>, n_embd=<span class="number">1600</span>),  <span class="comment"># 1558M params</span></span><br><span class="line">        &#125;[model_type]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># create a from-scratch initialized minGPT model</span></span><br><span class="line">        config = GPTConfig(block_size=<span class="number">1024</span>, **layer_config)</span><br><span class="line">        model = GPT(config)</span><br><span class="line">        sd = model.state_dict()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># init a huggingface/transformers model</span></span><br><span class="line">        model_hf = GPT2LMHeadModel.from_pretrained(model_type)</span><br><span class="line">        sd_hf = model_hf.state_dict()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># copy while ensuring all of the parameters are aligned and match in names and shapes</span></span><br><span class="line">        keys = [k <span class="keyword">for</span> k <span class="keyword">in</span> sd_hf <span class="keyword">if</span> <span class="keyword">not</span> k.endswith(<span class="string">&#x27;attn.masked_bias&#x27;</span>)]  <span class="comment"># ignore these</span></span><br><span class="line">        transposed = [<span class="string">&#x27;attn.c_attn.weight&#x27;</span>, <span class="string">&#x27;attn.c_proj.weight&#x27;</span>, <span class="string">&#x27;mlp.c_fc.weight&#x27;</span>, <span class="string">&#x27;mlp.c_proj.weight&#x27;</span>]</span><br><span class="line">        <span class="comment"># basically the openai checkpoints use a &quot;Conv1D&quot; module, but we only want to use a vanilla Linear</span></span><br><span class="line">        <span class="comment"># this means that we have to transpose these weights when we import them</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(keys) == <span class="built_in">len</span>(sd)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> keys:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">any</span>(k.endswith(w) <span class="keyword">for</span> w <span class="keyword">in</span> transposed):</span><br><span class="line">                <span class="comment"># special treatment for the Conv1D weights we need to transpose</span></span><br><span class="line">                <span class="keyword">assert</span> sd_hf[k].shape[::-<span class="number">1</span>] == sd[k].shape</span><br><span class="line">                <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                    sd[k].copy_(sd_hf[k].t())</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># vanilla copy over the other parameters</span></span><br><span class="line">                <span class="keyword">assert</span> sd_hf[k].shape == sd[k].shape</span><br><span class="line">                <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                    sd[k].copy_(sd_hf[k])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<h3 id="GPT-configure-optimizers"><a href="#GPT-configure-optimizers" class="headerlink" title="GPT.configure_optimizers"></a>GPT.configure_optimizers</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPT</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self, weight_decay, learning_rate, betas</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        This long function is unfortunately doing something very simple and is being very defensive:</span></span><br><span class="line"><span class="string">        We are separating out all parameters of the model into two buckets: those that will experience</span></span><br><span class="line"><span class="string">        weight decay for regularization and those that won&#x27;t (biases, and layernorm/embedding weights).</span></span><br><span class="line"><span class="string">        We are then returning the PyTorch optimizer object.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># separate out all parameters to those that will and won&#x27;t experience regularizing weight decay</span></span><br><span class="line">        decay = <span class="built_in">set</span>()</span><br><span class="line">        no_decay = <span class="built_in">set</span>()</span><br><span class="line">        whitelist_weight_modules = (torch.nn.Linear,)</span><br><span class="line">        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)</span><br><span class="line">        <span class="keyword">for</span> mn, m <span class="keyword">in</span> <span class="variable language_">self</span>.named_modules():</span><br><span class="line">            <span class="keyword">for</span> pn, p <span class="keyword">in</span> m.named_parameters():</span><br><span class="line">                fpn = <span class="string">&#x27;%s.%s&#x27;</span> % (mn, pn) <span class="keyword">if</span> mn <span class="keyword">else</span> pn  <span class="comment"># full param name</span></span><br><span class="line">                <span class="comment"># random note: because named_modules and named_parameters are recursive</span></span><br><span class="line">                <span class="comment"># we will see the same tensors p many many times. but doing it this way</span></span><br><span class="line">                <span class="comment"># allows us to know which parent module any tensor p belongs to...</span></span><br><span class="line">                <span class="keyword">if</span> pn.endswith(<span class="string">&#x27;bias&#x27;</span>):</span><br><span class="line">                    <span class="comment"># all biases will not be decayed</span></span><br><span class="line">                    no_decay.add(fpn)</span><br><span class="line">                <span class="keyword">elif</span> pn.endswith(<span class="string">&#x27;weight&#x27;</span>) <span class="keyword">and</span> <span class="built_in">isinstance</span>(m, whitelist_weight_modules):</span><br><span class="line">                    <span class="comment"># weights of whitelist modules will be weight decayed</span></span><br><span class="line">                    decay.add(fpn)</span><br><span class="line">                <span class="keyword">elif</span> pn.endswith(<span class="string">&#x27;weight&#x27;</span>) <span class="keyword">and</span> <span class="built_in">isinstance</span>(m, blacklist_weight_modules):</span><br><span class="line">                    <span class="comment"># weights of blacklist modules will NOT be weight decayed</span></span><br><span class="line">                    no_decay.add(fpn)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># validate that we considered every parameter</span></span><br><span class="line">        param_dict = &#123;pn: p <span class="keyword">for</span> pn, p <span class="keyword">in</span> <span class="variable language_">self</span>.named_parameters()&#125;</span><br><span class="line">        inter_params = decay &amp; no_decay</span><br><span class="line">        union_params = decay | no_decay</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(inter_params) == <span class="number">0</span>, <span class="string">&quot;parameters %s made it into both decay/no_decay sets!&quot;</span> % (<span class="built_in">str</span>(inter_params),)</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(</span><br><span class="line">            param_dict.keys() - union_params) == <span class="number">0</span>, <span class="string">&quot;parameters %s were not separated into either decay/no_decay set!&quot;</span> \</span><br><span class="line">                                                    % (<span class="built_in">str</span>(param_dict.keys() - union_params),)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># create the pytorch optimizer object</span></span><br><span class="line">        optim_groups = [</span><br><span class="line">            &#123;<span class="string">&quot;params&quot;</span>: [param_dict[pn] <span class="keyword">for</span> pn <span class="keyword">in</span> <span class="built_in">sorted</span>(<span class="built_in">list</span>(decay))], <span class="string">&quot;weight_decay&quot;</span>: weight_decay&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;params&quot;</span>: [param_dict[pn] <span class="keyword">for</span> pn <span class="keyword">in</span> <span class="built_in">sorted</span>(<span class="built_in">list</span>(no_decay))], <span class="string">&quot;weight_decay&quot;</span>: <span class="number">0.0</span>&#125;,</span><br><span class="line">        ]</span><br><span class="line">        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas)</span><br><span class="line">        <span class="keyword">return</span> optimizer</span><br></pre></td></tr></table></figure>

<h2 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h2><h3 id="导入包-1"><a href="#导入包-1" class="headerlink" title="导入包"></a>导入包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Train a GPT model on a dataset of text. One GPU version.</span></span><br><span class="line"><span class="string">The text is assumed to pre-tokenized and inside files train.pt and val.pt</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> ast <span class="keyword">import</span> literal_eval</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> wandb</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> GPTConfig, GPT</span><br></pre></td></tr></table></figure>

<h3 id="默认配置"><a href="#默认配置" class="headerlink" title="默认配置"></a>默认配置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default config values</span></span><br><span class="line"><span class="comment"># I/O</span></span><br><span class="line">out_dir = <span class="string">&#x27;out&#x27;</span></span><br><span class="line">eval_interval = <span class="number">500</span></span><br><span class="line">log_interval = <span class="number">1</span></span><br><span class="line">eval_iters = <span class="number">50</span></span><br><span class="line">eval_only = <span class="literal">False</span> <span class="comment"># if True, script exits right after the first eval</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># wandb logging</span></span><br><span class="line">wandb_log = <span class="literal">False</span> <span class="comment"># disabled by default</span></span><br><span class="line">wandb_entity = <span class="string">&#x27;karpathy&#x27;</span></span><br><span class="line">wandb_project = <span class="string">&#x27;owt&#x27;</span></span><br><span class="line">wandb_run_name = <span class="string">&#x27;gpt2&#x27;</span> <span class="comment"># &#x27;run&#x27; + str(time.time())</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># data</span></span><br><span class="line">dataset = <span class="string">&#x27;openwebtext&#x27;</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">block_size = <span class="number">1024</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">device = <span class="string">&#x27;cuda:0&#x27;</span></span><br><span class="line">init_from = <span class="string">&#x27;scratch&#x27;</span> <span class="comment"># &#x27;scratch&#x27; or &#x27;resume&#x27; or &#x27;gpt2*&#x27;</span></span><br><span class="line">dropout = <span class="number">0.1</span></span><br><span class="line">n_layer = <span class="number">12</span></span><br><span class="line">n_head = <span class="number">12</span></span><br><span class="line">n_embd = <span class="number">768</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># adamw optimizer</span></span><br><span class="line">learning_rate = <span class="number">2.5e-4</span> <span class="comment"># max learning rate</span></span><br><span class="line">max_iters = <span class="number">500000</span> <span class="comment"># total number of training iterations</span></span><br><span class="line">weight_decay = <span class="number">1e-2</span></span><br><span class="line">betas = (<span class="number">0.9</span>, <span class="number">0.95</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># learning rate decay settings</span></span><br><span class="line">decay_lr = <span class="literal">True</span> <span class="comment"># whether to decay the learning rate</span></span><br><span class="line">warmup_iters = <span class="number">2000</span> <span class="comment"># how many steps to warm up for</span></span><br><span class="line">lr_decay_iters = <span class="number">320000</span> <span class="comment"># how many steps to decay the learning rate for</span></span><br><span class="line">min_lr = <span class="number">1e-5</span> <span class="comment"># minimum learning rate</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -----------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># poor man&#x27;s Configurator. Potentially a bad idea. Example usage:</span></span><br><span class="line"><span class="comment"># python train.py override_file --batch_size=32</span></span><br><span class="line"><span class="comment"># this will first run config/override_file.py, then override batch_size to 32</span></span><br><span class="line"><span class="keyword">for</span> arg <span class="keyword">in</span> sys.argv[<span class="number">1</span>:]:</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;=&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> arg:</span><br><span class="line">        <span class="comment"># assume it&#x27;s the name of a config file</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> arg.startswith(<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">        config_file = os.path.join(<span class="string">&#x27;config&#x27;</span>, arg + <span class="string">&#x27;.py&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Overriding config with <span class="subst">&#123;config_file&#125;</span>:&quot;</span>)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(config_file) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="built_in">print</span>(f.read())</span><br><span class="line">        <span class="built_in">exec</span>(<span class="built_in">open</span>(config_file).read())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># assume it&#x27;s a --key=value argument</span></span><br><span class="line">        <span class="keyword">assert</span> arg.startswith(<span class="string">&#x27;--&#x27;</span>)</span><br><span class="line">        key, val = arg.split(<span class="string">&#x27;=&#x27;</span>)</span><br><span class="line">        key = key[<span class="number">2</span>:]</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">in</span> <span class="built_in">globals</span>():</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="comment"># attempt to eval it it (e.g. if bool, number, or etc)</span></span><br><span class="line">                attempt = literal_eval(val)</span><br><span class="line">            <span class="keyword">except</span> SyntaxError:</span><br><span class="line">                <span class="comment"># if that goes wrong, just use the string</span></span><br><span class="line">                attempt = val</span><br><span class="line">            <span class="comment"># ensure the types match ok</span></span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">type</span>(attempt) == <span class="built_in">type</span>(<span class="built_in">globals</span>()[key])</span><br><span class="line">            <span class="comment"># cross fingers</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Overriding: <span class="subst">&#123;key&#125;</span> = <span class="subst">&#123;attempt&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="built_in">globals</span>()[key] = attempt</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&quot;Unknown config key: <span class="subst">&#123;key&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="其他准备"><a href="#其他准备" class="headerlink" title="其他准备"></a>其他准备</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">os.makedirs(out_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">torch.manual_seed(<span class="number">1337</span>)</span><br><span class="line">torch.backends.cuda.matmul.allow_tf32 = <span class="literal">True</span> <span class="comment"># allow tf32 on matmul</span></span><br><span class="line">torch.backends.cudnn.allow_tf32 = <span class="literal">True</span> <span class="comment"># allow tf32 on cudnn</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># poor man&#x27;s data loader, TODO use real DataLoader...</span></span><br><span class="line">data_dir = os.path.join(<span class="string">&#x27;data&#x27;</span>, dataset)</span><br><span class="line">train_data = np.memmap(os.path.join(data_dir, <span class="string">&#x27;train.bin&#x27;</span>), dtype=np.uint16, mode=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">val_data = np.memmap(os.path.join(data_dir, <span class="string">&#x27;val.bin&#x27;</span>), dtype=np.uint16, mode=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_batch</span>(<span class="params">split</span>):</span><br><span class="line">    data = train_data <span class="keyword">if</span> split == <span class="string">&#x27;train&#x27;</span> <span class="keyword">else</span> val_data</span><br><span class="line">    ix = torch.randint(<span class="built_in">len</span>(data) - block_size, (batch_size,))</span><br><span class="line">    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) <span class="keyword">for</span> i <span class="keyword">in</span> ix])</span><br><span class="line">    y = torch.stack([torch.from_numpy((data[i+<span class="number">1</span>:i+<span class="number">1</span>+block_size]).astype(np.int64)) <span class="keyword">for</span> i <span class="keyword">in</span> ix])</span><br><span class="line">    x, y = x.to(device), y.to(device)</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br></pre></td></tr></table></figure>

<h4 id="模型初始化"><a href="#模型初始化" class="headerlink" title="模型初始化"></a>模型初始化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model init</span></span><br><span class="line"><span class="comment"># TODO I don&#x27;t love this whole part/API yet</span></span><br><span class="line">model_args = <span class="built_in">dict</span>(n_layer = n_layer, n_head = n_head, n_embd = n_embd, block_size = block_size, dropout = dropout)</span><br><span class="line"><span class="keyword">if</span> init_from == <span class="string">&#x27;scratch&#x27;</span>:</span><br><span class="line">    <span class="comment"># init a new model from scratch</span></span><br><span class="line">    gptconf = GPTConfig(**model_args)</span><br><span class="line">    model = GPT(gptconf)</span><br><span class="line"><span class="keyword">elif</span> init_from == <span class="string">&#x27;resume&#x27;</span>:</span><br><span class="line">    <span class="comment"># resume training from a checkpoint. <span class="doctag">TODO:</span> do we resume iter_num etc too? (yes...)</span></span><br><span class="line">    ckpt_path = os.path.join(out_dir, <span class="string">&#x27;ckpt.pt&#x27;</span>)</span><br><span class="line">    checkpoint = torch.load(ckpt_path)</span><br><span class="line">    checkpoint_model_args = checkpoint[<span class="string">&#x27;model_args&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> model_args.items():</span><br><span class="line">        <span class="keyword">assert</span> checkpoint_model_args[k] == v, <span class="string">&quot;for now&quot;</span></span><br><span class="line">    gptconf = GPTConfig(**model_args)</span><br><span class="line">    model = GPT(gptconf)</span><br><span class="line">    model.load_state_dict(checkpoint[<span class="string">&#x27;model&#x27;</span>])</span><br><span class="line"><span class="keyword">elif</span> init_from.startswith(<span class="string">&#x27;gpt2&#x27;</span>):</span><br><span class="line">    <span class="comment"># initialize from OpenAI GPT-2 weights</span></span><br><span class="line">    model = GPT.from_pretrained(init_from)</span><br><span class="line">    <span class="keyword">if</span> block_size &lt; model.block_size:</span><br><span class="line">        model.crop_block_size(block_size)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>

<h4 id="loss评估"><a href="#loss评估" class="headerlink" title="loss评估"></a>loss评估</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">estimate_loss</span>():</span><br><span class="line">    out = &#123;&#125;</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">for</span> split <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]:</span><br><span class="line">        losses = torch.zeros(eval_iters)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(eval_iters):</span><br><span class="line">            X, Y = get_batch(split)</span><br><span class="line">            <span class="keyword">with</span> torch.amp.autocast(device_type=<span class="string">&quot;cuda&quot;</span>, dtype=torch.bfloat16):</span><br><span class="line">                logits, loss = model(X, Y)</span><br><span class="line">            losses[k] = loss.item()</span><br><span class="line">        out[split] = losses.mean()</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>

<h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># optimizer</span></span><br><span class="line">optimizer = model.configure_optimizers(weight_decay, learning_rate, betas)</span><br><span class="line"><span class="keyword">if</span> init_from == <span class="string">&#x27;resume&#x27;</span>:</span><br><span class="line">    optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer&#x27;</span>])</span><br></pre></td></tr></table></figure>

<h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># learning rate decay scheduler (cosine with warmup)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_lr</span>(<span class="params"><span class="built_in">iter</span></span>):</span><br><span class="line">    <span class="comment"># 1) linear warmup for warmup_iters steps</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">iter</span> &lt; warmup_iters:</span><br><span class="line">        <span class="keyword">return</span> learning_rate * <span class="built_in">iter</span> / warmup_iters</span><br><span class="line">    <span class="comment"># 2) if iter &gt; lr_decay_iters, return min learning rate</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">iter</span> &gt; lr_decay_iters:</span><br><span class="line">        <span class="keyword">return</span> min_lr</span><br><span class="line">    <span class="comment"># 3) in between, use cosine decay down to min learning rate</span></span><br><span class="line">    decay_ratio = (<span class="built_in">iter</span> - warmup_iters) / (lr_decay_iters - warmup_iters)</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= decay_ratio &lt;= <span class="number">1</span></span><br><span class="line">    coeff = <span class="number">0.5</span> * (<span class="number">1.0</span> + math.cos(math.pi * decay_ratio)) <span class="comment"># ranges 0..1</span></span><br><span class="line">    <span class="keyword">return</span> min_lr + coeff * (learning_rate - min_lr)</span><br></pre></td></tr></table></figure>



<h4 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># logging</span></span><br><span class="line"><span class="keyword">if</span> wandb_log:</span><br><span class="line">    wandb.init(project=wandb_project, entity=wandb_entity, name=wandb_run_name)</span><br><span class="line">    wandb.config = &#123;</span><br><span class="line">        <span class="string">&quot;batch_size&quot;</span>: batch_size,</span><br><span class="line">        <span class="string">&quot;block_size&quot;</span>: block_size,</span><br><span class="line">        <span class="string">&quot;learning_rate&quot;</span>: learning_rate, <span class="comment"># TODO log everything else too</span></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<h3 id="training-loop"><a href="#training-loop" class="headerlink" title="training loop"></a>training loop</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># training loop</span></span><br><span class="line">iter_num = <span class="number">0</span></span><br><span class="line">num_tokens = <span class="number">0</span></span><br><span class="line">best_val_loss = <span class="number">1e9</span></span><br><span class="line">t0 = time.time()</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># determine the learning rate for this iteration</span></span><br><span class="line">    <span class="keyword">if</span> decay_lr:</span><br><span class="line">        lr = get_lr(iter_num)</span><br><span class="line">        <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">            param_group[<span class="string">&#x27;lr&#x27;</span>] = lr</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        lr = learning_rate</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> iter_num % eval_interval == <span class="number">0</span>:</span><br><span class="line">        losses = estimate_loss()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;step <span class="subst">&#123;iter_num&#125;</span>: train loss <span class="subst">&#123;losses[<span class="string">&#x27;train&#x27;</span>]:<span class="number">.4</span>f&#125;</span>, val loss <span class="subst">&#123;losses[<span class="string">&#x27;val&#x27;</span>]:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> wandb_log:</span><br><span class="line">            wandb.log(&#123;</span><br><span class="line">                <span class="string">&quot;iter&quot;</span>: iter_num,</span><br><span class="line">                <span class="string">&quot;num_tokens&quot;</span>: num_tokens,</span><br><span class="line">                <span class="string">&quot;train/loss&quot;</span>: losses[<span class="string">&#x27;train&#x27;</span>],</span><br><span class="line">                <span class="string">&quot;val/loss&quot;</span>: losses[<span class="string">&#x27;val&#x27;</span>],</span><br><span class="line">                <span class="string">&quot;lr&quot;</span>: lr,</span><br><span class="line">            &#125;)</span><br><span class="line">        <span class="keyword">if</span> losses[<span class="string">&#x27;val&#x27;</span>] &lt; best_val_loss:</span><br><span class="line">            best_val_loss = losses[<span class="string">&#x27;val&#x27;</span>]</span><br><span class="line">            <span class="keyword">if</span> iter_num &gt; <span class="number">0</span>: <span class="comment"># don&#x27;t save checkpoints on very first iteration...</span></span><br><span class="line">                checkpoint = &#123;</span><br><span class="line">                    <span class="string">&#x27;model&#x27;</span>: model.state_dict(),</span><br><span class="line">                    <span class="string">&#x27;optimizer&#x27;</span>: optimizer.state_dict(),</span><br><span class="line">                    <span class="string">&#x27;model_args&#x27;</span>: model_args,</span><br><span class="line">                    <span class="string">&#x27;iter_num&#x27;</span>: iter_num,</span><br><span class="line">                &#125;</span><br><span class="line">                torch.save(checkpoint, os.path.join(out_dir, <span class="string">&#x27;ckpt.pt&#x27;</span>))</span><br><span class="line">    <span class="keyword">if</span> iter_num == <span class="number">0</span> <span class="keyword">and</span> eval_only:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    X, Y = get_batch(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.amp.autocast(device_type=<span class="string">&quot;cuda&quot;</span>, dtype=torch.bfloat16):</span><br><span class="line">        logits, loss = model(X, Y)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad(set_to_none=<span class="literal">True</span>)</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> gradient clipping</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    t1 = time.time()</span><br><span class="line">    dt = t1 - t0</span><br><span class="line">    t0 = t1</span><br><span class="line">    <span class="keyword">if</span> iter_num % log_interval == <span class="number">0</span>:</span><br><span class="line">        lossf = loss.item()  <span class="comment"># loss as float. TODO CPU-GPU sync: profile, make sure not slow af</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;iter <span class="subst">&#123;iter_num&#125;</span>: loss <span class="subst">&#123;lossf:<span class="number">.4</span>f&#125;</span>, time <span class="subst">&#123;dt*<span class="number">1000</span>:<span class="number">.2</span>f&#125;</span>ms&quot;</span>)</span><br><span class="line">    iter_num += <span class="number">1</span></span><br><span class="line">    num_tokens += X.numel()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># termination conditions</span></span><br><span class="line">    <span class="keyword">if</span> iter_num &gt;= max_iters:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>



<h1 id="seq2seq模型"><a href="#seq2seq模型" class="headerlink" title="seq2seq模型"></a>seq2seq模型</h1><p>Seq2Seq（Sequence-to-Sequence）模型是一种用于处理序列到序列转换任务的神经网络模型。它通常用于自然语言处理（NLP）任务，如机器翻译、文本摘要、对话生成等。Seq2Seq模型的核心思想是将一个输入序列（如一句话）转换成一个输出序列（如另一句话）。</p>
<h3 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h3><p>Seq2Seq模型通常由两个主要部分组成：</p>
<ol>
<li><strong>编码器（Encoder）</strong>：编码器负责将输入序列转换成一个固定大小的上下文向量（context vector）。这个向量通常是输入序列的表示，包含了输入序列的所有信息。</li>
<li><strong>解码器（Decoder）</strong>：解码器负责将编码器生成的上下文向量转换成输出序列。解码器会逐步生成输出序列中的每个元素，直到生成一个终止符（如句号）。</li>
</ol>
<h3 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h3><ol>
<li><p><strong>编码阶段</strong>：</p>
<ul>
<li>输入序列（如一句话）被送入编码器。</li>
<li>编码器通常是一个循环神经网络（RNN），如LSTM（长短期记忆网络）或GRU（门控循环单元）。</li>
<li>编码器逐步处理输入序列中的每个元素（如单词），并更新其内部状态。</li>
<li>最终，编码器生成一个上下文向量，这个向量包含了输入序列的完整信息。</li>
</ul>
</li>
<li><p><strong>解码阶段</strong>：</p>
<ul>
<li>解码器也通常是一个RNN，它以编码器生成的上下文向量为初始状态。</li>
<li>解码器逐步生成输出序列中的每个元素。在每一步，解码器会根据当前的内部状态和已经生成的部分输出序列，预测下一个输出元素。</li>
<li>解码器会继续生成输出序列，直到生成一个终止符（如句号），表示输出序列的结束。</li>
</ul>
</li>
</ol>
<h3 id="注意力机制（Attention-Mechanism）"><a href="#注意力机制（Attention-Mechanism）" class="headerlink" title="注意力机制（Attention Mechanism）"></a>注意力机制（Attention Mechanism）</h3><p>传统的Seq2Seq模型存在一个问题：编码器生成的上下文向量需要包含输入序列的所有信息，这可能导致信息丢失，尤其是在处理长序列时。为了解决这个问题，引入了注意力机制（Attention Mechanism）。</p>
<p>注意力机制允许解码器在生成每个输出元素时，关注输入序列中的不同部分。具体来说，解码器在每一步都会计算一个注意力权重，这些权重决定了输入序列中哪些部分对当前输出元素的生成最为重要。这样，解码器可以动态地关注输入序列中的不同部分，从而更好地生成输出序列。</p>
<h3 id="应用场景-1"><a href="#应用场景-1" class="headerlink" title="应用场景"></a>应用场景</h3><ul>
<li><strong>机器翻译</strong>：将一种语言的句子翻译成另一种语言的句子。</li>
<li><strong>文本摘要</strong>：将长篇文章或段落压缩成简短的摘要。</li>
<li><strong>对话生成</strong>：根据用户的输入生成合适的回复。</li>
<li><strong>语音识别</strong>：将语音信号转换成文本。</li>
</ul>
<p>下面是一个简单的Seq2Seq模型的代码实现，使用PyTorch框架。这个实现不包含注意力机制和其他复杂技巧，仅用于演示基本的Seq2Seq模型的工作原理。</p>
<h3 id="1-导入必要的库"><a href="#1-导入必要的库" class="headerlink" title="1. 导入必要的库"></a>1. 导入必要的库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一些超参数</span></span><br><span class="line">input_size = <span class="number">10</span>  <span class="comment"># 输入词汇表大小</span></span><br><span class="line">output_size = <span class="number">10</span>  <span class="comment"># 输出词汇表大小</span></span><br><span class="line">hidden_size = <span class="number">256</span>  <span class="comment"># 隐藏层大小</span></span><br><span class="line">embedding_dim = <span class="number">128</span>  <span class="comment"># 词嵌入维度</span></span><br><span class="line">seq_length = <span class="number">5</span>  <span class="comment"># 序列长度</span></span><br><span class="line">batch_size = <span class="number">32</span>  <span class="comment"># 批量大小</span></span><br></pre></td></tr></table></figure>

<h3 id="2-定义编码器（Encoder）"><a href="#2-定义编码器（Encoder）" class="headerlink" title="2. 定义编码器（Encoder）"></a>2. 定义编码器（Encoder）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, embedding_dim, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.hidden_size = hidden_size</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(input_size, embedding_dim)</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.GRU(embedding_dim, hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_seq</span>):</span><br><span class="line">        embedded = <span class="variable language_">self</span>.embedding(input_seq)</span><br><span class="line">        output, hidden = <span class="variable language_">self</span>.rnn(embedded)</span><br><span class="line">        <span class="keyword">return</span> hidden</span><br></pre></td></tr></table></figure>

<h3 id="3-定义解码器（Decoder）"><a href="#3-定义解码器（Decoder）" class="headerlink" title="3. 定义解码器（Decoder）"></a>3. 定义解码器（Decoder）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, output_size, embedding_dim, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.hidden_size = hidden_size</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(output_size, embedding_dim)</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.GRU(embedding_dim, hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.out = nn.Linear(hidden_size, output_size)</span><br><span class="line">        <span class="variable language_">self</span>.softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_seq, hidden</span>):</span><br><span class="line">        embedded = <span class="variable language_">self</span>.embedding(input_seq)</span><br><span class="line">        output, hidden = <span class="variable language_">self</span>.rnn(embedded, hidden)</span><br><span class="line">        output = <span class="variable language_">self</span>.softmax(<span class="variable language_">self</span>.out(output[<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br></pre></td></tr></table></figure>

<h3 id="4-定义Seq2Seq模型"><a href="#4-定义Seq2Seq模型" class="headerlink" title="4. 定义Seq2Seq模型"></a>4. 定义Seq2Seq模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2Seq</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2Seq, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder = encoder</span><br><span class="line">        <span class="variable language_">self</span>.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_seq, target_seq</span>):</span><br><span class="line">        encoder_hidden = <span class="variable language_">self</span>.encoder(input_seq)</span><br><span class="line">        decoder_hidden = encoder_hidden</span><br><span class="line">        decoder_input = torch.tensor([[<span class="number">0</span>]] * batch_size)  <span class="comment"># 使用一个特殊的起始符号</span></span><br><span class="line"></span><br><span class="line">        outputs = torch.zeros(seq_length, batch_size, output_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(seq_length):</span><br><span class="line">            decoder_output, decoder_hidden = <span class="variable language_">self</span>.decoder(decoder_input, decoder_hidden)</span><br><span class="line">            outputs[t] = decoder_output</span><br><span class="line">            decoder_input = target_seq[t].unsqueeze(<span class="number">0</span>)  <span class="comment"># 使用目标序列作为输入</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>

<h3 id="5-初始化模型和优化器"><a href="#5-初始化模型和优化器" class="headerlink" title="5. 初始化模型和优化器"></a>5. 初始化模型和优化器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">encoder = Encoder(input_size, embedding_dim, hidden_size)</span><br><span class="line">decoder = Decoder(output_size, embedding_dim, hidden_size)</span><br><span class="line">model = Seq2Seq(encoder, decoder)</span><br><span class="line"></span><br><span class="line">criterion = nn.NLLLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>

<h3 id="6-训练模型"><a href="#6-训练模型" class="headerlink" title="6. 训练模型"></a>6. 训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设我们有一些输入和目标序列数据</span></span><br><span class="line">input_seq = torch.randint(<span class="number">0</span>, input_size, (seq_length, batch_size))</span><br><span class="line">target_seq = torch.randint(<span class="number">0</span>, output_size, (seq_length, batch_size))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(input_seq, target_seq)</span><br><span class="line">    loss = criterion(output.view(-<span class="number">1</span>, output_size), target_seq.view(-<span class="number">1</span>))</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>, Loss: <span class="subst">&#123;loss.item()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="7-使用模型进行预测"><a href="#7-使用模型进行预测" class="headerlink" title="7. 使用模型进行预测"></a>7. 使用模型进行预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设我们有一些新的输入序列数据</span></span><br><span class="line">new_input_seq = torch.randint(<span class="number">0</span>, input_size, (seq_length, batch_size))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output = model(new_input_seq, torch.zeros(seq_length, batch_size).long())</span><br><span class="line">    predicted_seq = torch.argmax(output, dim=<span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(predicted_seq)</span><br></pre></td></tr></table></figure>

<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>这个简单的Seq2Seq模型展示了如何使用PyTorch实现一个基本的编码器-解码器架构。虽然这个实现不包含注意力机制和其他复杂技巧，但它足以展示Seq2Seq模型的基本工作原理。在实际应用中，可以根据需要添加更多的功能和优化。</p>
<h1 id="KV-Cache"><a href="#KV-Cache" class="headerlink" title="KV Cache"></a>KV Cache</h1><p>KV Cache主要用在模型推理之中，在每次推理的Attention机制中，由于推理是自回归的，所以有很多的重复计算。</p>
<p>举个例子：</p>
<img src="\llm\image-20240905215753488.png" alt="image-20240905215753488" style="zoom:50%;" />

<p>在上下文为<code>t1-t4</code>时，计算attention分数时，就已经计算了1-4的权重，生成t5之后预测t6时，又要计算一次注意力分数，虽然不太一样，但是显然有一部分的值是重复计算的。所以要进行缓存。</p>
<h2 id="KV-Cache缺陷"><a href="#KV-Cache缺陷" class="headerlink" title="KV Cache缺陷"></a>KV Cache缺陷</h2><ol>
<li>KV Cache是非常占显存的，对不断增长的 LLM 的窗口长度的需要与有限的 GPU 显存之间的矛盾。</li>
</ol>
<h1 id="开源模型"><a href="#开源模型" class="headerlink" title="开源模型"></a>开源模型</h1><h2 id="Llama系列"><a href="#Llama系列" class="headerlink" title="Llama系列"></a>Llama系列</h2><h3 id="Llama-1"><a href="#Llama-1" class="headerlink" title="Llama"></a>Llama</h3><ul>
<li><p><strong>Pre-normalization</strong> [GPT3]. To improve the training stability, we normalize the input of each transformer sub-layer, instead of normalizing the output. We use the <strong>RMSNorm</strong> normalizing function, introduced by Zhang and Sennrich (2019).</p>
</li>
<li><p><strong>SwiGLU activation function</strong> [PaLM]. We replace the ReLU non-linearity by the SwiGLU activation function, introduced by Shazeer (2020) to improve the performance. We use a dimension of 2 3 4d instead of 4d as in PaLM.</p>
</li>
<li><p><strong>Rotary Embeddings</strong> [GPTNeo]. We remove the absolute positional embeddings, and instead, add rotary positional embeddings (RoPE), introduced by Su et al. (2021), at each layer of the network. The details of the hyper-parameters for our different models are given in Table 2.</p>
</li>
<li><p>using the <strong>AdamW</strong> optimizer, with: β1 &#x3D; 0.9, β2 &#x3D; 0.95</p>
</li>
<li><p><strong>cosine learning rate schedule</strong>, such that the final learning rate is equal to 10% of the maximal learning rate.</p>
</li>
<li><p><strong>weight decay of 0.1</strong> and <strong>gradient clipping of 1.0</strong>. </p>
</li>
<li><p>2, 000 warmup steps, and vary the learning rate and batch size with the size of the model</p>
</li>
<li><p>Key hyperparameters</p>
<img src="\llm\image-20240907135305713.png" alt="image-20240907135305713" style="zoom: 67%;" /></li>
</ul>
<h3 id="Llama-3-1"><a href="#Llama-3-1" class="headerlink" title="Llama 3.1"></a>Llama 3.1</h3><ul>
<li><p>Dense Transformer</p>
</li>
<li><p>context window of up to 128K tokens</p>
</li>
<li><p>Annealing Data（退火数据）：Empirically, we find that annealing (see Section 3.4.3) on small amounts of high-quality code and mathematical data can boost the performance of pre-trained models on key benchmarks.</p>
<p>意思就是说，你预训练的时候，高质量数据和其他数据混是在一起训练的，想要多利用一下高质量数据，就在之后再单独使用高质量的数据再训练一遍，并调高学习率，然后在训练过程中慢慢降低学习率。</p>
</li>
<li><p>Model Architecture（ a few small <strong>modifications compared to Llama 2</strong>）</p>
<ul>
<li>grouped query attention（<strong>GQA</strong>）</li>
<li>use an <strong>attention mask that prevents self-attention between different documents</strong> within the same sequence.</li>
<li>We use a <strong>vocabulary with 128K tokens</strong>. Our token vocabulary combines 100K tokens from the tiktoken3 tokenizer with 28K additional tokens to better support non-English languages. Compared to the Llama 2 tokenizer, our new tokenizer <strong>improves compression rates on a sample of English data from 3.17 to 3.94 characters per token</strong>. This enables the model to “read” more text for the same amount of training compute. We also found that <strong>adding 28K tokens from select non-English languages improved both compression ratios and downstream performance, with no impact on English tokenization</strong>.</li>
<li>We increase the RoPE base <strong>frequency hyperparameter to 500,000</strong>. This enables us to <strong>better support longer contexts</strong>; Xiong et al. (2023) showed this value to be effective for context lengths up to 32,768.</li>
</ul>
</li>
<li><p>Key hyperparameters</p>
<p><img src="/.%5Cllm%5Cimage-20240907125454274.png" alt="image-20240907125454274"></p>
</li>
<li><p>Scaling Law</p>
<ul>
<li><p>1</p>
<p><img src="/.%5Cllm%5Cimage-20240907131041216.png" alt="image-20240907131041216"></p>
</li>
<li><p>2</p>
<p><img src="/.%5Cllm%5Cimage-20240907131251659.png" alt="image-20240907131251659"></p>
</li>
</ul>
</li>
<li><p>Training Recipe</p>
</li>
<li><p>We pre-train Llama 3 405B using <strong>AdamW</strong> with a <strong>peak learning rate of 8 × 10−5</strong> , a <strong>linear warm up of 8,000 steps</strong>, and a <strong>cosine learning rate schedule decaying to 8 × 10−7 over 1,200,000 steps</strong>.</p>
</li>
<li><p>initial batch size of <strong>4M tokens and sequences of length 4,096</strong>, and double these values to a batch size of <strong>8M sequences of 8,192 tokens</strong> after pre-training 252M tokens. We <strong>double the batch size again to 16M</strong> after pre-training on 2.87T tokens. We found this training recipe to be very stable: we observed <strong>few loss spikes</strong> and did not require interventions to correct for model training divergence.</p>
</li>
<li><p>Adjusting the data mix</p>
<ul>
<li><strong>increased the percentage of non-English</strong> data during pre-training to improve the multilingual performance of Llama 3</li>
<li><strong>upsample mathematical data</strong> to improve the model’s mathematical reasoning performance</li>
<li><strong>added more recent web data</strong> in the later stages of pre-training to advance the model’s knowledge cut-off</li>
<li><strong>downsampled subsets</strong> of the pre-training data that were later <strong>identified as being lower quality</strong></li>
</ul>
</li>
<li><p>Long Context Pre-Training</p>
<ul>
<li>do not train on long sequences earlier because the compute in self-attention layers grows quadratically in the sequence length</li>
<li>increase the supported context length in increments, pre-training until the model has successfully adapted to the increased context length. We assess successful adaptation by measuring: <ul>
<li>model performance on short-context evaluations has recovered completely</li>
<li>the model perfectly solves “needle in a haystack” tasks up to that length</li>
</ul>
</li>
<li><strong>increased context length gradually in six stages</strong>, starting from the original <strong>8K</strong> context window and ending in the final <strong>128K</strong> context window. This long-context pre-training stage was performed using approximately <strong>800B training tokens</strong>.</li>
</ul>
</li>
<li><p>Annealing</p>
<ul>
<li>During pre-training <strong>on the final 40M tokens</strong>, we <strong>linearly annealed the learning rate to 0</strong>, maintaining a context length of 128K tokens.</li>
<li>During this annealing phase, we also adjusted the data mix to <strong>upsample data sources of very high quality</strong>.</li>
<li>Finally, we <strong>compute the average of model checkpoints</strong> (Polyak (1991) averaging) during annealing to produce the final pre-trained model.</li>
</ul>
<blockquote>
<p>Polyak平均（Polyak Averaging），也称为指数移动平均（Exponential Moving Average, EMA），是一种在机器学习和优化领域中常用的技术，用于提高模型的稳定性和泛化能力。它通过对训练过程中保存的多个模型检查点进行加权平均来生成一个最终的模型。</p>
<h3 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h3><ol>
<li><p><strong>保存模型检查点</strong>：在训练过程中，每隔一定的迭代次数或达到某个条件时，保存当前模型的参数（权重和偏置）。这些保存的参数称为模型检查点。</p>
</li>
<li><p><strong>加权平均</strong>：Polyak平均通过对这些检查点进行加权平均来生成最终的模型参数。具体来说，假设在训练过程中保存了 ( T ) 个检查点，每个检查点的参数为 ( \theta_t )（其中 ( t ) 表示第 ( t ) 个检查点），那么最终的模型参数 ( \theta_{\text{avg}} ) 可以通过以下公式计算：<br>$$<br>\theta_{\text{avg}} &#x3D; \frac{1}{T} \sum_{t&#x3D;1}^{T} \theta_t<br>$$<br>在实际应用中，通常使用指数加权平均（Exponential Moving Average, EMA）来替代简单的算术平均。指数加权平均的公式为：</p>
<p>$$<br>\theta_{\text{avg}}^{(t)} &#x3D; \alpha \theta_{\text{avg}}^{(t-1)} + (1 - \alpha) \theta_t<br>$$<br>其中，( \alpha ) 是一个介于0和1之间的平滑因子，控制着历史参数的权重。较小的 ( \alpha ) 值意味着更重视最近的检查点，而较大的 ( \alpha ) 值则意味着更重视历史的检查点。</p>
</li>
</ol>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol>
<li><p><strong>减少方差</strong>：通过平均多个检查点，Polyak平均可以减少模型的方差，从而提高模型的稳定性和泛化能力。</p>
</li>
<li><p><strong>平滑噪声</strong>：训练过程中，模型参数可能会受到噪声的影响。Polyak平均通过平滑这些噪声，有助于生成更稳定的模型。</p>
</li>
<li><p><strong>提高泛化能力</strong>：通过平均多个检查点，Polyak平均可以找到一个更接近全局最优解的模型参数，从而提高模型在未见数据上的表现。</p>
</li>
</ol>
<h3 id="应用场景-2"><a href="#应用场景-2" class="headerlink" title="应用场景"></a>应用场景</h3><p>Polyak平均广泛应用于深度学习中的各种任务，特别是在训练大型神经网络时。它常用于生成预训练模型，这些模型可以用于后续的微调任务或直接用于推理。</p>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p>Polyak平均是一种通过对训练过程中保存的多个模型检查点进行加权平均来生成最终模型的技术。它通过减少方差、平滑噪声和提高泛化能力，有助于生成更稳定和性能更好的模型。</p>
</blockquote>
</li>
</ul>
<h2 id="GPT系列"><a href="#GPT系列" class="headerlink" title="GPT系列"></a>GPT系列</h2><h3 id="GPT-1"><a href="#GPT-1" class="headerlink" title="GPT-1"></a>GPT-1</h3><p><a target="_blank" rel="noopener" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">language_understanding_paper.pdf (openai.com)</a></p>
<p>2017年，Google推出了Transformer模型，这一架构因其在性能上的显著优势迅速吸引了OpenAI团队的注意。OpenAI随后将研发重点转移到<strong>Transformer</strong>架构，并在2018年发布了GPT-1模型。GPT-1是基于生成式预训练（Generative Pre-Training）的Transformer架构，采用了<strong>仅有解码器的Transformer模型</strong>，专注于预测下一个词元。尽管GPT-1的<strong>参数规模相对较小</strong>，它采用了无监督预训练和有监督微调相结合的方法，以增强模型的通用任务求解能力。</p>
<p>同年，Google发布了BERT模型，它专注于自然语言理解任务（NLU），并只使用了Transformer的编码器部分。BERT-Large模型在多个NLU任务上取得了显著的性能提升，成为当时自然语言处理领域的明星模型，引领了一波研究热潮。然而，GPT-1由于规模与BERT-Base相当，且在公开评测数据集上的性能未能达到最优，因此没有在学术界引起足够的关注。GPT-1和BERT虽然都采用了Transformer架构，但它们的应用重点和架构设计有所不同，分别代表了自然语言生成和自然语言理解两个领域的早期探索。这些早期工作为后续更强大的GPT模型，如GPT-3和GPT-4，奠定了研究基础。</p>
<h3 id="GPT-2"><a href="#GPT-2" class="headerlink" title="GPT-2"></a>GPT-2</h3><p><a target="_blank" rel="noopener" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners (openai.com)</a></p>
<p>GPT-2继承了GPT-1的架构，并将<strong>参数规模扩大到15亿</strong>，使用大规模网页数据集WebText进行预训练。与GPT-1相比，GPT-2的<strong>创新之处在于尝试通过增加模型参数规模来提升性能</strong>，同时去除针对特定任务的微调环节，<strong>探索使用无监督预训练的语言模型来解决多种下游任务，而无需显式地使用标注数据进行微调</strong>。</p>
<p>GPT-2的研究重点在于多任务学习，即通过一种通用的概率形式来刻画不同任务的输出预测，将输入、输出和任务信息都以自然语言的形式进行描述。这样，后续的任务求解过程就可以视为文本生成问题。<strong>OpenAI团队在GPT-2的论文中解释了无监督预训练在下游任务中取得良好效果的原因，即特定任务的有监督学习目标与无监督学习目标（语言建模）在本质上是相同的，都旨在预测下一个词元。因此，优化无监督的全局学习目标本质上也是在优化有监督的任务学习目标。</strong></p>
<p>此外，OpenAI创始人采访时的观点与GPT-2论文中的讨论非常相似。他认为，神经网络学到的是生成文本过程中的某种表示，这些模型的生成文本实际上是真实世界的投影。语言模型对下一个单词的预测越准确，对世界知识的保真度就越高，在这个过程中获得的分辨度也就越高。</p>
<p>综上所述，GPT-2模型通过扩大参数规模和使用无监督预训练，探索了一种新的多任务学习框架，旨在提高模型的通用性和灵活性，减少对特定任务微调的依赖。同时，它也强调了语言模型在理解和生成自然语言文本方面的重要性，以及通过准确预测下一个词元来提高对世界知识的理解。</p>
<h3 id="GPT-3"><a href="#GPT-3" class="headerlink" title="GPT-3"></a>GPT-3</h3><p>OpenAI在2020年推出了具有里程碑意义的GPT-3模型，其模型<strong>参数规模扩展到了175B</strong>，相较于GPT-2提升了100余倍，标志着对模型扩展的极限尝试。在GPT-3的训练之前，OpenAI已经进行了充分的实验探索，包括小版本模型的尝试、数据收集与清洗、并行训练技巧等，这些工作为GPT-3的成功奠定了基础。</p>
<p><strong>GPT-3首次提出了“上下文学习”概念</strong>，允许大语言模型通过少样本学习解决各种任务，消除了对新任务进行微调的需求。这种学习方式使得GPT-3的训练和使用可以通过语言建模的形式统一描述，即预训练阶段在给定上下文条件下预测后续文本序列，使用阶段则根据任务描述和示例数据推理正确的解决方案。GPT-3在自然语言处理任务中表现出色，对于需要复杂推理或领域适配的任务也显示出良好的解决能力。论文指出，上下文学习对于大模型的性能增益尤为显著，而对于小模型则收益较小。</p>
<p>GPT-3的成功证明了将神经网络扩展到超大规模可以显著提升模型性能，并建立了基于提示学习方法的技术路线，为大语言模型的未来发展提供了新的思路和方法。</p>
<h3 id="InstructGPT"><a href="#InstructGPT" class="headerlink" title="InstructGPT"></a>InstructGPT</h3><p><a target="_blank" rel="noopener" href="https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf">Training_language_models_to_follow_instructions_with_human_feedback.pdf (openai.com)</a></p>
<p>OpenAI在GPT-3模型的基础上，<strong>通过两种主要途径进行了改进：代码数据训练和人类偏好对齐</strong>。首先，为了解决GPT-3在编程和数学问题求解上的不足，OpenAI于2021年推出了Codex模型，该模型在GitHub代码数据上进行了微调，显著提升了解决复杂问题的能力。此外，通过开发一种对比方法训练文本和代码嵌入，进一步改善了相关任务的性能。这些工作促成了GPT-3.5模型的开发，表明在代码数据上的训练对提高模型的综合性能，尤其是代码能力具有重要作用。</p>
<p>其次，OpenAI自2017年起就开始了人类偏好对齐的研究，通过强化学习算法从人类标注的偏好数据中学习改进模型性能。<strong>2017年，OpenAI提出了PPO算法，成为后续人类对齐技术的标配</strong>。2022年，OpenAI推出了InstructGPT，<strong>正式建立了基于人类反馈的强化学习算法RLHF</strong>，旨在改进GPT-3模型与人类对齐的能力，提高指令遵循能力，并缓解有害内容的生成，这对大语言模型的安全部署至关重要。</p>
<p>OpenAI在其技术博客中描述了对齐研究的技术路线，并总结了三个有前景的研究方向：使用人类反馈训练人工智能系统、协助人类评估和进行对齐研究。通过这些增强技术，OpenAI将改进后的GPT模型命名为GPT-3.5，它不仅展现了更强的综合能力，也标志着OpenAI在大语言模型研究方面迈出了重要一步。</p>
<h3 id="ChatGPT"><a href="#ChatGPT" class="headerlink" title="ChatGPT"></a>ChatGPT</h3><p>2022 年 11 月，OpenAI 发布了基于 GPT 模型的人工智能对话应用 服务 ChatGPT。<strong>ChatGPT沿用了InstructGPT的训练技术，并针对对话能力进行了优化</strong>。它结合了人类生成的对话数据进行训练，展现出丰富的世界知识、复杂问题求解能力、多轮对话上下文追踪与建模能力以及与人类价值观对齐的能力。ChatGPT还支持插件机制，扩展了功能，超越了以往所有人机对话系统的能力水平，引发了社会的高度关注。</p>
<h3 id="GPT-4"><a href="#GPT-4" class="headerlink" title="GPT-4"></a>GPT-4</h3><p>继 ChatGPT 后，OpenAI 于 2023 年 3 月发布了 GPT-4 。它是GPT系列模型的重要升级，<strong>首次将输入模态从单一文本扩展到图文双模态</strong>。GPT-4在解决复杂任务方面的能力显著强于GPT-3.5，在面向人类的考试中取得了优异成绩。</p>
<p>微软的研究团队对GPT-4进行了大规模测试，认为其展现出通用人工智能的潜力。<strong>GPT-4还进行了六个月的迭代对齐，增强了对恶意或挑衅性查询的安全响应</strong>。OpenAI在技术报告中强调了GPT-4的安全开发重要性，并应用了干预策略来缓解潜在问题，如幻觉、隐私泄露等。</p>
<p>GPT-4引入了”红队攻击”机制减少有害内容生成，并建立了深度学习训练基础架构，引入了可预测扩展的训练机制。更重要的是，GPT-4 搭建了完备的深度学习训练 基础架构，进一步引入了可预测扩展的训练机制，可以在模型训练过程中通过较 少计算开销来准确预测模型的最终性能。</p>
<h3 id="GPT-4V"><a href="#GPT-4V" class="headerlink" title="GPT-4V"></a>GPT-4V</h3><p>OpenAI对GPT-4系列模型进行了重要技术升级，发布了GPT-4V（2023年9月）和GPT-4 Turbo（2023年11月），这些升级显著增强了模型的视觉能力和安全性。GPT-4V专注于视觉输入的安全部署，广泛讨论了相关风险评估和缓解策略，而GPT-4 Turbo则在多个方面进行了优化，包括提升模型整体能力、扩展知识来源、支持更长上下文窗口、优化性能和价格，并引入了新功能。</p>
<p>同年，OpenAI推出了Assistants API，以提升开发效率，使开发人员能够快速创建面向特定任务的智能助手。此外，新版本的GPT模型通过GPT-4 Turbo with Vision、DALL·E-3、TTS等技术，进一步增强了多模态能力，提升了任务性能并扩展了能力范围，加强了以GPT模型为核心的大模型应用生态系统。</p>
<h3 id="GPT-4o"><a href="#GPT-4o" class="headerlink" title="GPT-4o"></a>GPT-4o</h3><p>今年5月14日，OpenAI春季发布会，发布了新型旗舰模型“GPT-4o”，GPT-4o的“o”代表“omni”，源自拉丁语“omnis”。在英语中“omni”常被用作词根，用来表示“全部”或“所有”的概念。<strong>GPT-4o是一个多模态大模型，支持文本、音频和图像的任意组合输入，并能生成文本、音频和图像的任意组合输出</strong>。与现有模型相比，它在视觉和音频理解方面尤其出色。</p>
<p>GPT-4o可以在音频、视觉和文本中进行实时推理，接受文本、音频和图像的任何组合作为输入，并生成文本、音频和图像的任何组合进行输出。它可以最短在232毫秒内响应音频输入，平均为320毫秒，这与人类在对话中的响应时间相似。此外，GPT-4o 还可以调整说话的语气，从夸张戏剧到冰冷机械，以适应不同的交流场景。令人兴奋的是，GPT-4o 还具备唱歌的功能，增添了更多的趣味性和娱乐性。</p>
<p>GPT-4o不仅在传统的文本能力上与GPT-4 Turbo的性能相当，还在 API 方面更快速，价格还更便宜 50%。总结来说，与 GPT-4 Turbo 相比，GPT-4o 速度提高了 2 倍，价格减半，限制速率提高了 5 倍。GPT-4o 目前的上下文窗口为 128k，模型知识截止日期为 2023 年 10 月。</p>
<h2 id="Qwen"><a href="#Qwen" class="headerlink" title="Qwen"></a>Qwen</h2><h2 id="ChatGLM-1"><a href="#ChatGLM-1" class="headerlink" title="ChatGLM"></a>ChatGLM</h2><h2 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h2><h1 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine-Tuning"></a>Fine-Tuning</h1><h2 id="Prompt-Tuning"><a href="#Prompt-Tuning" class="headerlink" title="Prompt Tuning"></a>Prompt Tuning</h2><h2 id="Adapter-Tuning"><a href="#Adapter-Tuning" class="headerlink" title="Adapter Tuning"></a>Adapter Tuning</h2><h2 id="LoRA-1"><a href="#LoRA-1" class="headerlink" title="LoRA"></a>LoRA</h2><h1 id="分布式训练"><a href="#分布式训练" class="headerlink" title="分布式训练"></a>分布式训练</h1><blockquote>
<p>一些缩写：DP（Data Parallel）、DDP（Distributed Data Parallel），FSDP（Fully Sharded Data Parallel）、PP（Pipeline Parallelism）、MP（Model Parallelism）</p>
</blockquote>
<p>Data Parallelism</p>
<ul>
<li>机器与机器之间的带宽 &lt;&lt; 内存与GPU之间的带宽 &lt;  GPU与GPU之间的带宽，这将会是一个瓶颈点，要尽量避免机器与机器之间的通讯</li>
</ul>
<p>在大型语言模型（LLM）的分布式训练中，主要有以下几种并行方式：</p>
<ol>
<li><p><strong>数据并行（Data Parallelism）</strong>：</p>
<ul>
<li>数据并行是最常见的并行方式之一。在这种模式下，训练数据被分割成多个部分，每个部分被分配给不同的计算节点（如GPU或TPU）。每个节点独立地计算梯度，然后通过某种形式的同步机制（如AllReduce操作）来汇总梯度并更新模型参数。</li>
<li>优点：实现简单，易于扩展。</li>
<li>缺点：当模型非常大时，单个节点的内存可能不足以容纳整个模型。</li>
</ul>
</li>
<li><p><strong>模型并行（Model Parallelism）</strong>：</p>
<ul>
<li>模型并行是将模型的不同层或部分分配到不同的计算节点上。每个节点只负责模型的一部分，节点之间需要进行通信以完成前向和反向传播。</li>
<li>优点：适用于非常大的模型，可以克服单个节点的内存限制。</li>
<li>缺点：实现复杂，节点间的通信开销较大。</li>
</ul>
</li>
<li><p><strong>流水线并行（Pipeline Parallelism）</strong>：</p>
<ul>
<li>流水线并行是模型并行的一种变体，它将模型的层分成多个阶段，并将每个阶段分配给不同的计算节点。数据在不同节点之间以流水线的方式传递，从而提高了训练的吞吐量。</li>
<li>优点：可以有效利用多个节点的计算资源，减少空闲时间。</li>
<li>缺点：需要精心设计流水线，以最小化流水线气泡（即某些节点空闲的时间）。</li>
</ul>
</li>
<li><p><strong>张量并行（Tensor Parallelism）</strong>：</p>
<ul>
<li>张量并行是将模型的张量操作（如矩阵乘法）分割到多个计算节点上。例如，可以将一个大的矩阵乘法操作分解为多个小的矩阵乘法操作，并在不同的节点上并行执行。</li>
<li>优点：可以有效利用多个节点的计算资源。</li>
<li>缺点：需要特定的库支持，实现复杂。</li>
</ul>
</li>
<li><p><strong>混合并行（Hybrid Parallelism）</strong>：</p>
<ul>
<li>混合并行是上述几种并行方式的组合使用。例如，可以在数据并行的基础上，结合模型并行或流水线并行，以充分利用计算资源并克服单个节点的限制。</li>
<li>优点：灵活性高，可以根据具体需求进行调整。</li>
<li>缺点：设计和实现复杂，需要深入理解各种并行方式的优缺点。</li>
</ul>
</li>
</ol>
<p>在实际应用中，选择哪种并行方式或组合取决于模型的规模、计算资源的配置以及训练效率的需求。</p>
<h2 id="分布式训练挑战有哪些？"><a href="#分布式训练挑战有哪些？" class="headerlink" title="分布式训练挑战有哪些？"></a>分布式训练挑战有哪些？</h2><h2 id="DeepSpeed"><a href="#DeepSpeed" class="headerlink" title="DeepSpeed"></a>DeepSpeed</h2><p>DeepSpeed 是由 Microsoft 开发的一个深度学习优化库，旨在提高大型模型训练的效率和可扩展性。它通过一系列创新技术和优化策略，使得在有限的计算资源下训练更大、更复杂的模型成为可能。DeepSpeed 可以与 PyTorch 无缝集成，提供了一套强大的工具和功能，帮助开发者更高效地进行分布式训练。</p>
<p>以下是 DeepSpeed 的一些关键特性和优势：</p>
<ol>
<li><p><strong>ZeRO（Zero Redundancy Optimizer）</strong>：</p>
<ul>
<li>ZeRO 是 DeepSpeed 的核心优化技术之一，它通过消除模型状态在分布式训练中的冗余复制，显著减少了内存占用。ZeRO 分为三个优化阶段（ZeRO-1、ZeRO-2 和 ZeRO-3），每个阶段逐步增加优化的复杂度和效果。</li>
<li>ZeRO-1：优化优化器状态的内存占用。</li>
<li>ZeRO-2：在 ZeRO-1 的基础上，进一步优化梯度的内存占用。</li>
<li>ZeRO-3：在 ZeRO-2 的基础上，优化模型权重的内存占用，实现近乎无冗余的分布式训练。</li>
</ul>
</li>
<li><p><strong>梯度累积</strong>：</p>
<ul>
<li>DeepSpeed 支持梯度累积，可以在不增加内存消耗的情况下，通过多次前向和反向传播累积梯度，然后进行一次优化步骤，从而模拟更大的批量大小。</li>
</ul>
</li>
<li><p><strong>混合精度训练</strong>：</p>
<ul>
<li>DeepSpeed 支持自动混合精度（AMP）训练，可以在训练过程中自动选择合适的精度（FP16 或 FP32），以减少内存占用和提高计算效率，同时保持模型精度。</li>
</ul>
</li>
<li><p><strong>模型并行</strong>：</p>
<ul>
<li>DeepSpeed 提供了灵活的模型并行功能，可以将模型的不同层或部分分配到不同的计算设备上，以支持训练超大规模的模型。</li>
</ul>
</li>
<li><p><strong>流水线并行</strong>：</p>
<ul>
<li>DeepSpeed 支持流水线并行，可以将模型的训练过程分解为多个阶段，并在不同的计算设备上并行执行，从而提高训练吞吐量。</li>
</ul>
</li>
<li><p><strong>优化器</strong>：</p>
<ul>
<li>DeepSpeed 提供了多种优化器，包括 Adam、SGD 等，并针对分布式训练进行了优化，以提高训练效率和稳定性。</li>
</ul>
</li>
<li><p><strong>集成和易用性</strong>：</p>
<ul>
<li>DeepSpeed 可以与 PyTorch 无缝集成，提供了简单易用的 API，使得开发者可以轻松地将现有的 PyTorch 代码迁移到 DeepSpeed 上进行分布式训练。</li>
</ul>
</li>
</ol>
<p>通过这些特性和优势，DeepSpeed 使得在有限的计算资源下训练数十亿甚至万亿参数的模型成为可能，极大地推动了大型模型研究和应用的发展。</p>
<h2 id="Megatron-LM-1"><a href="#Megatron-LM-1" class="headerlink" title="Megatron-LM"></a>Megatron-LM</h2><h2 id="一些细节"><a href="#一些细节" class="headerlink" title="一些细节"></a>一些细节</h2><h3 id="Ring-AllReduce"><a href="#Ring-AllReduce" class="headerlink" title="Ring-AllReduce"></a>Ring-AllReduce</h3><ol>
<li><p>假设有三个GPU，每个GPU上有完整模型，和一次反向传播的梯度a,b,c，目的是每个GPU上都有三个GPU上梯度的和。</p>
<img src="\llm\image-20240821235724539.png" alt="image-20240821235724539" style="zoom: 33%;" />
</li>
<li><p>第一阶段：Scatter-Reduce</p>
<img src="\llm\image-20240821235952764.png" alt="image-20240821235952764" style="zoom:33%;" />

<img src="\llm\image-20240822000036703.png" alt="image-20240822000036703" style="zoom:33%;" />

<img src="\llm\image-20240822000113640.png" alt="image-20240822000113640" style="zoom:33%;" />

<img src="\llm\image-20240822000135158.png" alt="image-20240822000135158" style="zoom:33%;" />
</li>
<li><p>第二阶段：AllGather</p>
<img src="\llm\image-20240822000244175.png" alt="image-20240822000244175" style="zoom:33%;" />

<img src="\llm\image-20240822000306524.png" alt="image-20240822000306524" style="zoom:33%;" />

<img src="\llm\image-20240822000332133.png" alt="image-20240822000332133" style="zoom:33%;" />

<img src="\llm\image-20240822000345183.png" alt="image-20240822000345183" style="zoom:33%;" /></li>
</ol>
<h1 id="Alignment"><a href="#Alignment" class="headerlink" title="Alignment"></a>Alignment</h1><h2 id="RLHF"><a href="#RLHF" class="headerlink" title="RLHF"></a>RLHF</h2><p>目标是训练LLM使它的回答更符合人类的偏好。</p>
<p>具体步骤：</p>
<ol>
<li><p>创建偏好数据集：preference dataset</p>
<p>格式为：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;input_text&quot;</span><span class="punctuation">:</span><span class="string">&quot;...&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;winnig_choice&quot;</span><span class="punctuation">:</span><span class="string">&quot;...&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;losing_choice&quot;</span><span class="punctuation">:</span><span class="string">&quot;...&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    ...</span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>使用偏好数据集训练一个奖励模型（Reward Model）</p>
</li>
<li><p>创建提示数据集（Prompt Dataset）</p>
</li>
</ol>
<h3 id="一些问题-1"><a href="#一些问题-1" class="headerlink" title="一些问题"></a>一些问题</h3><ol>
<li>为什么不用SFT，RLHF相比SFT好在哪？</li>
</ol>
<h2 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h2><h2 id="DPO"><a href="#DPO" class="headerlink" title="DPO"></a>DPO</h2><h1 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a>Agent</h1><p>介绍文章：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://medium.com/@aydinKerem/what-is-an-llm-agent-and-how-does-it-work-1d4d9e4381ca">What is an LLM Agent and how does it work? | by Kerem Aydın | Medium</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/introduction-to-llm-agents/">Introduction to LLM Agents | NVIDIA Technical Blog</a></li>
<li><a target="_blank" rel="noopener" href="https://www.ionio.ai/blog/what-is-llm-agent-ultimate-guide-to-llm-agent-with-technical-breakdown">What is LLM Agent? Ultimate Guide to LLM Agent With Technical Breakdown] (ionio.ai)</a></li>
</ol>
<p>简而言之：</p>
<p>LLM 代理是一种超越简单文本生成的 AI 系统。它使用大型语言模型 （LLM） 作为其中央计算引擎，使其能够进行对话、执行任务、推理并表现出一定程度的自主性。</p>
<h1 id="基础-杂项"><a href="#基础-杂项" class="headerlink" title="基础&#x2F;杂项"></a>基础&#x2F;杂项</h1><h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><h3 id="动量（Momentum）"><a href="#动量（Momentum）" class="headerlink" title="动量（Momentum）"></a>动量（Momentum）</h3><p>$$<br>\begin{align}<br>V_{dw} &amp;&#x3D; \beta V_{dw}+(1-\beta)d_w \<br>W &amp;&#x3D; W - \alpha V_{dw}<br>\end{align}<br>$$</p>
<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p>$$<br>\begin{align}<br>S_{dw} &amp;&#x3D; \beta S_{dw}+(1-\beta)d_w^2 \<br>W &amp;&#x3D; W - \alpha \frac{dw}{\sqrt{S_{dw}}}<br>\end{align}<br>$$</p>
<p>其中dw平方是按元素的（element wise的）</p>
<h3 id="Adam（Adaptive-momentum-estimation）"><a href="#Adam（Adaptive-momentum-estimation）" class="headerlink" title="Adam（Adaptive momentum estimation）"></a>Adam（Adaptive momentum estimation）</h3><p><strong>这两个公式可以看出，显然来自动量和RMSProp，是他们的结合</strong><br>$$<br>\begin{align}<br>V_{dw}&amp;&#x3D;\beta_1V_{dw}+(1-\beta_1)dw\<br>S_{dw}&amp;&#x3D;\beta_2 S_{dw}+(1-\beta_2)d_w^2\<br>\beta_1 &amp;&#x3D; 0.9\<br>\beta_2 &amp;&#x3D; 0.999<br>\end{align}<br>$$<br>其中dw平方是按元素的（element wise的）<br>$$<br>V_{dw}^{correct}&#x3D;\frac{V_{dw}}{(1-\beta_1^t)},\quad t&#x3D;t^{th} \quad \text{iteration}\<br>S_{dw}^{correct}&#x3D;\frac{S_{dw}}{(1-\beta_2^t)},\quad t&#x3D;t^{th} \quad \text{iteration}\<br>$$<br>最后<br>$$<br>\begin{align}<br>W&amp;&#x3D;W-\alpha\frac{V_{dw}^{correct}}{\sqrt{S_{dw}^{correct}}+\epsilon}\<br>\epsilon &amp;&#x3D; 10^{-8},\quad\text{author suggested}<br>\end{align}<br>$$</p>
<h3 id="AdamW"><a href="#AdamW" class="headerlink" title="AdamW"></a>AdamW</h3><p>$$<br>\begin{align}<br>W&amp;&#x3D;W-\alpha(\frac{V_{dw}^{correct}}{\sqrt{S_{dw}^{correct}}+\epsilon}+\lambda W)\<br>\end{align}<br>$$</p>
<p>需要保存的数据有</p>
<ol>
<li>W：权重本身</li>
<li>dW：权重梯度</li>
<li>V：一阶矩估计，也叫Momentum</li>
<li>S：二阶矩估计，也叫Variance</li>
</ol>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>ReLU激活函数缓解了梯度消失问题，这样可以加速收敛。</p>
<h3 id="GeLU"><a href="#GeLU" class="headerlink" title="GeLU"></a>GeLU</h3><p>$$<br>\text{GELU}(x)&#x3D;0.5<em>x</em>(1+\text{Tanh}(\sqrt{2&#x2F;\pi}<em>(x+0.044715</em>x^3)))<br>$$</p>
<p><img src="/.%5Cllm%5CGELU.png" alt="../_images/GELU.png"></p>
<h4 id="GeLU函数的特点："><a href="#GeLU函数的特点：" class="headerlink" title="GeLU函数的特点："></a>GeLU函数的特点：</h4><ol>
<li><strong>平滑性</strong>：GeLU函数在整个实数范围内都是平滑的，这有助于优化过程，因为它避免了像ReLU函数在零点处的非平滑性。</li>
<li><strong>非线性</strong>：GeLU函数是非线性的，能够捕捉复杂的非线性关系，这对于深度神经网络的学习能力至关重要。</li>
<li><strong>自适应性</strong>：GeLU函数在输入值较大时趋近于线性，而在输入值较小时趋近于零，这种特性使得它在处理不同范围的输入时具有一定的自适应性。</li>
</ol>
<h4 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h4><ul>
<li><strong>性能优越</strong>：在许多实验中，GeLU函数的表现优于传统的ReLU和Sigmoid函数，尤其是在自然语言处理任务中。</li>
<li><strong>计算效率</strong>：虽然GeLU函数涉及误差函数的计算，但在现代硬件上，这种计算开销是可以接受的，并且通常可以通过优化库（如TensorFlow和PyTorch）来高效实现。</li>
</ul>
<h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4><ul>
<li><strong>计算复杂度</strong>：相比于ReLU，GeLU函数的计算稍微复杂一些，因为它涉及到误差函数的计算。</li>
<li><strong>近似计算</strong>：在实际应用中，GeLU函数通常使用近似计算来提高计算效率，例如使用多项式近似。</li>
</ul>
<h4 id="应用场景："><a href="#应用场景：" class="headerlink" title="应用场景："></a>应用场景：</h4><p>GeLU函数在自然语言处理任务中表现尤为出色，例如在BERT、GPT等预训练语言模型中广泛使用。它的平滑性和非线性特性使得它能够更好地捕捉文本数据中的复杂关系。</p>
<p>总的来说，GeLU激活函数是一个在许多情况下表现优异的选择，尤其是在需要捕捉复杂非线性关系的深度学习模型中，特别是在自然语言处理领域。</p>
<h3 id="Swish"><a href="#Swish" class="headerlink" title="Swish"></a>Swish</h3><blockquote>
<p>See <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.08415">Gaussian Error Linear Units (GELUs)</a> where the SiLU (Sigmoid Linear Unit) was originally coined, and see <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1702.03118">Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning</a> and <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.05941v1">Swish: a Self-Gated Activation Function</a> where the SiLU was experimented with later.</p>
</blockquote>
<p>$$<br>\text{silu}(x)&#x3D;x∗σ(x), \text{where σ(x) is the logistic sigmoid.}<br>$$</p>
<p><img src="https://pytorch.org/docs/stable/_images/SiLU.png" alt="../_images/SiLU.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = nn.SiLU()</span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">2</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>

<p>Swish激活函数是一种相对较新的神经网络激活函数，它在2017年由Google Brain的研究人员提出。Swish函数的形式如下：</p>
<p>$$<br>\text{Swish}(x) &#x3D; x \cdot \sigma(\beta x)<br>$$<br>其中，( \sigma(x) ) 是Sigmoid函数，以下为定义，而 ( \beta ) 是一个可学习的参数或常数。<br>$$<br> \sigma(x) &#x3D; \frac{1}{1 + e^{-x}}<br>$$</p>
<h4 id="Swish函数的特点："><a href="#Swish函数的特点：" class="headerlink" title="Swish函数的特点："></a>Swish函数的特点：</h4><ol>
<li><strong>平滑性</strong>：Swish函数在整个实数范围内都是平滑的，这有助于优化过程，因为它避免了像ReLU函数在零点处的非平滑性。</li>
<li><strong>非单调性</strong>：与ReLU不同，Swish函数在某些区间内是非单调的，这使得它在某些情况下能够更好地捕捉复杂的非线性关系。</li>
<li><strong>自适应性</strong>：当 ( \beta ) 设置为1时，Swish函数在大多数情况下表现得很好，但也可以通过训练来调整 ( \beta ) 的值，以适应不同的网络结构和任务。</li>
</ol>
<h4 id="优点：-1"><a href="#优点：-1" class="headerlink" title="优点："></a>优点：</h4><ul>
<li><strong>性能优越</strong>：在许多实验中，Swish函数的表现优于传统的ReLU和Sigmoid函数，尤其是在深度神经网络中。</li>
<li><strong>计算效率</strong>：虽然Swish函数涉及Sigmoid函数的计算，但在现代硬件上，这种计算开销是可以接受的，并且通常可以通过优化库（如TensorFlow和PyTorch）来高效实现。</li>
</ul>
<h4 id="缺点：-1"><a href="#缺点：-1" class="headerlink" title="缺点："></a>缺点：</h4><ul>
<li><strong>计算复杂度</strong>：相比于ReLU，Swish函数的计算稍微复杂一些，因为它涉及到Sigmoid函数的计算。</li>
<li><strong>参数选择</strong>：虽然 ( \beta ) 可以是一个常数或可学习的参数，但选择合适的 ( \beta ) 值可能需要一些实验和调整。</li>
</ul>
<p>总的来说，Swish激活函数是一个在许多情况下表现优异的选择，尤其是在需要捕捉复杂非线性关系的深度学习模型中。</p>
<h3 id="SwiGLU"><a href="#SwiGLU" class="headerlink" title="SwiGLU"></a>SwiGLU</h3><blockquote>
<p>DeepSeek Coder V2.5 生成</p>
</blockquote>
<p>ArXiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.05202">https://arxiv.org/abs/2002.05202</a></p>
<p>SwiGLU（Swish-Gated Linear Unit）是一种结合了Swish激活函数和Gated Linear Unit（GLU）的激活函数，它在某些深度学习模型中表现出色，尤其是在自然语言处理（NLP）任务中。SwiGLU的设计旨在利用Swish函数的非线性和GLU的门控机制，以提高模型的表达能力和性能。</p>
<h4 id="SwiGLU的定义："><a href="#SwiGLU的定义：" class="headerlink" title="SwiGLU的定义："></a>SwiGLU的定义：</h4><p>SwiGLU的数学表达式如下：</p>
<p>$$<br>\text{SwiGLU}(x, W, V, b, c) &#x3D; \text{Swish}(xW + b) \otimes (xV + c)<br>$$<br>其中：</p>
<ul>
<li>( x ) 是输入向量。</li>
<li>( W ) 和 ( V ) 是权重矩阵。</li>
<li>( b ) 和 ( c ) 是偏置向量。</li>
<li>( \text{Swish}(x) ) 是Swish激活函数，定义为 ( \text{Swish}(x) &#x3D; x \cdot \sigma(\beta x) )，其中 ( \sigma ) 是Sigmoid函数。</li>
<li>( \otimes ) 表示逐元素乘法（Hadamard product）。</li>
</ul>
<h4 id="SwiGLU的特点："><a href="#SwiGLU的特点：" class="headerlink" title="SwiGLU的特点："></a>SwiGLU的特点：</h4><ol>
<li><strong>非线性</strong>：通过Swish激活函数引入非线性，增强了模型的表达能力。</li>
<li><strong>门控机制</strong>：通过GLU的门控机制，控制信息的流动，使得模型能够更好地捕捉输入数据中的重要特征。</li>
<li><strong>组合优势</strong>：SwiGLU结合了Swish和GLU的优点，既保留了Swish的非线性特性，又利用了GLU的门控机制，从而在某些任务中表现出色。</li>
</ol>
<h4 id="优点：-2"><a href="#优点：-2" class="headerlink" title="优点："></a>优点：</h4><ul>
<li><strong>性能优越</strong>：在许多实验中，SwiGLU的表现优于单独使用Swish或GLU，尤其是在需要复杂非线性和门控机制的任务中。</li>
<li><strong>表达能力强</strong>：通过结合Swish和GLU，SwiGLU能够更好地捕捉输入数据中的复杂关系，提高模型的表达能力。</li>
</ul>
<h4 id="缺点：-2"><a href="#缺点：-2" class="headerlink" title="缺点："></a>缺点：</h4><ul>
<li><strong>计算复杂度</strong>：相比于单独使用Swish或GLU，SwiGLU的计算复杂度更高，因为它涉及到更多的矩阵运算和逐元素乘法。</li>
<li><strong>参数较多</strong>：SwiGLU需要更多的参数（权重矩阵和偏置向量），这可能会增加模型的复杂性和训练时间。</li>
</ul>
<h4 id="应用场景：-1"><a href="#应用场景：-1" class="headerlink" title="应用场景："></a>应用场景：</h4><p>SwiGLU在自然语言处理任务中表现尤为出色，例如在Transformer模型中，它可以用于改进模型的编码器和解码器层。它的非线性和门控机制使得它能够更好地捕捉文本数据中的复杂关系，提高模型的性能。</p>
<p>总的来说，SwiGLU是一种在某些深度学习模型中表现优异的激活函数，尤其是在需要复杂非线性和门控机制的任务中，特别是在自然语言处理领域。</p>
<h3 id="GLU"><a href="#GLU" class="headerlink" title="GLU"></a>GLU</h3><blockquote>
<p>DeepSeek Coder V2.5 生成</p>
</blockquote>
<p>GLU（Gated Linear Unit）是一种在深度学习中常用的激活函数，尤其是在自然语言处理（NLP）任务中。GLU的设计旨在通过门控机制来控制信息的流动，从而提高模型的表达能力和性能。</p>
<h4 id="GLU的定义："><a href="#GLU的定义：" class="headerlink" title="GLU的定义："></a>GLU的定义：</h4><p>GLU的数学表达式如下：</p>
<p>$$<br>\text{GLU}(x) &#x3D; (xW + b) \otimes \sigma(xV + c)<br>$$<br>其中：</p>
<ul>
<li>( x ) 是输入向量。</li>
<li>( W ) 和 ( V ) 是权重矩阵。</li>
<li>( b ) 和 ( c ) 是偏置向量。</li>
<li>( \sigma ) 是Sigmoid函数，定义为 ( \sigma(x) &#x3D; \frac{1}{1 + e^{-x}} )。</li>
<li>( \otimes ) 表示逐元素乘法（Hadamard product）。</li>
</ul>
<h4 id="GLU的特点："><a href="#GLU的特点：" class="headerlink" title="GLU的特点："></a>GLU的特点：</h4><ol>
<li><strong>门控机制</strong>：GLU通过Sigmoid函数生成一个门控向量，该向量控制输入向量的哪些部分应该被保留或抑制。这种门控机制使得模型能够更好地捕捉输入数据中的重要特征。</li>
<li><strong>线性变换</strong>：在应用门控机制之前，输入向量首先通过线性变换（即 ( xW + b )），这使得模型能够学习到输入数据的不同表示。</li>
<li><strong>非线性</strong>：虽然GLU本身不引入非线性，但通过Sigmoid函数的门控机制，它能够在一定程度上模拟非线性行为。</li>
</ol>
<h4 id="优点：-3"><a href="#优点：-3" class="headerlink" title="优点："></a>优点：</h4><ul>
<li><strong>表达能力强</strong>：通过门控机制，GLU能够更好地捕捉输入数据中的复杂关系，提高模型的表达能力。</li>
<li><strong>计算效率</strong>：GLU的计算相对简单，因为它只涉及线性变换和逐元素乘法，这使得它在现代硬件上可以高效实现。</li>
</ul>
<h4 id="缺点：-3"><a href="#缺点：-3" class="headerlink" title="缺点："></a>缺点：</h4><ul>
<li><strong>非线性不足</strong>：相比于其他激活函数（如ReLU、Swish等），GLU本身不引入非线性，这可能会限制其在某些任务中的表现。</li>
<li><strong>参数较多</strong>：GLU需要更多的参数（权重矩阵和偏置向量），这可能会增加模型的复杂性和训练时间。</li>
</ul>
<h4 id="应用场景：-2"><a href="#应用场景：-2" class="headerlink" title="应用场景："></a>应用场景：</h4><p>GLU在自然语言处理任务中表现尤为出色，例如在Transformer模型中，它可以用于改进模型的编码器和解码器层。它的门控机制使得它能够更好地捕捉文本数据中的复杂关系，提高模型的性能。</p>
<h4 id="变体："><a href="#变体：" class="headerlink" title="变体："></a>变体：</h4><ul>
<li><strong>GTU（Gated Tanh Unit）</strong>：类似于GLU，但使用Tanh函数作为激活函数，而不是Sigmoid函数。</li>
<li><strong>Bilinear（双线性）</strong>：在某些情况下，使用双线性变换来替代线性变换，以进一步增强模型的表达能力。</li>
</ul>
<p>总的来说，GLU是一种在某些深度学习模型中表现优异的激活函数，尤其是在需要门控机制的任务中，特别是在自然语言处理领域。</p>
<h2 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h2><h3 id="RMSNorm"><a href="#RMSNorm" class="headerlink" title="RMSNorm"></a>RMSNorm</h3><p>RMSNorm（Root Mean Square Normalization）是一种用于神经网络中的归一化技术，类似于Batch Normalization和Layer Normalization，但它有其独特的计算方式和应用场景。</p>
<h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><p>RMSNorm的核心思想是通过计算输入数据的均方根（Root Mean Square, RMS）来进行归一化。具体来说，RMSNorm首先计算输入向量的均方根值，然后每个元素除以这个均方根值，从而实现归一化。</p>
<h4 id="计算步骤"><a href="#计算步骤" class="headerlink" title="计算步骤"></a>计算步骤</h4><ol>
<li><p><strong>计算均方根</strong>：对于输入向量 ( x &#x3D; [x_1, x_2, \dots, x_n] )，首先计算其均方根值：<br>$$<br>\text{RMS}(x) &#x3D; \sqrt{\frac{1}{n} \sum_{i&#x3D;1}^{n} x_i^2}<br>$$</p>
</li>
<li><p><strong>归一化</strong>：然后对每个元素进行归一化：<br>$$<br>\hat{x}_i &#x3D; \frac{x_i}{\text{RMS}(x)}<br>$$</p>
</li>
<li><p><strong>缩放和平移</strong>（可选）：与Layer Normalization类似，RMSNorm也可以引入缩放和平移参数，以增加模型的表达能力：<br>$$<br>y_i &#x3D; \gamma \hat{x}_i + \beta<br>$$</p>
<p>其中，(\gamma) 和 (\beta) 是可学习的参数。</p>
</li>
</ol>
<blockquote>
<p>注意：<strong>RMSNorm也是有状态的</strong>，因为它在训练过程中会维护和更新一些内部状态。具体来说，RMSNorm会计算每个特征维度上的均方根值，并在训练过程中使用这些值来缩放输入特征。这些均方根值是根据输入数据动态计算的，并且在训练过程中可能会发生变化。</p>
<p>在推理阶段，RMSNorm会使用训练过程中计算得到的均方根值来对输入进行归一化。因此，RMSNorm的状态（即均方根值）在训练和推理过程中是保持一致的。</p>
</blockquote>
<p>在大型语言模型（LLM）训练中，输入数据的形状通常为 (B, T, C)，其中 (B) 是批次大小（batch size），(T) 是序列长度（sequence length），(C) 是特征维度（feature dimension）。RMSNorm在这种情况下是如何计算的呢？</p>
<p>RMSNorm是按特征维度（即每个神经元的输入）来计算的，而不是按批次（batch）或序列长度（sequence length）来计算的。具体来说，RMSNorm对每个特征维度独立地计算均方根（Root Mean Square），然后使用这个值来缩放该特征维度的输入。</p>
<p>假设输入数据的形状为 (B, T, C)，RMSNorm的计算步骤如下：</p>
<ol>
<li><p><strong>计算均方根</strong>：对于每个特征维度 (c)（即第 (c) 个特征），计算该维度上所有输入值的平方和的均值，然后取平方根：<br>$$<br>\text{RMS}(x_{:,:,c}) &#x3D; \sqrt{\frac{1}{B \times T} \sum_{b&#x3D;1}^{B} \sum_{t&#x3D;1}^{T} x_{b,t,c}^2}<br>$$<br>其中，(x_{b,t,c}) 是第 (b) 个批次、第 (t) 个时间步、第 (c) 个特征的输入值。</p>
</li>
<li><p><strong>缩放输入</strong>：使用计算得到的均方根值来缩放每个特征维度的输入：<br>$$<br>\hat{x}<em>{b,t,c} &#x3D; \frac{x</em>{b,t,c}}{\text{RMS}(x_{:,:,c}) + \epsilon}<br>$$<br>其中，(\epsilon) 是一个很小的常数，用于防止除零错误。</p>
</li>
<li><p><strong>可选的仿射变换</strong>：在缩放之后，RMSNorm还可以应用一个可选的仿射变换（即缩放和偏移）：<br>$$<br>y_{b,t,c} &#x3D; \gamma_c \hat{x}_{b,t,c} + \beta_c<br>$$<br>其中，(\gamma_c) 和 (\beta_c) 是可学习的参数。</p>
</li>
</ol>
<p>总结来说，RMSNorm在计算时是按特征维度 (c) 来计算的，而不是按批次 (B) 或序列长度 (T) 来计算的。每个特征维度的均方根值是独立计算的，并且用于缩放该特征维度的输入。这与Layer Normalization的计算方式类似，只是Layer Normalization在计算均值和方差时考虑了所有特征维度。</p>
<h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><ul>
<li><strong>计算效率高</strong>：RMSNorm的计算复杂度较低，因为它不需要计算均值和方差，只需要计算均方根。</li>
<li><strong>稳定性好</strong>：在一些情况下，RMSNorm比Batch Normalization和Layer Normalization更能保持模型的稳定性，特别是在处理长序列数据时。</li>
</ul>
<h4 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h4><p>RMSNorm是一种通过计算输入向量的均方根来进行归一化的技术，它在计算效率和稳定性方面具有优势，特别适用于处理长序列数据的神经网络模型。</p>
<h3 id="LayerNorm"><a href="#LayerNorm" class="headerlink" title="LayerNorm"></a>LayerNorm</h3><h3 id="BatchNorm"><a href="#BatchNorm" class="headerlink" title="BatchNorm"></a>BatchNorm</h3><p><strong>BatchNorm（批量归一化）层是一个有状态的层。它维护两个状态变量</strong>：</p>
<ol>
<li><p><strong>均值（Mean）</strong>：在训练过程中，BatchNorm层会计算每个批次的均值，并使用这些均值的移动平均来估计整个数据集的均值。这个移动平均值在训练结束后被用作推理阶段的均值。</p>
</li>
<li><p><strong>方差（Variance）</strong>：类似地，BatchNorm层会计算每个批次的方差，并使用这些方差的移动平均来估计整个数据集的方差。这个移动平均值在训练结束后被用作推理阶段的方差。</p>
</li>
</ol>
<p>这些状态变量在训练过程中不断更新，并且在推理阶段被固定下来，用于对输入数据进行归一化。</p>
<p>BatchNorm（批量归一化）在训练阶段和推理阶段的计算过程有所不同：</p>
<ul>
<li><strong>训练阶段</strong>：BatchNorm层使用每个批次的均值和方差进行归一化，并更新全局均值和方差。</li>
<li><strong>推理阶段</strong>：BatchNorm层使用训练阶段估计的全局均值和方差进行归一化，并应用学习到的缩放和平移参数。</li>
</ul>
<h4 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h4><p>在训练阶段，BatchNorm层对每个批次的数据进行归一化处理。具体步骤如下：</p>
<ol>
<li><p><strong>计算批次均值和方差</strong>：</p>
<ul>
<li><p>对于输入的批次数据 ( \mathbf{x} )，计算批次均值 ( \mu_B ) 和批次方差 ( \sigma_B^2 )：<br>$$<br>\mu_B &#x3D; \frac{1}{m} \sum_{i&#x3D;1}^{m} x_i<br>$$</p>
<p>$$<br>\sigma_B^2 &#x3D; \frac{1}{m} \sum_{i&#x3D;1}^{m} (x_i - \mu_B)^2<br>$$</p>
<p>其中 ( m ) 是批次大小，( x_i ) 是批次中的第 ( i ) 个样本。</p>
</li>
</ul>
</li>
<li><p><strong>归一化</strong>：</p>
<ul>
<li>使用批次均值和方差对输入数据进行归一化：<br>$$<br>\hat{x}_i &#x3D; \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}<br>$$<br>其中 ( \epsilon ) 是一个很小的常数，用于防止除零错误。</li>
</ul>
</li>
<li><p><strong>缩放和平移</strong>：</p>
<ul>
<li>对归一化后的数据进行缩放和平移操作：<br>$$<br>y_i &#x3D; \gamma \hat{x}_i + \beta<br>$$<br>其中 ( \gamma ) 和 ( \beta ) 是可学习的参数，分别用于缩放和平移。</li>
</ul>
</li>
<li><p><strong>更新全局均值和方差</strong>：</p>
<ul>
<li><p>使用批次均值和方差的移动平均来更新全局均值 ( \mu_{global} ) 和全局方差 ( \sigma_{global}^2 )：<br>$$<br>\mu_{global} &#x3D; \alpha \mu_{global} + (1 - \alpha) \mu_B<br>$$</p>
<p>$$<br>\sigma_{global}^2 &#x3D; \alpha \sigma_{global}^2 + (1 - \alpha) \sigma_B^2<br>$$</p>
<p>其中 ( \alpha ) 是一个动量参数，通常取值接近1（例如0.9或0.99）。</p>
</li>
</ul>
</li>
</ol>
<h4 id="推理阶段"><a href="#推理阶段" class="headerlink" title="推理阶段"></a>推理阶段</h4><p>在推理阶段，BatchNorm层使用训练阶段估计的全局均值和方差进行归一化。具体步骤如下：</p>
<ol>
<li><p><strong>使用全局均值和方差</strong>：</p>
<ul>
<li>使用训练阶段估计的全局均值 ( \mu_{global} ) 和全局方差 ( \sigma_{global}^2 ) 对输入数据进行归一化：<br>$$<br>\hat{x}<em>i &#x3D; \frac{x_i - \mu</em>{global}}{\sqrt{\sigma_{global}^2 + \epsilon}}<br>$$</li>
</ul>
</li>
<li><p><strong>缩放和平移</strong>：</p>
<ul>
<li>对归一化后的数据进行缩放和平移操作：<br>$$<br>y_i &#x3D; \gamma \hat{x}_i + \beta<br>$$<br>其中 ( \gamma ) 和 ( \beta ) 是训练阶段学习到的参数。</li>
</ul>
</li>
</ol>
<h4 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h4><p>- </p>
<p>这种设计使得BatchNorm层在训练和推理阶段都能有效地工作，同时保持了模型的有状态性。</p>
<h2 id="防过拟合"><a href="#防过拟合" class="headerlink" title="防过拟合"></a>防过拟合</h2><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p><strong>Dropout层在训练阶段和推理阶段的行为有所不同</strong>。</p>
<ul>
<li><strong>训练阶段</strong>：Dropout层随机丢弃一部分输入特征，并对未丢弃的特征进行缩放。</li>
<li><strong>推理阶段</strong>：Dropout层不进行随机丢弃，所有输入特征保持不变。</li>
</ul>
<h4 id="训练阶段-1"><a href="#训练阶段-1" class="headerlink" title="训练阶段"></a>训练阶段</h4><p>在训练阶段，Dropout层会随机“丢弃”（即设置为零）输入特征的一部分。具体步骤如下：</p>
<ol>
<li><p><strong>随机丢弃</strong>：</p>
<ul>
<li>对于输入的每个元素 ( x_i )，以概率 ( p ) 将其设置为零，以概率 ( 1 - p ) 保持不变。</li>
<li>这个过程可以表示为：<br>$$<br>y_i &#x3D; \frac{x_i}{1 - p} \cdot \text{Bernoulli}(1 - p)<br>$$<br>其中 ( \text{Bernoulli}(1 - p) ) 是一个伯努利随机变量，取值为1的概率是 ( 1 - p )，取值为0的概率是 ( p )。</li>
</ul>
</li>
<li><p><strong>缩放</strong>：</p>
<ul>
<li>为了保持期望值不变，Dropout层会对未丢弃的元素进行缩放，通常是除以 ( 1 - p )。</li>
</ul>
</li>
</ol>
<h4 id="推理阶段-1"><a href="#推理阶段-1" class="headerlink" title="推理阶段"></a>推理阶段</h4><p>在推理阶段，Dropout层不会进行随机丢弃操作。具体步骤如下：</p>
<ol>
<li><strong>不丢弃</strong>：<ul>
<li>在推理阶段，所有输入元素都保持不变，不进行随机丢弃。</li>
<li>由于在训练阶段已经对未丢弃的元素进行了缩放（除以 ( 1 - p )），在推理阶段不需要额外的缩放操作。</li>
</ul>
</li>
</ol>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><h3 id="Gradient-Clipping"><a href="#Gradient-Clipping" class="headerlink" title="Gradient Clipping"></a>Gradient Clipping</h3><h3 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h3><p>在监督学习任务中，将真实标签稍微调整，使其不完全是硬目标（例如，将标签从[1, 0, 0]调整为[0.9, 0.05, 0.05]），以鼓励模型学习更多的模式而不是记忆训练数据。</p>
<h2 id="混合精度训练"><a href="#混合精度训练" class="headerlink" title="混合精度训练"></a>混合精度训练</h2><ul>
<li><p>Nvidia卡的fp16计算性能强于fp32,所以混精度训练一般是加速计算的</p>
</li>
<li><p>权重、激活值（activations）都是使用fp16</p>
</li>
<li><p>fp16精度相比fp32小</p>
</li>
<li><p>权重有一个fp32的副本用于权重更新，优化器状态也有一个fp32的副本</p>
</li>
<li><p>假设共 φ 个参数，所以一共占用</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center">(单位：byte)</th>
</tr>
</thead>
<tbody><tr>
<td align="center">W</td>
<td align="center">权重（用于前向）</td>
<td align="center">fp16</td>
<td align="center">2φ</td>
</tr>
<tr>
<td align="center">activations</td>
<td align="center">激活值（用于计算梯度）</td>
<td align="center">fp16</td>
<td align="center">2φ</td>
</tr>
<tr>
<td align="center">W</td>
<td align="center">权重fp32副本（用于更新）</td>
<td align="center">fp32</td>
<td align="center">4φ</td>
</tr>
<tr>
<td align="center">Momentum</td>
<td align="center">Adam优化器状态</td>
<td align="center">fp32</td>
<td align="center">4φ</td>
</tr>
<tr>
<td align="center">Variance</td>
<td align="center">Adam优化器状态</td>
<td align="center">fp32</td>
<td align="center">4φ</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">16φ</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">dW</td>
<td align="center">权重梯度（反向传播时产生）</td>
<td align="center">fp16，W更新时转为fp32</td>
<td align="center">2φ</td>
</tr>
</tbody></table>
<ul>
<li><p>fp16 W：2φ</p>
</li>
<li><p>fp16 activations：2φ</p>
</li>
<li><p>fp16 dW：W梯度</p>
</li>
<li><p>fp32 W：用于更新W的W副本</p>
</li>
<li><p>fp32 Momentum：Adam优化器状态之一</p>
</li>
<li><p>fp32 Variance：Adam优化器状态之一</p>
</li>
</ul>
</li>
</ul>
<h2 id="微批次-梯度累积"><a href="#微批次-梯度累积" class="headerlink" title="微批次 &#x2F; 梯度累积"></a>微批次 &#x2F; 梯度累积</h2><p>在深度学习训练中，”micro batch”（微批次）是指将一个较大的训练批次（batch）进一步细分为更小的子批次。这种做法通常用于处理内存限制或提高训练效率。</p>
<h3 id="主要用途和优点"><a href="#主要用途和优点" class="headerlink" title="主要用途和优点"></a>主要用途和优点</h3><ol>
<li><p><strong>内存优化</strong>：</p>
<ul>
<li>在训练大型模型或使用较大批次时，GPU 内存可能会成为瓶颈。通过将一个批次分成多个微批次，可以减少每个微批次所需的内存，从而能够在有限的 GPU 内存中训练更大的模型或使用更大的批次。</li>
</ul>
</li>
<li><p><strong>梯度累积</strong>：</p>
<ul>
<li><p>通过在每个微批次上计算梯度，并在多个微批次之间累积这些梯度，可以模拟更大批次的效果。这种方法称为梯度累积（Gradient Accumulation）。最终，当所有微批次的梯度累积完成后，再进行一次参数更新。</p>
<blockquote>
<p>一般在不使用<strong>micro-batch（微批次）</strong>的时候，计算过程通常是先计算每个样本的梯度，然后计算整个批次的平均梯度。</p>
<p>所以在此之前：<br>$$<br>\delta \bar{W} &#x3D; \frac{\sum_{i&#x3D;1}^{batch_size}\Delta W_{sample_i}}{batch_size}<br>$$<br>使用<strong>micro-batch（微批次）</strong>：</p>
<ol>
<li><p>计算micro batch梯度和<br>$$<br>\delta W_{micro_j} &#x3D; \sum_{i&#x3D;1}^{micro_batch_size}\Delta W_{sample_i}\quad j\in[1,n_micro_batch]<br>$$</p>
</li>
<li><p><strong>梯度累积</strong><br>$$<br>\delta W_{total} &#x3D;\sum_{j&#x3D;1}^{n_micro_batch}\delta W_{micro_j}<br>$$</p>
</li>
<li><p>计算总平均梯度<br>$$<br>\delta \bar W &#x3D; \frac{\delta W_{total}}{batch_size&#x3D; (n_micro_batch \times micro_batch_size)}<br>$$<br>也就是计算总梯度和的方法变了一下。</p>
</li>
<li><p>在单GPU上：</p>
<p>gradient_accumulation_steps &#x3D; n_micro_batch &#x3D; 微批量个数 &#x3D; batch_size &#x2F; micro_batch_size</p>
<p>在多GPU上，可以每个GPU先算出他所在的GPU上的部分微批次梯度和，然后汇总所有GPU的微批次和。</p>
<p>batch_size &#x3D; micro_batch_size_per_gpu * n_micro_batch_per_gpu * n_gpu</p>
</li>
</ol>
</blockquote>
</li>
<li><p>梯度累积使得即使在没有足够内存支持大批次的情况下，也能实现大批次训练的效果。</p>
</li>
</ul>
</li>
<li><p><strong>提高训练效率</strong>：</p>
<ul>
<li>在某些情况下，使用微批次可以提高训练效率。例如，当数据加载或预处理成为瓶颈时，使用微批次可以更好地利用计算资源。</li>
</ul>
</li>
</ol>
<h2 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h2><p>Beam Search（束搜索）是一种在序列生成任务中常用的启发式搜索算法，特别是在自然语言处理（NLP）领域，如机器翻译、文本摘要和对话系统中。与贪心搜索（Greedy Search）不同，Beam Search在每一步都保留多个候选序列，从而增加了找到更好解的可能性。</p>
<p>基于输⼊序列⽣成输出序列的条件概率是：<br>$$<br>\prod^{T’}<em>{t’&#x3D;1}P(y</em>{t’}|y_1,\cdots,y_{t’-1},c)<br>$$<br>也就是所有生成的token的概率的乘积，我们<strong>将该条件概率最⼤的输出序列称为最优输出序列</strong>。而<strong>贪婪搜索的主要问题是不能保证得到最优输出序列</strong>。</p>
<h3 id="贪婪搜索"><a href="#贪婪搜索" class="headerlink" title="贪婪搜索"></a>贪婪搜索</h3><p>贪婪搜索（greedy search）。对于输出序列任⼀时间步t ′，我 们从|Y|个词中搜索出<strong>条件概率最⼤的词作为输出</strong>。</p>
<p>⼀旦搜索出“<code>&lt;EOS&gt;</code>”符号，或者输出序列⻓度已经达到了最⼤⻓度T ′，便完成输出。<br>$$<br>y_{t’}&#x3D;\text{argmax}<em>{y\in Y}P(y|y_1,\cdots,y</em>{t’-1},c)<br>$$</p>
<h3 id="贪婪搜索会产生非最优输出序列"><a href="#贪婪搜索会产生非最优输出序列" class="headerlink" title="贪婪搜索会产生非最优输出序列"></a>贪婪搜索会产生非最优输出序列</h3><p><strong>因为贪婪搜索只考虑当前token的最大概率，而不考虑选择该token之后生成的token的概率值。</strong></p>
<p>假设输出词典⾥⾯有“A”“B”“C”和“<code>&lt;EOS&gt;</code>”这4个词。图10.9中每个时间步下的4个数字分别代表了该时间步⽣成“A”“B”“C”和“<code>&lt;EOS&gt;</code>”这4个词的条件概率。在每个时间步，贪婪搜索选取条件概率最⼤的词。因此，图10.9中将⽣成输出序列“A”“B”“C”“<code>&lt;EOS&gt;</code>”。 该输出序列的条件概率是0.5 × 0.4 × 0.4 × 0.6 &#x3D; 0.048。</p>
<img src="\llm\image-20240907154955389.png" alt="image-20240907154955389" style="zoom: 50%;" />

<p>如果时间步2选取了条件概率第⼆⼤的 词“C”。由于时间步3所基于的时间步1和2的输出⼦序列由图10.9中的“A”“B”变为了图10.10中的 “A”“C”，图10.10中时间步3⽣成各个词的条件概率发⽣了变化。我们选取条件概率最⼤的词“B”。 此时时间步4所基于的前3个时间步的输出⼦序列为“A”“C”“B”，与图10.9中的“A”“B”“C” 不同。因此，图10.10中时间步4⽣成各个词的条件概率也与图10.9中的不同。我们发现，此时的 输出序列“A”“C”“B”“<code>&lt;EOS&gt;</code>”的条件概率是0.5 × 0.3 × 0.6 × 0.6 &#x3D; 0.054，⼤于贪婪搜索得到的输出序列的条件概率。因此，贪婪搜索得到的输出序列“A”“B”“C”“<code>&lt;EOS&gt;</code>”并⾮最优输出序列。</p>
<img src="\llm\image-20240907155054741.png" alt="image-20240907155054741" style="zoom:50%;" />

<h3 id="穷举搜索"><a href="#穷举搜索" class="headerlink" title="穷举搜索"></a>穷举搜索</h3><p>如果⽬标是得到最优输出序列，我们可以考虑穷举搜索（exhaustive search）：穷举所有可能的 输出序列，输出条件概率最⼤的序列。 <strong>虽然穷举搜索可以得到最优输出序列，但它的计算开销很容易过⼤</strong>。例如，当|Y| &#x3D; 10000且T ′ &#x3D; 10时，我们将评估10000^10 &#x3D; 10^40个序列：这⼏乎不可能完成。而贪婪搜索的计 算开销是O(|Y| T ′ )，通常显著小于穷举搜索的计算开销。例如，当|Y| &#x3D; 10000且T ′ &#x3D; 10时，我 们只需评估10000 × 10 &#x3D; 10^5个序列。</p>
<h3 id="束搜索基本原理"><a href="#束搜索基本原理" class="headerlink" title="束搜索基本原理"></a>束搜索基本原理</h3><p>束搜索（beam search）是对贪婪搜索的⼀个改进算法。它有⼀个束宽（beam size）超参数。我 们将它设为k。在时间步1时，选取当前时间步条件概率最⼤的k个词，分别组成k个候选输出序列 的⾸词。在之后的每个时间步，基于上个时间步的k个候选输出序列，从k |Y|个可能的输出序列 中选取条件概率最⼤的k个，作为该时间步的候选输出序列。最终，我们从各个时间步的候选输 出序列中筛选出包含特殊符号“”的序列，并将它们中所有特殊符号“”后⾯的⼦序 列舍弃，得到最终候选输出序列的集合。</p>
<ol>
<li><strong>初始化</strong>：从初始状态开始，生成所有可能的下一个状态（通常是生成一个词或字符）。</li>
<li><strong>选择</strong>：根据某种评分函数（如语言模型的概率），选择得分最高的 <code>k</code> 个候选序列，其中 <code>k</code> 是束宽（Beam Width）。</li>
<li><strong>扩展</strong>：对这 <code>k</code> 个候选序列中的每一个，生成所有可能的下一个状态。</li>
<li><strong>更新</strong>：从所有新生成的序列中，再次选择得分最高的 <code>k</code> 个候选序列。</li>
<li><strong>重复</strong>：重复上述步骤，直到满足终止条件（如达到最大长度或生成终止符）。</li>
</ol>
<h3 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h3><ul>
<li><strong>更好的解</strong>：相比于贪心搜索，Beam Search能够探索更多的可能性，因此更有可能找到全局最优或接近最优的解。</li>
<li><strong>灵活性</strong>：可以通过调整束宽 <code>k</code> 来平衡搜索的深度和广度。较大的 <code>k</code> 可以提高找到更好解的概率，但也会增加计算复杂度。</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li><strong>计算复杂度</strong>：随着束宽 <code>k</code> 的增加，计算复杂度显著增加，尤其是在序列长度较长时。</li>
<li><strong>局部最优</strong>：尽管Beam Search比贪心搜索更好，但它仍然可能陷入局部最优，尤其是在束宽较小的情况下。</li>
</ul>
<h3 id="应用场景-3"><a href="#应用场景-3" class="headerlink" title="应用场景"></a>应用场景</h3><ul>
<li><strong>机器翻译</strong>：在生成目标语言句子时，Beam Search可以帮助找到更自然的翻译。</li>
<li><strong>文本摘要</strong>：在生成摘要时，Beam Search可以帮助生成更连贯和信息量更大的摘要。</li>
<li><strong>对话系统</strong>：在生成对话回复时，Beam Search可以帮助生成更合适的回复。</li>
</ul>
<h2 id="评测指标"><a href="#评测指标" class="headerlink" title="评测指标"></a>评测指标</h2><h3 id="F1"><a href="#F1" class="headerlink" title="F1"></a>F1</h3><p>F1指标（F1 Score）是机器学习和统计学中用于衡量分类模型性能的一种指标。它结合了精确率（Precision）和召回率（Recall），提供了一个单一的数值来评估模型的整体性能。F1指标特别适用于处理不平衡数据集，即正负样本比例严重失衡的情况。</p>
<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>F1指标是精确率和召回率的调和平均数，其公式如下：</p>
<p>$$<br>F1 &#x3D; 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}<br>$$</p>
<h4 id="精确率（Precision）"><a href="#精确率（Precision）" class="headerlink" title="精确率（Precision）"></a>精确率（Precision）</h4><p>精确率是指模型预测为正类的样本中，实际为正类的比例。它衡量了模型预测的准确性。</p>
<p>$$<br>\text{Precision} &#x3D; \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}<br>$$</p>
<h4 id="召回率（Recall）"><a href="#召回率（Recall）" class="headerlink" title="召回率（Recall）"></a>召回率（Recall）</h4><p>召回率是指实际为正类的样本中，被模型正确预测为正类的比例。它衡量了模型找到所有正类样本的能力。</p>
<p>$$<br>\text{Recall} &#x3D; \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}<br>$$</p>
<h4 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h4><ul>
<li><strong>F1指标的取值范围</strong>：0到1之间。值越接近1，表示模型的性能越好。</li>
<li><strong>F1指标的优点</strong>：它同时考虑了精确率和召回率，因此能够平衡模型的准确性和覆盖率。</li>
<li><strong>适用场景</strong>：特别适用于不平衡数据集，因为它不会像准确率（Accuracy）那样受到类别不平衡的影响。</li>
</ul>
<h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h4><p>假设一个二分类模型在测试集上的表现如下：</p>
<ul>
<li>真阳性（TP）：50</li>
<li>假阳性（FP）：10</li>
<li>假阴性（FN）：15</li>
</ul>
<p>计算精确率和召回率：</p>
<p>$$<br>\text{Precision} &#x3D; \frac{50}{50 + 10} &#x3D; \frac{50}{60} \approx 0.833<br>$$</p>
<p>$$<br>\text{Recall} &#x3D; \frac{50}{50 + 15} &#x3D; \frac{50}{65} \approx 0.769<br>$$<br>计算F1指标：</p>
<p>$$<br>F1 &#x3D; 2 \times \frac{0.833 \times 0.769}{0.833 + 0.769} \approx 0.8<br>$$</p>
<h3 id="Micro-F1-Macro-F1"><a href="#Micro-F1-Macro-F1" class="headerlink" title="Micro-F1 &amp; Macro-F1"></a>Micro-F1 &amp; Macro-F1</h3><p>在多分类问题中，我们通常会使用微平均（micro-averaging）和宏平均（macro-averaging）来计算F1指标。这两种方法在计算F1指标时有所不同，主要区别在于它们如何处理不同类别的贡献。</p>
<h4 id="宏平均（Macro-F1）"><a href="#宏平均（Macro-F1）" class="headerlink" title="宏平均（Macro-F1）"></a>宏平均（Macro-F1）</h4><p>宏平均是对每个类别的F1指标进行简单平均。具体步骤如下：</p>
<ol>
<li><strong>计算每个类别的精确率和召回率</strong>：<ul>
<li>对于每个类别，分别计算其精确率（Precision）和召回率（Recall）。</li>
</ul>
</li>
<li><strong>计算每个类别的F1指标</strong>：<ul>
<li>使用每个类别的精确率和召回率计算其F1指标。</li>
</ul>
</li>
<li><strong>对所有类别的F1指标求平均</strong>：<ul>
<li>将所有类别的F1指标进行简单平均，得到宏平均F1指标（Macro-F1）。</li>
</ul>
</li>
</ol>
<p>公式如下：</p>
<p>$$<br>\text{Macro-Precision} &#x3D; \frac{1}{C} \sum_{i&#x3D;1}^{C} \text{Precision}_i<br>$$</p>
<p>$$<br>\text{Macro-Recall} &#x3D; \frac{1}{C} \sum_{i&#x3D;1}^{C} \text{Recall}_i<br>$$</p>
<p>$$<br>\text{Macro-F1} &#x3D; 2 \times \frac{\text{Macro-Precision} \times \text{Macro-Recall}}{\text{Macro-Precision} + \text{Macro-Recall}}<br>$$</p>
<p>其中，( C ) 是类别的总数。</p>
<h4 id="微平均（Micro-F1）"><a href="#微平均（Micro-F1）" class="headerlink" title="微平均（Micro-F1）"></a>微平均（Micro-F1）</h4><p>微平均是先对所有类别的真阳性（TP）、假阳性（FP）和假阴性（FN）进行全局统计，然后计算整体的精确率和召回率，最后计算F1指标。具体步骤如下：</p>
<ol>
<li><strong>全局统计</strong>：<ul>
<li>将所有类别的真阳性（TP）、假阳性（FP）和假阴性（FN）进行累加，得到全局统计量。</li>
</ul>
</li>
<li><strong>计算整体的精确率和召回率</strong>：<ul>
<li>使用全局统计量计算整体的精确率（Precision）和召回率（Recall）。</li>
</ul>
</li>
<li><strong>计算微平均F1指标</strong>：<ul>
<li>使用整体的精确率和召回率计算微平均F1指标（Micro-F1）。</li>
</ul>
</li>
</ol>
<p>公式如下：</p>
<p>$$<br>\text{Micro-Precision} &#x3D; \frac{\sum_{i&#x3D;1}^{C} \text{TP}<em>i}{\sum</em>{i&#x3D;1}^{C} \text{TP}<em>i + \sum</em>{i&#x3D;1}^{C} \text{FP}_i}<br>$$</p>
<p>$$<br>\text{Micro-Recall} &#x3D; \frac{\sum_{i&#x3D;1}^{C} \text{TP}<em>i}{\sum</em>{i&#x3D;1}^{C} \text{TP}<em>i + \sum</em>{i&#x3D;1}^{C} \text{FN}_i}<br>$$</p>
<p>$$<br>\text{Micro-F1} &#x3D; 2 \times \frac{\text{Micro-Precision} \times \text{Micro-Recall}}{\text{Micro-Precision} + \text{Micro-Recall}}<br>$$</p>
<h4 id="区别总结"><a href="#区别总结" class="headerlink" title="区别总结"></a>区别总结</h4><ul>
<li><strong>宏平均（Macro-F1）</strong>：对每个类别的F1指标进行简单平均，不考虑类别样本数量的差异。它更关注每个类别的性能，适用于类别分布较为均匀的情况。</li>
<li><strong>微平均（Micro-F1）</strong>：先对所有类别的TP、FP和FN进行全局统计，然后计算整体的精确率和召回率。它更关注整体性能，适用于类别分布不均匀的情况。</li>
</ul>
<h4 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h4><ul>
<li><strong>宏平均</strong>：适用于类别分布较为均匀的情况，能够更好地反映每个类别的性能。</li>
<li><strong>微平均</strong>：适用于类别分布不均匀的情况，能够更好地反映整体性能。</li>
</ul>
<h4 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h4><p>假设有一个三分类问题，各类别的TP、FP和FN如下：</p>
<table>
<thead>
<tr>
<th>类别</th>
<th>TP</th>
<th>FP</th>
<th>FN</th>
</tr>
</thead>
<tbody><tr>
<td>A</td>
<td>10</td>
<td>5</td>
<td>2</td>
</tr>
<tr>
<td>B</td>
<td>5</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>C</td>
<td>2</td>
<td>1</td>
<td>3</td>
</tr>
</tbody></table>
<p>计算宏平均和微平均F1指标：</p>
<h4 id="宏平均"><a href="#宏平均" class="headerlink" title="宏平均"></a>宏平均</h4><ol>
<li>计算每个类别的精确率和召回率：<ul>
<li>类别A：Precision &#x3D; 10 &#x2F; (10 + 5) &#x3D; 0.67, Recall &#x3D; 10 &#x2F; (10 + 2) &#x3D; 0.83</li>
<li>类别B：Precision &#x3D; 5 &#x2F; (5 + 3) &#x3D; 0.625, Recall &#x3D; 5 &#x2F; (5 + 4) &#x3D; 0.556</li>
<li>类别C：Precision &#x3D; 2 &#x2F; (2 + 1) &#x3D; 0.667, Recall &#x3D; 2 &#x2F; (2 + 3) &#x3D; 0.4</li>
</ul>
</li>
<li>计算每个类别的F1指标：<ul>
<li>类别A：F1 &#x3D; 2 * (0.67 * 0.83) &#x2F; (0.67 + 0.83) ≈ 0.74</li>
<li>类别B：F1 &#x3D; 2 * (0.625 * 0.556) &#x2F; (0.625 + 0.556) ≈ 0.59</li>
<li>类别C：F1 &#x3D; 2 * (0.667 * 0.4) &#x2F; (0.667 + 0.4) ≈ 0.5</li>
</ul>
</li>
<li>计算宏平均F1指标：<ul>
<li>Macro-F1 &#x3D; (0.74 + 0.59 + 0.5) &#x2F; 3 ≈ 0.61</li>
</ul>
</li>
</ol>
<h4 id="微平均"><a href="#微平均" class="headerlink" title="微平均"></a>微平均</h4><ol>
<li>全局统计：<ul>
<li>TP &#x3D; 10 + 5 + 2 &#x3D; 17</li>
<li>FP &#x3D; 5 + 3 + 1 &#x3D; 9</li>
<li>FN &#x3D; 2 + 4 + 3 &#x3D; 9</li>
</ul>
</li>
<li>计算整体的精确率和召回率：<ul>
<li>Micro-Precision &#x3D; 17 &#x2F; (17 + 9) ≈ 0.654</li>
<li>Micro-Recall &#x3D; 17 &#x2F; (17 + 9) ≈ 0.654</li>
</ul>
</li>
<li>计算微平均F1指标：<ul>
<li>Micro-F1 &#x3D; 2 * (0.654 * 0.654) &#x2F; (0.654 + 0.654) ≈ 0.654</li>
</ul>
</li>
</ol>
<h4 id="总结-5"><a href="#总结-5" class="headerlink" title="总结"></a>总结</h4><ul>
<li><strong>宏平均F1</strong>：0.61</li>
<li><strong>微平均F1</strong>：0.654</li>
</ul>
<p>宏平均和微平均F1指标在不同情况下有不同的适用性，选择哪种方法取决于具体问题的需求和数据分布。</p>
<h2 id="知识蒸馏"><a href="#知识蒸馏" class="headerlink" title="知识蒸馏"></a>知识蒸馏</h2><p>知识蒸馏（Knowledge Distillation）是一种机器学习技术，主要用于模型压缩和知识传递。它的核心思想是通过训练一个较小的模型（称为学生模型）来模仿一个较大的、已经训练好的模型（称为教师模型）的输出行为。这个过程可以帮助学生模型学习到教师模型的知识，从而在保持较高性能的同时，<strong>减少模型的复杂度和计算资源的需求</strong>。</p>
<h3 id="主要步骤"><a href="#主要步骤" class="headerlink" title="主要步骤"></a>主要步骤</h3><ol>
<li><p><strong>教师模型的训练</strong>：</p>
<ul>
<li>首先，训练一个复杂的、性能较好的模型（教师模型）。这个模型通常是一个深度神经网络，具有大量的参数和层数。</li>
</ul>
</li>
<li><p><strong>学生模型的训练</strong>：</p>
<ul>
<li>接下来，训练一个较小的模型（学生模型），目标是使其输出尽可能接近教师模型的输出。学生模型通常比教师模型更简单，参数更少。</li>
</ul>
</li>
<li><p><strong>知识传递</strong>：</p>
<ul>
<li>在训练学生模型时，不仅使用真实标签（硬标签），还使用教师模型的输出（软标签）作为额外的监督信号。软标签通常包含更多的信息，因为它们反映了教师模型对不同类别的相对置信度。</li>
</ul>
</li>
<li><p><strong>损失函数</strong>：</p>
<ul>
<li>训练学生模型的损失函数通常包括两部分：一部分是学生模型输出与真实标签之间的损失（如交叉熵损失），另一部分是学生模型输出与教师模型输出之间的损失（如KL散度）。</li>
</ul>
</li>
</ol>
<h3 id="优点-3"><a href="#优点-3" class="headerlink" title="优点"></a>优点</h3><ul>
<li><strong>模型压缩</strong>：知识蒸馏可以将复杂的教师模型压缩成一个更小的学生模型，减少计算资源和存储需求。</li>
<li><strong>性能保持</strong>：尽管学生模型较小，但通过模仿教师模型的行为，它仍然可以保持较高的性能。</li>
<li><strong>知识传递</strong>：学生模型不仅学习到真实标签的知识，还学习到教师模型的高级特征和决策过程。</li>
</ul>
<h3 id="应用场景-4"><a href="#应用场景-4" class="headerlink" title="应用场景"></a>应用场景</h3><ul>
<li><strong>移动设备和嵌入式系统</strong>：由于资源限制，这些设备通常无法运行大型模型，知识蒸馏可以将复杂的模型压缩成适合这些设备的轻量级模型。</li>
<li><strong>实时应用</strong>：在需要快速响应的应用中，知识蒸馏可以减少模型的推理时间，提高效率。</li>
<li><strong>大规模分布式系统</strong>：在分布式系统中，知识蒸馏可以帮助减少通信开销和计算负载。</li>
</ul>
<h3 id="总结-6"><a href="#总结-6" class="headerlink" title="总结"></a>总结</h3><p>知识蒸馏是一种有效的模型压缩和知识传递技术，通过训练一个较小的学生模型来模仿一个复杂的教师模型，可以在保持高性能的同时，显著减少模型的复杂度和计算资源的需求。</p>
<h2 id="HMM-Hidden-Markov-Model"><a href="#HMM-Hidden-Markov-Model" class="headerlink" title="HMM (Hidden Markov Model)"></a>HMM <strong>(Hidden Markov Model)</strong></h2><ul>
<li><strong>含义</strong>：隐马尔可夫模型是一种统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。在NLP中，HMM常用于词性标注、语音识别等任务。</li>
<li><strong>应用</strong>：HMM假设观测序列是由一个不可见的马尔可夫链生成的，这个链的状态不能直接观测到，但可以通过观测序列来推断。</li>
</ul>
<h2 id="CRF-Conditional-Random-Field"><a href="#CRF-Conditional-Random-Field" class="headerlink" title="CRF (Conditional Random Field)"></a>CRF <strong>(Conditional Random Field)</strong></h2><ul>
<li><strong>含义</strong>：条件随机场是一种判别式概率模型，它在给定一组输入随机变量条件下，对另一组输出随机变量的条件概率进行建模。在NLP中，CRF常用于序列标注任务，如命名实体识别、词性标注等。</li>
<li><strong>应用</strong>：与HMM不同，CRF是判别模型，它直接对条件概率进行建模，可以更好地处理上下文信息和特征工程。</li>
</ul>
<h2 id="LDA-Latent-Dirichlet-Allocation"><a href="#LDA-Latent-Dirichlet-Allocation" class="headerlink" title="LDA (Latent Dirichlet Allocation)"></a>LDA <strong>(Latent Dirichlet Allocation)</strong></h2><ul>
<li><strong>含义</strong>：潜在狄利克雷分配是一种主题模型，它假设每个文档是由多个主题混合而成的，而每个主题又是一组词的概率分布。在NLP中，LDA常用于文本挖掘、主题建模等任务。</li>
<li><strong>应用</strong>：LDA可以帮助我们从大量文档中提取出潜在的主题结构，每个主题由一组词表示，每个文档则由这些主题的混合表示。</li>
</ul>
<h2 id="MinHash"><a href="#MinHash" class="headerlink" title="MinHash"></a>MinHash</h2><p>MinHash是一种用于高效估计集合相似度的技术，特别适用于处理大规模数据集。它的核心思想是通过哈希函数将集合中的元素映射到一个较小的空间，从而快速计算集合之间的Jaccard相似度。</p>
<h3 id="MinHash的工作原理"><a href="#MinHash的工作原理" class="headerlink" title="MinHash的工作原理"></a>MinHash的工作原理</h3><ol>
<li><p><strong>哈希函数</strong>：首先，选择多个（通常是几百个）不同的哈希函数。每个哈希函数将集合中的元素映射到一个整数值。</p>
</li>
<li><p><strong>最小哈希值</strong>：对于每个哈希函数，计算集合中所有元素的哈希值，并记录下最小的哈希值。这个最小的哈希值被称为“最小哈希值”。</p>
</li>
<li><p><strong>签名矩阵</strong>：将所有哈希函数的最小哈希值组合成一个向量，这个向量被称为集合的“签名”。多个集合的签名可以组成一个签名矩阵。</p>
</li>
<li><p><strong>相似度估计</strong>：通过比较两个集合的签名，可以估计它们的Jaccard相似度。具体来说，如果两个集合的签名中有许多相同的最小哈希值，那么它们的Jaccard相似度就较高。</p>
</li>
</ol>
<h3 id="为什么使用MinHash？"><a href="#为什么使用MinHash？" class="headerlink" title="为什么使用MinHash？"></a>为什么使用MinHash？</h3><ul>
<li><strong>高效性</strong>：MinHash通过哈希函数将集合压缩成较小的签名，大大减少了计算相似度的时间复杂度。</li>
<li><strong>适用于大规模数据</strong>：特别适合处理大规模数据集，因为它不需要直接比较集合中的所有元素。</li>
<li><strong>广泛应用</strong>：常用于搜索引擎中的文档相似度计算、推荐系统中的用户兴趣匹配、以及数据库中的重复数据检测等场景。</li>
</ul>
<h3 id="示例-3"><a href="#示例-3" class="headerlink" title="示例"></a>示例</h3><p>假设有两个集合A和B：</p>
<ul>
<li>集合A &#x3D; {a, b, c, d}</li>
<li>集合B &#x3D; {b, c, e, f}</li>
</ul>
<p>选择两个哈希函数h1和h2，计算每个元素的哈希值：</p>
<table>
<thead>
<tr>
<th>元素</th>
<th>h1(x)</th>
<th>h2(x)</th>
</tr>
</thead>
<tbody><tr>
<td>a</td>
<td>3</td>
<td>5</td>
</tr>
<tr>
<td>b</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>c</td>
<td>4</td>
<td>1</td>
</tr>
<tr>
<td>d</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>e</td>
<td>5</td>
<td>4</td>
</tr>
<tr>
<td>f</td>
<td>6</td>
<td>6</td>
</tr>
</tbody></table>
<p>对于集合A，计算每个哈希函数的最小哈希值：</p>
<ul>
<li>h1(A) &#x3D; min(3, 1, 4, 2) &#x3D; 1</li>
<li>h2(A) &#x3D; min(5, 2, 1, 3) &#x3D; 1</li>
</ul>
<p>对于集合B，计算每个哈希函数的最小哈希值：</p>
<ul>
<li>h1(B) &#x3D; min(1, 4, 5, 6) &#x3D; 1</li>
<li>h2(B) &#x3D; min(2, 1, 4, 6) &#x3D; 1</li>
</ul>
<p>因此，集合A和B的签名分别是[1, 1]和[1, 1]。由于签名完全相同，可以估计它们的Jaccard相似度非常高。</p>
<p>通过这种方式，MinHash能够在不直接比较集合中所有元素的情况下，高效地估计集合之间的相似度。</p>
<h2 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h2><h3 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h3><p>KL散度（Kullback-Leibler Divergence），也称为相对熵（Relative Entropy），是信息论中的一个重要概念，<strong>用于衡量两个概率分布之间的差异</strong>。具体来说，KL散度衡量的是当我们用一个概率分布 ( Q ) 来近似另一个概率分布 ( P ) 时，所损失的信息量。</p>
<h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><p>给定两个概率分布 ( P ) 和 ( Q )，KL散度 ( D_{KL}(P \parallel Q) ) 定义为：_<br>$$<br>D_{KL}(P \parallel Q) &#x3D; \sum_{i} P(i) \log \left( \frac{P(i)}{Q(i)} \right)<br>$$<br>或者在连续情况下：</p>
<p>$$<br>D_{KL}(P \parallel Q) &#x3D; \int_{-\infty}^{\infty} P(x) \log \left( \frac{P(x)}{Q(x)} \right) dx<br>$$</p>
<h4 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h4><ol>
<li><p><strong>非负性</strong>：KL散度总是非负的，即 ( D_{KL}(P \parallel Q) \geq 0 )。当且仅当 ( P ) 和 ( Q ) 完全相同时，KL散度为零。</p>
</li>
<li><p><strong>不对称性</strong>：KL散度不是对称的，即 ( D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P) )。这意味着用 ( Q ) 近似 ( P ) 和用 ( P ) 近似 ( Q ) 是不同的。</p>
</li>
<li><p><strong>非度量性</strong>：KL散度不满足三角不等式，因此它不是一个度量（metric）。</p>
</li>
</ol>
<h4 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h4><p>假设我们有两个离散概率分布 ( P ) 和 ( Q )：</p>
<p>$$<br>P &#x3D; {0.6, 0.4}, \quad Q &#x3D; {0.5, 0.5}<br>$$<br>计算KL散度：</p>
<p>$$<br>D_{KL}(P \parallel Q) &#x3D; 0.6 \log \left( \frac{0.6}{0.5} \right) + 0.4 \log \left( \frac{0.4}{0.5} \right)<br>$$<br>计算每一项：</p>
<p>$$<br>0.6 \log \left( \frac{0.6}{0.5} \right) \approx 0.6 \times 0.1823 &#x3D; 0.1094<br>$$</p>
<p>$$<br>0.4 \log \left( \frac{0.4}{0.5} \right) \approx 0.4 \times (-0.2231) &#x3D; -0.0892<br>$$</p>
<p>因此：</p>
<p>$$<br>D_{KL}(P \parallel Q) \approx 0.1094 - 0.0892 &#x3D; 0.0202<br>$$<br>这个值表示用 ( Q ) 近似 ( P ) 时，信息损失约为0.0202。</p>
<h3 id="JS散度"><a href="#JS散度" class="headerlink" title="JS散度"></a>JS散度</h3><p><strong>一种对称的衡量两个概率分布之间差异的算法</strong>，称为<strong>Jensen-Shannon散度</strong>（Jensen-Shannon Divergence，简称JSD）。JSD是基于KL散度的一种对称且平滑的度量方法。</p>
<h4 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h4><p>给定两个概率分布 ( P ) 和 ( Q )，Jensen-Shannon散度 ( JSD(P \parallel Q) ) 定义为：</p>
<p>$$<br>JSD(P \parallel Q) &#x3D; \frac{1}{2} D_{KL}(P \parallel M) + \frac{1}{2} D_{KL}(Q \parallel M)<br>$$<br>其中，( M ) 是 ( P ) 和 ( Q ) 的平均分布：</p>
<p>$$<br>M &#x3D; \frac{P + Q}{2}<br>$$</p>
<h4 id="性质-1"><a href="#性质-1" class="headerlink" title="性质"></a>性质</h4><ol>
<li><p><strong>对称性</strong>：Jensen-Shannon散度是对称的，即 ( JSD(P \parallel Q) &#x3D; JSD(Q \parallel P) )。</p>
</li>
<li><p><strong>非负性</strong>：JSD总是非负的，即 ( JSD(P \parallel Q) \geq 0 )。当且仅当 ( P ) 和 ( Q ) 完全相同时，JSD为零。</p>
</li>
<li><p><strong>平滑性</strong>：JSD在 ( P ) 和 ( Q ) 之间是平滑的，而KL散度在某些情况下可能不连续。</p>
</li>
<li><p><strong>有界性</strong>：JSD的值在0到1之间（假设 ( P ) 和 ( Q ) 是归一化的概率分布）。</p>
</li>
</ol>
<h4 id="例子-2"><a href="#例子-2" class="headerlink" title="例子"></a>例子</h4><p>假设我们有两个离散概率分布 ( P ) 和 ( Q )：</p>
<p>$$<br>P &#x3D; {0.6, 0.4}, \quad Q &#x3D; {0.5, 0.5}<br>$$<br>首先计算平均分布 ( M )：</p>
<p>$$<br>M &#x3D; \frac{P + Q}{2} &#x3D; \left{ \frac{0.6 + 0.5}{2}, \frac{0.4 + 0.5}{2} \right} &#x3D; {0.55, 0.45}<br>$$<br>然后计算KL散度 ( D_{KL}(P \parallel M) ) 和 ( D_{KL}(Q \parallel M) )：</p>
<p>$$<br>D_{KL}(P \parallel M) &#x3D; 0.6 \log \left( \frac{0.6}{0.55} \right) + 0.4 \log \left( \frac{0.4}{0.45} \right)<br>$$</p>
<p>$$<br>D_{KL}(Q \parallel M) &#x3D; 0.5 \log \left( \frac{0.5}{0.55} \right) + 0.5 \log \left( \frac{0.5}{0.45} \right)<br>$$<br>计算每一项：</p>
<p>$$<br>0.6 \log \left( \frac{0.6}{0.55} \right) \approx 0.6 \times 0.087 &#x3D; 0.0522<br>$$</p>
<p>$$<br>0.4 \log \left( \frac{0.4}{0.45} \right) \approx 0.4 \times (-0.105) &#x3D; -0.042<br>$$</p>
<p>$$<br>D_{KL}(P \parallel M) \approx 0.0522 - 0.042 &#x3D; 0.0102<br>$$</p>
<p>$$<br>0.5 \log \left( \frac{0.5}{0.55} \right) \approx 0.5 \times (-0.094) &#x3D; -0.047<br>$$</p>
<p>$$<br>0.5 \log \left( \frac{0.5}{0.45} \right) \approx 0.5 \times 0.105 &#x3D; 0.0525<br>$$</p>
<p>$$<br>D_{KL}(Q \parallel M) \approx -0.047 + 0.0525 &#x3D; 0.0055<br>$$<br>最后，计算JSD：</p>
<p>$$<br>JSD(P \parallel Q) &#x3D; \frac{1}{2} (0.0102 + 0.0055) &#x3D; 0.00785<br>$$</p>
<h3 id="Hessian-Matrix"><a href="#Hessian-Matrix" class="headerlink" title="Hessian Matrix"></a>Hessian Matrix</h3><p>是一个多元函数的二阶偏导数构成的方阵，描述了函数的局部曲率。对于一个具有 ( n ) 个变量的函数 ( f(x_1, x_2, \dots, x_n) )，其Hessian矩阵 ( \mathbf{H} ) 定义为：<br>$$<br>\mathbf{H} &#x3D; \begin{pmatrix}<br>\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \<br>\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n^2}<br>\end{pmatrix}<br>$$<br>其中，矩阵的每个元素 ( H_{ij} &#x3D; \frac{\partial^2 f}{\partial x_i \partial x_j} ) 是函数 ( f ) 对变量 ( x_i ) 和 ( x_j ) 的二阶偏导数。</p>
<h4 id="重要性质"><a href="#重要性质" class="headerlink" title="重要性质"></a>重要性质</h4><ol>
<li><p><strong>对称性</strong>：如果函数 ( f ) 的二阶偏导数连续，那么Hessian矩阵是对称的，即 ( H_{ij} &#x3D; H_{ji} )。</p>
</li>
<li><p><strong>局部极值判定</strong>：</p>
<ul>
<li>在函数的临界点（即一阶导数为零的点），Hessian矩阵的特征值可以帮助判断该点是局部极大值、局部极小值还是鞍点。</li>
<li>如果Hessian矩阵在临界点是正定的（所有特征值为正），则该点是局部极小值。</li>
<li>如果Hessian矩阵在临界点是负定的（所有特征值为负），则该点是局部极大值。</li>
<li>如果Hessian矩阵在临界点有正有负的特征值，则该点是鞍点。</li>
</ul>
</li>
<li><p><strong>优化中的应用</strong>：Hessian矩阵在优化算法中非常重要，尤其是在牛顿法中，它用于确定搜索方向和步长。</p>
</li>
</ol>
<h4 id="例子-3"><a href="#例子-3" class="headerlink" title="例子"></a>例子</h4><p>考虑一个二元函数 ( f(x, y) &#x3D; x^2 + y^2 )，其Hessian矩阵为：</p>
<p>$$<br>\mathbf{H} &#x3D; \begin{pmatrix}<br>\frac{\partial^2 f}{\partial x^2} &amp; \frac{\partial^2 f}{\partial x \partial y} \<br>\frac{\partial^2 f}{\partial y \partial x} &amp; \frac{\partial^2 f}{\partial y^2}<br>\end{pmatrix} &#x3D; \begin{pmatrix}<br>2 &amp; 0 \<br>0 &amp; 2<br>\end{pmatrix}<br>$$<br>这个矩阵是对称的，并且所有特征值为正，表明在原点 ( (0, 0) ) 处，函数 ( f(x, y) ) 有局部极小值。</p>
<p>Hessian矩阵在数学、物理、工程和经济学等多个领域都有广泛的应用。</p>
<h3 id="Inverse-Hessian-Matrix"><a href="#Inverse-Hessian-Matrix" class="headerlink" title="Inverse Hessian Matrix"></a>Inverse Hessian Matrix</h3><p>是海森矩阵（Hessian Matrix）的逆矩阵。对于一个多元函数 ( f(x_1, x_2, \dots, x_n) )，其海森矩阵 ( \mathbf{H} ) 的逆矩阵 ( \mathbf{H}^{-1} ) 具有一些重要的性质和应用。以下是逆海森矩阵的一些主要性质：</p>
<h4 id="1-存在性"><a href="#1-存在性" class="headerlink" title="1. 存在性"></a>1. <strong>存在性</strong></h4><p>逆海森矩阵 ( \mathbf{H}^{-1} ) 存在的条件是海森矩阵 ( \mathbf{H} ) 是非奇异的，即 ( \det(\mathbf{H}) \neq 0 )。这意味着海森矩阵必须是一个满秩矩阵。</p>
<h4 id="2-对称性"><a href="#2-对称性" class="headerlink" title="2. 对称性"></a>2. <strong>对称性</strong></h4><p>如果海森矩阵 ( \mathbf{H} ) 是对称的，那么其逆矩阵 ( \mathbf{H}^{-1} ) 也是对称的。这是因为对称矩阵的逆矩阵仍然是对称的。</p>
<h4 id="3-正定性"><a href="#3-正定性" class="headerlink" title="3. 正定性"></a>3. <strong>正定性</strong></h4><ul>
<li><strong>正定性传递</strong>：如果海森矩阵 ( \mathbf{H} ) 是正定的（所有特征值为正），那么其逆矩阵 ( \mathbf{H}^{-1} ) 也是正定的。</li>
<li><strong>负定性传递</strong>：如果海森矩阵 ( \mathbf{H} ) 是负定的（所有特征值为负），那么其逆矩阵 ( \mathbf{H}^{-1} ) 也是负定的。</li>
</ul>
<h4 id="4-优化中的应用"><a href="#4-优化中的应用" class="headerlink" title="4. 优化中的应用"></a>4. <strong>优化中的应用</strong></h4><p>逆海森矩阵在优化算法中非常重要，尤其是在牛顿法（Newton’s Method）中。牛顿法使用逆海森矩阵来确定搜索方向和步长。具体来说，牛顿法的更新公式为：</p>
<p>$$<br>\mathbf{x}_{k+1} &#x3D; \mathbf{x}_k - \mathbf{H}^{-1}(\mathbf{x}_k) \nabla f(\mathbf{x}_k)<br>$$</p>
<p>其中，( \mathbf{x}_k ) 是当前的迭代点，( \nabla f(\mathbf{x}_k) ) 是函数 ( f ) 在 ( \mathbf{x}_k ) 处的梯度。</p>
<h4 id="5-近似逆海森矩阵"><a href="#5-近似逆海森矩阵" class="headerlink" title="5. 近似逆海森矩阵"></a>5. <strong>近似逆海森矩阵</strong></h4><p>在实际应用中，计算精确的逆海森矩阵可能非常复杂和耗时。因此，通常使用近似逆海森矩阵的方法，如拟牛顿法（Quasi-Newton Methods）中的BFGS（Broyden-Fletcher-Goldfarb-Shanno）算法和L-BFGS（Limited-memory BFGS）算法。这些方法通过迭代更新近似逆海森矩阵，从而避免了直接计算逆矩阵。</p>
<h4 id="6-特征值和特征向量"><a href="#6-特征值和特征向量" class="headerlink" title="6. 特征值和特征向量"></a>6. <strong>特征值和特征向量</strong></h4><p>逆海森矩阵的特征值是海森矩阵特征值的倒数。具体来说，如果 ( \lambda ) 是海森矩阵 ( \mathbf{H} ) 的特征值，对应的特征向量为 ( \mathbf{v} )，那么 ( \frac{1}{\lambda} ) 是逆海森矩阵 ( \mathbf{H}^{-1} ) 的特征值，对应的特征向量仍然是 ( \mathbf{v} )。</p>
<h4 id="例子-4"><a href="#例子-4" class="headerlink" title="例子"></a>例子</h4><p>考虑一个二元函数 ( f(x, y) &#x3D; x^2 + y^2 )，其海森矩阵为：</p>
<p>$$<br>\mathbf{H} &#x3D; \begin{pmatrix}<br>2 &amp; 0 \<br>0 &amp; 2<br>\end{pmatrix}<br>$$<br>其逆海森矩阵为：</p>
<p>$$<br>\mathbf{H}^{-1} &#x3D; \begin{pmatrix}<br>\frac{1}{2} &amp; 0 \<br>0 &amp; \frac{1}{2}<br>\end{pmatrix}<br>$$</p>
<p>这个逆矩阵是对称的，并且所有特征值为正，表明在原点 ( (0, 0) ) 处，函数 ( f(x, y) ) 有局部极小值。</p>
<p>总之，逆海森矩阵在数学、物理、工程和经济学等多个领域都有广泛的应用，尤其是在优化问题中扮演着重要角色。</p>
<h3 id="凸函数（Convex-Function）"><a href="#凸函数（Convex-Function）" class="headerlink" title="凸函数（Convex Function）"></a>凸函数（Convex Function）</h3><p>说白了就是像 x^2 那样的函数，二阶导为正，斜率是上升的。</p>
<h3 id="泊松分布（Poisson-distribution）"><a href="#泊松分布（Poisson-distribution）" class="headerlink" title="泊松分布（Poisson distribution）"></a>泊松分布（Poisson distribution）</h3><p>是一种统计与概率学里常见到的离散概率分布。泊松分布适用于描述单位时间内随机事件发生的次数的概率分布。</p>
<h4 id="泊松分布的特点："><a href="#泊松分布的特点：" class="headerlink" title="泊松分布的特点："></a>泊松分布的特点：</h4><ol>
<li><strong>离散性</strong>：泊松分布描述的是离散事件的发生次数，如某段时间内电话呼叫的次数、某段时间内到达商店的顾客数等。</li>
<li><strong>独立性</strong>：事件的发生是独立的，即一个事件的发生不影响另一个事件的发生。</li>
<li><strong>恒定性</strong>：在相同的时间间隔内，事件发生的概率是恒定的。</li>
<li><strong>小概率性</strong>：事件发生的概率很小，但事件发生的次数可以很大。</li>
</ol>
<h4 id="泊松分布的概率质量函数："><a href="#泊松分布的概率质量函数：" class="headerlink" title="泊松分布的概率质量函数："></a>泊松分布的概率质量函数：</h4><p>泊松分布的概率质量函数（PMF）为：</p>
<p>$$<br>P(X &#x3D; k) &#x3D; \frac{\lambda^k e^{-\lambda}}{k!}<br>$$<br>其中：</p>
<ul>
<li>( X ) 是随机变量，表示事件发生的次数。</li>
<li>( k ) 是事件发生的次数，( k &#x3D; 0, 1, 2, \ldots )。</li>
<li>( \lambda ) 是单位时间（或单位面积、单位体积等）内事件发生的平均次数，是一个正实数。</li>
<li>( e ) 是自然对数的底，约等于2.71828。</li>
<li>( k! ) 是 ( k ) 的阶乘。</li>
</ul>
<h4 id="泊松分布的期望值和方差："><a href="#泊松分布的期望值和方差：" class="headerlink" title="泊松分布的期望值和方差："></a>泊松分布的期望值和方差：</h4><ul>
<li><strong>期望值</strong>（均值）：( E(X) &#x3D; \lambda )</li>
<li><strong>方差</strong>：( Var(X) &#x3D; \lambda )</li>
</ul>
<h4 id="泊松分布的应用："><a href="#泊松分布的应用：" class="headerlink" title="泊松分布的应用："></a>泊松分布的应用：</h4><p>泊松分布在许多领域中都有广泛的应用，例如：</p>
<ul>
<li><strong>排队论</strong>：描述顾客到达的次数。</li>
<li><strong>保险业</strong>：描述理赔发生的次数。</li>
<li><strong>电信业</strong>：描述电话呼叫的次数。</li>
<li><strong>生物学</strong>：描述基因突变的次数。</li>
<li><strong>物理学</strong>：描述放射性衰变的次数。</li>
</ul>
<h4 id="泊松分布与其他分布的关系："><a href="#泊松分布与其他分布的关系：" class="headerlink" title="泊松分布与其他分布的关系："></a>泊松分布与其他分布的关系：</h4><ul>
<li><strong>二项分布的极限</strong>：当二项分布的试验次数 ( n ) 很大，而每次试验成功的概率 ( p ) 很小时，二项分布可以近似为泊松分布，其中 ( \lambda &#x3D; np )。</li>
<li><strong>指数分布</strong>：如果事件发生的时间间隔服从指数分布，那么事件发生的次数在单位时间内服从泊松分布。</li>
</ul>
<p>泊松分布在实际应用中非常有用，尤其是在需要描述稀有事件发生次数的场景中。</p>
<h3 id="交叉熵损失（Cross-Entropy-Loss）"><a href="#交叉熵损失（Cross-Entropy-Loss）" class="headerlink" title="交叉熵损失（Cross-Entropy Loss）"></a>交叉熵损失（Cross-Entropy Loss）</h3><p>交叉熵损失（Cross-Entropy Loss），也称为对数损失（Log Loss），是机器学习和深度学习中常用的一种损失函数，特别是在分类问题中。<strong>它用于衡量模型预测的概率分布与真实标签的概率分布之间的差异</strong>。</p>
<h4 id="定义-3"><a href="#定义-3" class="headerlink" title="定义"></a>定义</h4><p>假设我们有一个分类问题，其中真实标签为 ( y )，模型预测的概率分布为 ( \hat{y} )。交叉熵损失 ( L ) 可以定义为：</p>
<p>$$<br>L &#x3D; -\sum_{i&#x3D;1}^{C} y_i \log(\hat{y}_i)<br>$$<br>其中：</p>
<ul>
<li>( C ) 是类别的数量。</li>
<li>( y_i ) 是真实标签的第 ( i ) 个类别的概率（通常是独热编码，即只有一个类别的概率为1，其余为0）。</li>
<li>( \hat{y}_i ) 是模型预测的第 ( i ) 个类别的概率。</li>
</ul>
<h4 id="二分类问题"><a href="#二分类问题" class="headerlink" title="二分类问题"></a>二分类问题</h4><p>在二分类问题中，交叉熵损失可以简化为：</p>
<p>$$<br>L &#x3D; -[y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]<br>$$<br>其中：</p>
<ul>
<li>( y ) 是真实标签（0或1）。</li>
<li>( \hat{y} ) 是模型预测的概率（属于类别1的概率）。</li>
</ul>
<h4 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h4><p>在多分类问题中，交叉熵损失通常与softmax函数结合使用。假设有 ( C ) 个类别，模型输出的概率分布为<br>$$<br>\hat{y} &#x3D; [\hat{y}_1, \hat{y}_2, \dots, \hat{y}<em>C]<br>$$<br>真实标签为独热编码 ( y &#x3D; [y_1, y_2, \dots, y_C] )，则交叉熵损失为：<br>$$<br>L &#x3D; -\sum</em>{i&#x3D;1}^{C} y_i \log(\hat{y}_i)<br>$$</p>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ol>
<li><strong>非负性</strong>：交叉熵损失总是非负的，当且仅当模型预测的概率分布与真实标签完全一致时，损失为0。</li>
<li><strong>敏感性</strong>：交叉熵损失对预测错误的惩罚较大，尤其是当预测概率接近0或1时。</li>
<li><strong>平滑性</strong>：交叉熵损失是连续可微的，这使得它适合用于梯度下降等优化算法。</li>
</ol>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><p>交叉熵损失广泛应用于各种分类任务中，包括图像分类、文本分类、语音识别等。它通常与softmax激活函数结合使用，用于多分类问题，或者与sigmoid激活函数结合使用，用于二分类问题。</p>
<h4 id="总结-7"><a href="#总结-7" class="headerlink" title="总结"></a>总结</h4><p>交叉熵损失是一种衡量模型预测与真实标签之间差异的有效方法，特别适用于分类问题。它通过计算预测概率分布与真实标签分布之间的差异，帮助模型在训练过程中逐步调整参数，以提高预测的准确性。</p>
<h1 id="Qusention-Answer-Generation"><a href="#Qusention-Answer-Generation" class="headerlink" title="Qusention-Answer Generation"></a>Qusention-Answer Generation</h1><ol>
<li><a target="_blank" rel="noopener" href="https://aclanthology.org/2020.acl-main.20.pdf">https://aclanthology.org/2020.acl-main.20.pdf</a><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/seanie12/Info-HCVAE">https://github.com/seanie12/Info-HCVAE</a></li>
</ol>
</li>
<li><a target="_blank" rel="noopener" href="https://paperswithcode.com/task/question-answer-generation">https://paperswithcode.com/task/question-answer-generation</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.05179">https://arxiv.org/pdf/2109.05179</a></li>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S2666920X24000559?via=ihub">https://www.sciencedirect.com/science/article/pii/S2666920X24000559?via%3Dihub</a></li>
</ol>
<h1 id="模型量化"><a href="#模型量化" class="headerlink" title="模型量化"></a>模型量化</h1><h2 id="一些文章"><a href="#一些文章" class="headerlink" title="一些文章"></a>一些文章</h2><ul>
<li><a target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization">A Visual Guide to Quantization - by Maarten Grootendorst</a></li>
<li></li>
</ul>
<h2 id="GPTQ"><a href="#GPTQ" class="headerlink" title="GPTQ"></a>GPTQ</h2><h2 id="AQW"><a href="#AQW" class="headerlink" title="AQW"></a>AQW</h2><p>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.00978">2306.00978] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration (arxiv.org)</a></p>
<h2 id="Quantization-aware-training-QAT"><a href="#Quantization-aware-training-QAT" class="headerlink" title="Quantization aware training (QAT)"></a>Quantization aware training (QAT)</h2><p>Quantization aware training (QAT)是一种在深度学习模型训练过程中引入量化效应的技术。量化（Quantization）是指将模型中的浮点数参数（如权重和激活值）转换为低精度的整数表示，以减少模型的存储空间和计算复杂度。然而，直接对已经训练好的模型进行量化可能会导致显著的精度损失，因为量化过程引入了非线性误差。</p>
<p>Quantization aware training 通过在训练过程中模拟量化效应，使得模型能够在训练时就适应量化带来的误差，从而在量化后保持较高的精度。具体来说，QAT 在训练过程中对模型的前向传播和反向传播进行如下处理：</p>
<ol>
<li><p><strong>模拟量化</strong>：在前向传播过程中，模型的权重和激活值在计算之前会被模拟量化为低精度整数，然后在计算之后再反量化回浮点数。这样，模型在训练时就能感知到量化带来的误差。</p>
</li>
<li><p><strong>梯度修正</strong>：在反向传播过程中，由于量化操作是不可微的，直接计算梯度会导致梯度不准确。因此，QAT 使用一种称为“直通估计器”（Straight-Through Estimator, STE）的技术，将量化操作的梯度近似为恒等映射，从而使得梯度能够正确传播。</p>
</li>
</ol>
<p>通过这种方式，Quantization aware training 能够在训练过程中优化模型，使其在量化后仍然保持较高的精度。QAT 通常用于需要在边缘设备上部署的深度学习模型，因为这些设备通常对计算资源和存储空间有严格的限制。</p>
<h2 id="Post-training-quantization（PTQ）"><a href="#Post-training-quantization（PTQ）" class="headerlink" title="Post training quantization（PTQ）"></a>Post training quantization（PTQ）</h2><p>Post training quantization（PTQ，训练后量化）是一种在不重新训练模型的情况下，将已经训练好的深度学习模型转换为低精度表示的技术。这种技术旨在减少模型的存储空间和计算复杂度，同时尽量保持模型的预测精度。</p>
<h3 id="主要步骤-1"><a href="#主要步骤-1" class="headerlink" title="主要步骤"></a>主要步骤</h3><ol>
<li><p><strong>权重量化</strong>：</p>
<ul>
<li>将模型中的浮点数权重转换为低精度的整数表示。例如，将32位浮点数权重转换为8位整数。</li>
<li>量化过程通常涉及选择一个量化范围（如最小值和最大值），然后将权重线性映射到这个范围内的整数表示。</li>
</ul>
</li>
<li><p><strong>激活值量化</strong>：</p>
<ul>
<li>对模型的激活值（即每一层的输出）进行量化。与权重量化类似，激活值也被转换为低精度的整数表示。</li>
<li>激活值的量化通常在模型推理时进行，使用训练数据的一个子集（称为校准集）来确定量化范围。</li>
</ul>
</li>
<li><p><strong>量化感知推理</strong>：</p>
<ul>
<li>在模型推理过程中，权重和激活值在计算之前被量化，计算之后再反量化回浮点数。这样可以模拟量化带来的误差，但不需要重新训练模型。</li>
</ul>
</li>
</ol>
<h3 id="优点-4"><a href="#优点-4" class="headerlink" title="优点"></a>优点</h3><ul>
<li><strong>简单快速</strong>：不需要重新训练模型，因此实现起来相对简单和快速。</li>
<li><strong>减少存储和计算资源</strong>：通过将模型参数和激活值转换为低精度表示，可以显著减少模型的存储空间和计算复杂度。</li>
</ul>
<h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li><strong>精度损失</strong>：由于没有在训练过程中引入量化效应，直接对已经训练好的模型进行量化可能会导致显著的精度损失。</li>
<li><strong>需要校准数据</strong>：为了确定激活值的量化范围，通常需要使用一小部分训练数据进行校准，这可能会引入额外的数据依赖性。</li>
</ul>
<h3 id="适用场景-1"><a href="#适用场景-1" class="headerlink" title="适用场景"></a>适用场景</h3><p>Post training quantization 适用于那些对模型精度要求不是特别高，或者没有足够计算资源进行重新训练的场景。例如，在移动设备、嵌入式系统或边缘计算设备上部署深度学习模型时，PTQ 是一种常用的优化技术。</p>
<h3 id="总结-8"><a href="#总结-8" class="headerlink" title="总结"></a>总结</h3><p>Post training quantization 是一种在不重新训练模型的情况下，通过将模型参数和激活值转换为低精度表示来减少存储空间和计算复杂度的技术。虽然它可能会导致一定的精度损失，但在许多实际应用中，这种损失是可以接受的。</p>
<h2 id="W8A8"><a href="#W8A8" class="headerlink" title="W8A8"></a>W8A8</h2><p>W8A8 quantization 是一种特定的量化方案，其中 “W” 代表权重（Weights），”A” 代表激活值（Activations），”8” 表示量化后的位宽为8位。因此，W8A8 quantization 指的是将模型的权重和激活值都量化为8位整数表示。</p>
<h3 id="具体步骤-1"><a href="#具体步骤-1" class="headerlink" title="具体步骤"></a>具体步骤</h3><ol>
<li><p><strong>权重量化（W8）</strong>：</p>
<ul>
<li><p>将模型中的浮点数权重转换为8位整数。通常使用线性量化方法，将权重映射到一个固定的范围（如[-128, 127]）。</p>
</li>
<li><p>量化公式：<br>$$<br> \text{quantized_weight} &#x3D; \text{round}(\frac{\text{weight}}{\text{scale_factor}})<br>$$</p>
</li>
<li><p>反量化公式：<br>$$<br>\text{dequantized_weight} &#x3D; \text{quantized_weight} \times \text{scale_factor}<br>$$</p>
</li>
</ul>
</li>
<li><p><strong>激活值量化（A8）</strong>：</p>
<ul>
<li><p>将模型的激活值（即每一层的输出）转换为8位整数。同样使用线性量化方法，将激活值映射到一个固定的范围（如[0, 255]）。</p>
</li>
<li><p>量化公式：<br>$$<br> \text{quantized_activation} &#x3D; \text{round}(\frac{\text{activation}}{\text{scale_factor}})<br>$$</p>
</li>
<li><p>反量化公式：<br>$$<br> \text{dequantized_activation} &#x3D; \text{quantized_activation} \times \text{scale_factor}<br>$$</p>
</li>
</ul>
</li>
</ol>
<h3 id="优点-5"><a href="#优点-5" class="headerlink" title="优点"></a>优点</h3><ul>
<li><strong>减少存储空间</strong>：将32位浮点数转换为8位整数，可以显著减少模型的存储空间。</li>
<li><strong>加速计算</strong>：8位整数的计算通常比32位浮点数计算更快，尤其是在支持8位计算的硬件上（如某些CPU、GPU和专用的AI加速器）。</li>
<li><strong>低功耗</strong>：低精度计算通常消耗更少的能量，这对于移动设备和嵌入式系统尤为重要。</li>
</ul>
<h3 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li><strong>精度损失</strong>：由于量化引入了非线性误差，可能会导致模型的预测精度下降。</li>
<li><strong>需要校准数据</strong>：为了确定激活值的量化范围，通常需要使用一小部分训练数据进行校准。</li>
</ul>
<h3 id="适用场景-2"><a href="#适用场景-2" class="headerlink" title="适用场景"></a>适用场景</h3><p>W8A8 quantization 适用于那些对模型精度要求不是特别高，但对存储空间和计算速度有严格要求的场景。例如：</p>
<ul>
<li><strong>移动设备</strong>：如智能手机、平板电脑等，这些设备通常对存储空间和计算资源有严格的限制。</li>
<li><strong>嵌入式系统</strong>：如智能家居设备、工业控制系统等，这些系统通常需要低功耗和实时性。</li>
<li><strong>边缘计算</strong>：在边缘设备上部署深度学习模型，以减少数据传输和延迟。</li>
</ul>
<h3 id="总结-9"><a href="#总结-9" class="headerlink" title="总结"></a>总结</h3><p>W8A8 quantization 是一种将模型的权重和激活值都量化为8位整数的技术，旨在减少模型的存储空间和计算复杂度。虽然它可能会导致一定的精度损失，但在许多实际应用中，这种损失是可以接受的，尤其是在对存储空间和计算速度有严格要求的场景中。</p>
<h2 id="Group-wise-quantization"><a href="#Group-wise-quantization" class="headerlink" title="Group-wise quantization"></a>Group-wise quantization</h2><p>组量化是一种量化技术，它将模型的权重分成若干个组（groups），并对每个组分别进行量化。这种技术可以更好地适应权重的分布特性，从而在量化过程中减少精度损失。</p>
<h3 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h3><p>在传统的量化方法中，模型的所有权重通常被视为一个整体，使用相同的量化参数（如量化范围和缩放因子）进行量化。然而，权重在不同的层或不同的通道之间可能具有不同的分布特性。如果使用相同的量化参数对所有权重进行量化，可能会导致某些权重被过度量化，从而引入较大的误差。</p>
<p>Group-wise quantization 通过将权重分成若干个组，对每个组分别进行量化，从而更好地适应权重的分布特性。具体来说，每个组的权重使用独立的量化参数进行量化，这样可以减少量化误差，提高量化后的模型精度。</p>
<h3 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h3><ol>
<li><p><strong>分组</strong>：</p>
<ul>
<li>将模型的权重分成若干个组。分组的方式可以有多种，例如：<ul>
<li><strong>按通道分组</strong>：将每个卷积核的权重按通道分成若干个组。</li>
<li><strong>按层分组</strong>：将每个层的权重分成若干个组。</li>
<li><strong>按块分组</strong>：将权重分成若干个固定大小的块。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>量化</strong>：</p>
<ul>
<li>对每个组的权重分别进行量化。每个组使用独立的量化参数（如量化范围和缩放因子）进行量化。</li>
</ul>
</li>
<li><p><strong>反量化</strong>：</p>
<ul>
<li>在推理过程中，对每个组的权重进行反量化，恢复为浮点数表示，然后进行计算。</li>
</ul>
</li>
</ol>
<h3 id="优点-6"><a href="#优点-6" class="headerlink" title="优点"></a>优点</h3><ul>
<li><strong>减少精度损失</strong>：通过将权重分成若干个组，并对每个组分别进行量化，可以更好地适应权重的分布特性，减少量化误差，从而提高量化后的模型精度。</li>
<li><strong>灵活性</strong>：Group-wise quantization 提供了更多的灵活性，可以根据权重的分布特性选择合适的分组方式和量化参数。</li>
</ul>
<h3 id="缺点-3"><a href="#缺点-3" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li><strong>复杂度增加</strong>：Group-wise quantization 增加了量化的复杂度，因为需要对每个组分别进行量化和反量化。</li>
<li><strong>存储需求增加</strong>：每个组需要独立的量化参数，因此存储需求会增加。</li>
</ul>
<h3 id="适用场景-3"><a href="#适用场景-3" class="headerlink" title="适用场景"></a>适用场景</h3><p>Group-wise quantization 适用于那些权重分布不均匀的模型，或者对模型精度要求较高的场景。例如：</p>
<ul>
<li><strong>卷积神经网络（CNN）</strong>：在卷积神经网络中，不同通道的权重可能具有不同的分布特性，使用 Group-wise quantization 可以更好地适应这些特性。</li>
<li><strong>自然语言处理（NLP）模型</strong>：在自然语言处理模型中，不同层的权重可能具有不同的分布特性，使用 Group-wise quantization 可以减少量化误差。</li>
</ul>
<h3 id="总结-10"><a href="#总结-10" class="headerlink" title="总结"></a>总结</h3><p>Group-wise quantization 是一种将模型的权重分成若干个组，并对每个组分别进行量化的技术。它可以更好地适应权重的分布特性，减少量化误差，提高量化后的模型精度。虽然增加了量化的复杂度和存储需求，但在某些场景下，Group-wise quantization 是一种有效的优化技术。</p>
<h2 id="Per-channel-scaling"><a href="#Per-channel-scaling" class="headerlink" title="Per-channel scaling"></a>Per-channel scaling</h2><p>逐通道缩放是一种量化技术，它对神经网络中的每个通道（channel）分别应用独立的缩放因子（scale factor）。这种技术可以更好地适应不同通道的权重分布特性，从而在量化过程中减少精度损失。</p>
<h3 id="主要思想-1"><a href="#主要思想-1" class="headerlink" title="主要思想"></a>主要思想</h3><p>在传统的量化方法中，模型的所有权重通常使用相同的缩放因子进行量化。然而，不同通道的权重可能具有不同的分布特性，如果使用相同的缩放因子对所有通道进行量化，可能会导致某些通道的权重被过度量化，从而引入较大的误差。</p>
<p>Per-channel scaling 通过为每个通道分别应用独立的缩放因子，从而更好地适应不同通道的权重分布特性。具体来说，每个通道的权重使用独立的缩放因子进行量化，这样可以减少量化误差，提高量化后的模型精度。</p>
<h3 id="实现步骤-1"><a href="#实现步骤-1" class="headerlink" title="实现步骤"></a>实现步骤</h3><ol>
<li><p><strong>分通道</strong>：</p>
<ul>
<li>将模型的权重按通道分开。例如，在卷积神经网络（CNN）中，每个卷积核的权重可以按通道分成若干个组。</li>
</ul>
</li>
<li><p><strong>计算缩放因子</strong>：</p>
<ul>
<li>对每个通道分别计算缩放因子。缩放因子通常基于该通道权重的最大值和最小值来确定。</li>
</ul>
</li>
<li><p><strong>量化</strong>：</p>
<ul>
<li>对每个通道的权重分别进行量化。每个通道使用独立的缩放因子进行量化。</li>
</ul>
</li>
<li><p><strong>反量化</strong>：</p>
<ul>
<li>在推理过程中，对每个通道的权重进行反量化，恢复为浮点数表示，然后进行计算。</li>
</ul>
</li>
</ol>
<h3 id="优点-7"><a href="#优点-7" class="headerlink" title="优点"></a>优点</h3><ul>
<li><strong>减少精度损失</strong>：通过为每个通道分别应用独立的缩放因子，可以更好地适应不同通道的权重分布特性，减少量化误差，从而提高量化后的模型精度。</li>
<li><strong>灵活性</strong>：Per-channel scaling 提供了更多的灵活性，可以根据不同通道的权重分布特性选择合适的缩放因子。</li>
</ul>
<h3 id="缺点-4"><a href="#缺点-4" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li><strong>复杂度增加</strong>：Per-channel scaling 增加了量化的复杂度，因为需要对每个通道分别进行量化和反量化。</li>
<li><strong>存储需求增加</strong>：每个通道需要独立的缩放因子，因此存储需求会增加。</li>
</ul>
<h3 id="适用场景-4"><a href="#适用场景-4" class="headerlink" title="适用场景"></a>适用场景</h3><p>Per-channel scaling 适用于那些不同通道的权重分布不均匀的模型，或者对模型精度要求较高的场景。例如：</p>
<ul>
<li><strong>卷积神经网络（CNN）</strong>：在卷积神经网络中，不同通道的权重可能具有不同的分布特性，使用 Per-channel scaling 可以更好地适应这些特性。</li>
<li><strong>自然语言处理（NLP）模型</strong>：在自然语言处理模型中，不同层的权重可能具有不同的分布特性，使用 Per-channel scaling 可以减少量化误差。</li>
</ul>
<h3 id="总结-11"><a href="#总结-11" class="headerlink" title="总结"></a>总结</h3><p>Per-channel scaling 是一种对神经网络中的每个通道分别应用独立缩放因子的量化技术。它可以更好地适应不同通道的权重分布特性，减少量化误差，提高量化后的模型精度。虽然增加了量化的复杂度和存储需求，但在某些场景下，Per-channel scaling 是一种有效的优化技术。</p>
<h1 id="推理框架"><a href="#推理框架" class="headerlink" title="推理框架"></a>推理框架</h1><h2 id="vLLM"><a href="#vLLM" class="headerlink" title="vLLM"></a>vLLM</h2><h2 id="TensorRT-LLM"><a href="#TensorRT-LLM" class="headerlink" title="TensorRT-LLM"></a>TensorRT-LLM</h2><h2 id="LMdeploy"><a href="#LMdeploy" class="headerlink" title="LMdeploy"></a>LMdeploy</h2><h2 id="Triton"><a href="#Triton" class="headerlink" title="Triton"></a>Triton</h2><p>Triton 的编程模型是为了简化 GPU 编程而设计的，它提供了一种高级语言的抽象，使得编写高效的 GPU 内核变得更加容易。下面我将详细介绍 Triton 的编程模型，包括其核心概念和执行模型。</p>
<h3 id="1-核心概念"><a href="#1-核心概念" class="headerlink" title="1. 核心概念"></a>1. 核心概念</h3><h4 id="1-1-程序块（Program-Blocks）"><a href="#1-1-程序块（Program-Blocks）" class="headerlink" title="1.1 程序块（Program Blocks）"></a>1.1 程序块（Program Blocks）</h4><p>在 Triton 中，一个内核可以被划分成多个程序块（Program Blocks），每个程序块在 GPU 上的一个线程块中执行。程序块是 Triton 内核执行的基本单位，每个程序块都有一个唯一的索引，可以通过 <code>tl.program_id</code> 函数获取。</p>
<h4 id="1-2-线程（Threads）"><a href="#1-2-线程（Threads）" class="headerlink" title="1.2 线程（Threads）"></a>1.2 线程（Threads）</h4><p> 每个程序块由多个线程组成，这些线程在 GPU 上并行执行。线程在程序块内的索引可以通过 <code>tl.lane_id</code> 函数获取。线程通常用于处理数据的分块计算。 </p>
<h4 id="1-3-共享内存（Shared-Memory）"><a href="#1-3-共享内存（Shared-Memory）" class="headerlink" title="1.3 共享内存（Shared Memory）"></a>1.3 共享内存（Shared Memory）</h4><p>Triton 内核可以使用共享内存来存储中间计算结果。共享内存的生命周期与程序块相同，所有线程都可以访问共享内存。共享内存可以通过 <code>tl.shared_memory</code> 函数分配。 </p>
<h4 id="1-4-全局内存（Global-Memory）"><a href="#1-4-全局内存（Global-Memory）" class="headerlink" title="1.4 全局内存（Global Memory）"></a>1.4 全局内存（Global Memory）</h4><p>全局内存是 GPU 上的主要存储区域，所有程序块和线程都可以访问。Triton 提供了 <code>tl.load</code> 和 <code>tl.store</code> 函数来从全局内存加载数据和将数据存储到全局内存。</p>
<h3 id="2-执行模型"><a href="#2-执行模型" class="headerlink" title="2. 执行模型"></a>2. 执行模型</h3><h4 id="2-1-内核执行"><a href="#2-1-内核执行" class="headerlink" title="2.1 内核执行"></a>2.1 内核执行</h4><p>Triton 内核是通过装饰器 <code>@triton.jit</code> 来定义的，内核在执行时会被编译成 GPU 可执行代码。内核的执行配置（如网格大小和块大小）可以通过内核调用的参数指定。</p>
<h4 id="2-2-网格（Grid）"><a href="#2-2-网格（Grid）" class="headerlink" title="2.2 网格（Grid）"></a>2.2 网格（Grid）</h4><p>网格（Grid）是程序块的集合，每个内核执行时都会指定一个网格。网格的大小可以通过内核调用的参数 <code>grid</code> 指定。网格的每个单元对应一个程序块。</p>
<h4 id="2-3-块大小（Block-Size）"><a href="#2-3-块大小（Block-Size）" class="headerlink" title="2.3 块大小（Block Size）"></a>2.3 块大小（Block Size）</h4><p>块大小（Block Size）是指每个程序块中的线程数。块大小通常是一个编译时常量，通过内核的参数 <code>BLOCK_SIZE</code> 指定。块大小对性能有重要影响，需要根据具体的硬件架构进行调优。 </p>
<h3 id="3-示例解析"><a href="#3-示例解析" class="headerlink" title="3. 示例解析"></a>3. 示例解析</h3><p>以下是一个完整的 Triton 内核示例，用于实现矩阵乘法。我们将逐步解析这个示例，以更好地理解 Triton 的编程模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> triton </span><br><span class="line"><span class="keyword">import</span> triton.language <span class="keyword">as</span> tl </span><br><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line"><span class="meta">@triton.jit </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matmul_kernel</span>(<span class="params">A_ptr,  <span class="comment"># 指向矩阵 A 的指针    </span></span></span><br><span class="line"><span class="params">                  B_ptr,  <span class="comment"># 指向矩阵 B 的指针    </span></span></span><br><span class="line"><span class="params">                  C_ptr,  <span class="comment"># 指向矩阵 C 的指针    </span></span></span><br><span class="line"><span class="params">                  n, m, k,  <span class="comment"># 矩阵的维度    </span></span></span><br><span class="line"><span class="params">                  stride_a_n, stride_a_k,  <span class="comment"># A 矩阵的步长    </span></span></span><br><span class="line"><span class="params">                  stride_b_k, stride_b_m,  <span class="comment"># B 矩阵的步长    </span></span></span><br><span class="line"><span class="params">                  stride_c_n, stride_c_m,  <span class="comment"># C 矩阵的步长    </span></span></span><br><span class="line"><span class="params">                  BLOCK_SIZE: tl.constexpr  <span class="comment"># 块大小，编译时常量</span></span></span><br><span class="line"><span class="params">                 </span>):   </span><br><span class="line">    <span class="comment"># 获取当前程序块的坐标   </span></span><br><span class="line">	pid_n = tl.program_id(<span class="number">0</span>)  <span class="comment"># 程序块在 n 维度上的索引    </span></span><br><span class="line">	pid_m = tl.program_id(<span class="number">1</span>)  <span class="comment"># 程序块在 m 维度上的索引</span></span><br><span class="line">    </span><br><span class="line">	<span class="comment"># 定义一个块内的共享内存    </span></span><br><span class="line">	acc = tl.zeros([BLOCK_SIZE], dtype=tl.float32)    </span><br><span class="line">	<span class="keyword">for</span> pid_k <span class="keyword">in</span> <span class="built_in">range</span>(k):        </span><br><span class="line">        <span class="comment"># 从全局内存加载数据到共享内存        </span></span><br><span class="line">		a = tl.load(A_ptr + pid_n * stride_a_n + pid_k * stride_a_k)        </span><br><span class="line">		b = tl.load(B_ptr + pid_k * stride_b_k + pid_m * stride_b_m)        </span><br><span class="line">		acc += a * b</span><br><span class="line">        </span><br><span class="line">	<span class="comment"># 将结果写回全局内存    </span></span><br><span class="line">	tl.store(C_ptr + pid_n * stride_c_n + pid_m * stride_c_m)</span><br></pre></td></tr></table></figure>


<h1 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h1><h2 id="LLM-测评集"><a href="#LLM-测评集" class="headerlink" title="LLM 测评集"></a>LLM 测评集</h2><h3 id="General"><a href="#General" class="headerlink" title="General"></a>General</h3><h4 id="MMLU"><a href="#MMLU" class="headerlink" title="MMLU"></a>MMLU</h4><p>MMLU（Massive Multitask Language Understanding） 是一种广泛用于评估人工智能模型（如大型语言模型）在多任务语言理解能力方面的基准测试。MMLU 由斯坦福大学和 Google 的研究人员开发，旨在衡量模型在多个学科和领域中的知识掌握程度和问题解决能力。</p>
<h5 id="MMLU-的主要特点："><a href="#MMLU-的主要特点：" class="headerlink" title="MMLU 的主要特点："></a>MMLU 的主要特点：</h5><ol>
<li><p><strong>多学科覆盖</strong>：MMLU 涵盖了57个不同的学科，包括数学、物理、化学、生物学、计算机科学、历史、法律、经济学、哲学等。这使得测试能够全面评估模型在不同领域的知识广度和深度。</p>
</li>
<li><p><strong>多任务测试</strong>：MMLU 不仅测试模型在单一任务上的表现，还要求模型在多个任务之间切换，模拟真实世界中需要跨学科知识解决问题的场景。</p>
</li>
<li><p><strong>高质量问题集</strong>：测试中的问题由领域专家编写，确保问题的高质量和专业性。问题类型包括选择题、填空题等，涵盖了不同难度级别。</p>
</li>
<li><p><strong>标准化评估</strong>：MMLU 提供了一个标准化的评估框架，使得不同模型之间的性能可以进行公平比较。评估结果通常以准确率（Accuracy）来衡量。</p>
</li>
</ol>
<p>MMLU 被广泛用于评估大型语言模型（如 GPT-4、PaLM 等）在多任务语言理解方面的能力。通过 MMLU 测试，研究人员可以了解模型在不同学科中的表现，识别其优势和不足。</p>
<h5 id="MMLU-测试中的题型："><a href="#MMLU-测试中的题型：" class="headerlink" title="MMLU 测试中的题型："></a>MMLU 测试中的题型：</h5><ol>
<li><p><strong>选择题（Multiple Choice Questions）</strong>：</p>
<ul>
<li>这是 MMLU 中最常见的题型。选择题通常包括一个问题和多个选项（通常是四个选项），模型需要从中选择一个正确的答案。</li>
<li>例子：如前文提到的“在计算机科学中，以下哪种数据结构最适合用于实现一个高效的优先队列？”</li>
</ul>
</li>
<li><p><strong>填空题（Fill-in-the-Blank Questions）</strong>：</p>
<ul>
<li>填空题要求模型填补句子中的空白部分，通常是关键术语或短语。</li>
<li>例子：“在物理学中，牛顿第二定律的公式是 F &#x3D; _____。”（答案：ma）</li>
</ul>
</li>
<li><p><strong>简答题（Short Answer Questions）</strong>：</p>
<ul>
<li>简答题要求模型提供简短的答案，通常是对问题的直接回答或解释。</li>
<li>例子：“请简述达尔文的进化论的主要观点。”（答案：物种通过自然选择和适者生存逐渐进化。）</li>
</ul>
</li>
<li><p><strong>判断题（True&#x2F;False Questions）</strong>：</p>
<ul>
<li>判断题要求模型判断给定的陈述是否正确。</li>
<li>例子：“地球是太阳系中最大的行星。”（答案：错误）</li>
</ul>
</li>
</ol>
<h4 id="MMLU-Pro"><a href="#MMLU-Pro" class="headerlink" title="MMLU-Pro"></a>MMLU-Pro</h4><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2406.01574">2406.01574 (arxiv.org)</a></p>
<p>MMLU-Pro is a refined version of the MMLU dataset, which has been a standard for multiple-choice knowledge assessment. Recent research identified issues with the original MMLU, such as noisy data (some unanswerable questions) and decreasing difficulty due to advances in model capabilities and increased data contamination. MMLU-Pro addresses these issues by presenting models with 10 choices instead of 4, requiring reasoning on more questions, and undergoing expert review to reduce noise. As a result, MMLU-Pro is of higher quality and currently more challenging than the original.</p>
<h4 id="IFEval"><a href="#IFEval" class="headerlink" title="IFEval"></a>IFEval</h4><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.07911">https://arxiv.org/abs/2311.07911</a></p>
<p>IFEval is a dataset designed to test a model’s ability to follow explicit instructions, such as “include keyword x” or “use format y.” The focus is on the model’s adherence to formatting instructions rather than the content generated, allowing for the use of strict and rigorous metrics.</p>
<h4 id="BBH"><a href="#BBH" class="headerlink" title="BBH"></a>BBH</h4><p>Big Bench Hard - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.09261">https://arxiv.org/abs/2210.09261</a></p>
<p>A subset of 23 challenging tasks from the BigBench dataset to evaluate language models. The tasks use objective metrics, are highly difficult, and have sufficient sample sizes for statistical significance. They include multistep arithmetic, algorithmic reasoning (e.g., boolean expressions, SVG shapes), language understanding (e.g., sarcasm detection, name disambiguation), and world knowledge. BBH performance correlates well with human preferences, providing valuable insights into model capabilities.</p>
<h4 id="MuSR"><a href="#MuSR" class="headerlink" title="MuSR"></a>MuSR</h4><p>Multistep Soft Reasoning - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.16049">https://arxiv.org/abs/2310.16049</a></p>
<p>MuSR is a new dataset consisting of algorithmically generated complex problems, each around 1,000 words in length. The problems include murder mysteries, object placement questions, and team allocation optimizations. Solving these problems requires models to integrate reasoning with long-range context parsing. Few models achieve better than random performance on this dataset.</p>
<p>MuSR（Multistep Soft Reasoning）是一种用于评估人工智能模型在多步骤软推理任务中的能力的基准测试。MuSR 旨在衡量模型在处理需要逐步推理和综合多个信息源的问题时的表现。</p>
<h5 id="MuSR-的主要特点："><a href="#MuSR-的主要特点：" class="headerlink" title="MuSR 的主要特点："></a>MuSR 的主要特点：</h5><ol>
<li><p><strong>多步骤推理</strong>：MuSR 测试中的任务通常需要模型进行多步骤的推理。这意味着模型不仅需要理解问题的初始条件，还需要逐步推导出中间结果，最终得出最终答案。</p>
</li>
<li><p><strong>软推理</strong>：与硬推理（Hard Reasoning）不同，软推理（Soft Reasoning）允许模型在推理过程中有一定的灵活性和不确定性。模型需要在不确定或不完全信息的情况下进行推理，并给出合理的答案。</p>
</li>
<li><p><strong>多样性任务</strong>：MuSR 涵盖了多种类型的任务，包括但不限于：</p>
<ul>
<li><strong>逻辑推理</strong>：要求模型进行复杂的逻辑推理，解决涉及多个步骤和条件的问题。</li>
<li><strong>常识推理</strong>：测试模型在日常生活中的常识理解和应用能力。</li>
<li><strong>多模态任务</strong>：结合文本、图像、音频等多种模态的信息进行综合分析和推理。</li>
<li><strong>长文本理解</strong>：要求模型理解并回答涉及长篇文本的问题。</li>
</ul>
</li>
<li><p><strong>高质量问题集</strong>：MuSR 中的问题由领域专家编写，确保问题的高质量和专业性。这些问题旨在挑战模型的极限，揭示其在多步骤软推理任务中的表现。</p>
</li>
<li><p><strong>标准化评估</strong>：MuSR 提供了一个标准化的评估框架，使得不同模型之间的性能可以进行公平比较。评估结果通常以准确率、F1 分数等指标来衡量。</p>
</li>
</ol>
<h5 id="MuSR-的应用："><a href="#MuSR-的应用：" class="headerlink" title="MuSR 的应用："></a>MuSR 的应用：</h5><ul>
<li><p><strong>模型评估</strong>：MuSR 被广泛用于评估大型语言模型在处理多步骤软推理任务时的能力。通过 MuSR 测试，研究人员可以了解模型在面对复杂推理任务时的表现，识别其优势和不足。</p>
</li>
<li><p><strong>模型改进</strong>：基于 MuSR 的评估结果，研究人员可以针对模型的薄弱环节进行改进，提升其在特定任务上的表现。</p>
</li>
<li><p><strong>学术研究</strong>：MuSR 为学术界提供了一个标准化的工具，用于研究不同模型在多步骤软推理方面的进展和趋势。</p>
</li>
</ul>
<h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><h4 id="HumanEval"><a href="#HumanEval" class="headerlink" title="HumanEval"></a>HumanEval</h4><h4 id="MBPP-EvalPlus"><a href="#MBPP-EvalPlus" class="headerlink" title="MBPP EvalPlus"></a>MBPP EvalPlus</h4><h3 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h3><h4 id="GSM8K"><a href="#GSM8K" class="headerlink" title="GSM8K"></a>GSM8K</h4><p>GSM8K（Grade School Math 8K）是一个由8.5K个高质量的数学问题组成的数据集，专门设计用于评估和提升语言模型在解决数学问题方面的能力。这些问题涵盖了从小学到中学的数学知识，包括算术、代数、几何和概率等多个领域。</p>
<p>GSM8K的主要特点包括：</p>
<ol>
<li><strong>多样性</strong>：问题类型多样，包括单步和多步问题，涉及不同的数学概念和应用场景。</li>
<li><strong>高质量</strong>：所有问题都经过人工审核，确保其准确性和教育价值。</li>
<li><strong>难度适中</strong>：问题难度适中，适合评估模型在基础数学问题上的表现。</li>
<li><strong>开放性</strong>：GSM8K是一个开源数据集，可以免费使用和修改。</li>
</ol>
<p>GSM8K通常用于以下几个方面：</p>
<ul>
<li><strong>模型评估</strong>：用于评估语言模型在解决数学问题上的准确性和效率。</li>
<li><strong>模型训练</strong>：作为训练数据的一部分，帮助模型提升数学推理能力。</li>
<li><strong>研究</strong>：用于研究语言模型在数学推理方面的进展和局限性。</li>
</ul>
<p>通过使用GSM8K，研究人员和开发者可以更好地理解和提升语言模型在数学问题解决方面的能力，从而推动人工智能在教育和其他领域的应用。</p>
<h4 id="MATH"><a href="#MATH" class="headerlink" title="MATH"></a>MATH</h4><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.03874">https://arxiv.org/abs/2103.03874</a></p>
<p>MATH is a compilation of high-school level competition problems gathered from several sources, formatted consistently using Latex for equations and Asymptote for figures. Generations must fit a very specific output format. We keep only level 5 MATH questions and call it MATH Lvl 5.</p>
<h3 id="Reasoning"><a href="#Reasoning" class="headerlink" title="Reasoning"></a>Reasoning</h3><h4 id="ARC-Challenge"><a href="#ARC-Challenge" class="headerlink" title="ARC Challenge"></a>ARC Challenge</h4><h4 id="GPQA"><a href="#GPQA" class="headerlink" title="GPQA"></a>GPQA</h4><p>Graduate-Level Google-Proof Q&amp;A Benchmark - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.12022">https://arxiv.org/abs/2311.12022</a></p>
<p>GPQA is a highly challenging knowledge dataset with questions crafted by PhD-level domain experts in fields like biology, physics, and chemistry. These questions are designed to be difficult for laypersons but relatively easy for experts. The dataset has undergone multiple rounds of validation to ensure both difficulty and factual accuracy. Access to GPQA is restricted through gating mechanisms to minimize the risk of data contamination. Consequently, we do not provide plain text examples from this dataset, as requested by the authors.</p>
<h3 id="Tool-use"><a href="#Tool-use" class="headerlink" title="Tool use"></a>Tool use</h3><h4 id="BFCL"><a href="#BFCL" class="headerlink" title="BFCL"></a>BFCL</h4><h4 id="Nexus"><a href="#Nexus" class="headerlink" title="Nexus"></a>Nexus</h4><h3 id="Long-context"><a href="#Long-context" class="headerlink" title="Long context"></a>Long context</h3><h4 id="ZeroSCROLLS-QuALITY"><a href="#ZeroSCROLLS-QuALITY" class="headerlink" title="ZeroSCROLLS&#x2F;QuALITY"></a>ZeroSCROLLS&#x2F;QuALITY</h4><h4 id="InfiniteBench-En-MC"><a href="#InfiniteBench-En-MC" class="headerlink" title="InfiniteBench&#x2F;En.MC"></a>InfiniteBench&#x2F;En.MC</h4><h4 id="NIH-Multi-needle"><a href="#NIH-Multi-needle" class="headerlink" title="NIH&#x2F;Multi-needle"></a>NIH&#x2F;Multi-needle</h4><h3 id="Multilingual"><a href="#Multilingual" class="headerlink" title="Multilingual"></a>Multilingual</h3><h4 id="MGSN"><a href="#MGSN" class="headerlink" title="MGSN"></a>MGSN</h4><h1 id="传统机器学习算法"><a href="#传统机器学习算法" class="headerlink" title="传统机器学习算法"></a>传统机器学习算法</h1><blockquote>
<p>一些面经有提到</p>
</blockquote>
<h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><h1 id="一些问题-2"><a href="#一些问题-2" class="headerlink" title="一些问题"></a>一些问题</h1><ol>
<li><p>现有的LLM共通的问题是什么</p>
<p>现有的语言模型（LLM，Large Language Models）虽然在自然语言处理（NLP）任务中表现出色，但它们仍然面临一些共通的问题和挑战。以下是一些主要问题：</p>
<ol>
<li><p><strong>计算资源需求高</strong></p>
<ul>
<li><p><strong>训练成本</strong>：训练大型语言模型需要大量的计算资源，包括高性能的GPU或TPU集群。这导致训练成本非常高，限制了中小型企业和研究机构的参与。</p>
</li>
<li><p><strong>推理成本</strong>：在实际应用中，推理（即使用模型进行预测）也需要大量的计算资源，尤其是在处理长文本或高并发请求时。</p>
</li>
</ul>
</li>
<li><p><strong>数据依赖性强</strong></p>
</li>
</ol>
</li>
</ol>
<ul>
<li><p><strong>数据质量</strong>：模型的性能高度依赖于训练数据的质量。如果数据中存在噪声、偏见或不完整性，模型可能会学习到错误的模式。</p>
</li>
<li><p><strong>数据隐私</strong>：大规模语言模型通常需要大量的公开数据进行训练，这可能涉及到数据隐私问题，尤其是在处理敏感信息时。</p>
<ol start="3">
<li><p><strong>模型偏见</strong></p>
<ul>
<li><p><strong>社会偏见</strong>：语言模型可能会从训练数据中学习到社会偏见，例如性别、种族或文化偏见。这可能导致模型在生成文本或进行决策时表现出不公平的行为。</p>
</li>
<li><p><strong>领域偏见</strong>：模型在特定领域的数据上训练后，可能会在其他领域表现不佳，导致泛化能力不足。</p>
</li>
</ul>
</li>
<li><p><strong>可解释性差</strong></p>
<ul>
<li><p><strong>黑箱模型</strong>：大型语言模型通常是黑箱模型，难以解释其内部决策过程。这使得在实际应用中难以理解和信任模型的输出。</p>
</li>
<li><p><strong>调试困难</strong>：由于模型的复杂性，调试和诊断模型中的错误或偏见非常困难。</p>
</li>
</ul>
</li>
<li><p><strong>生成内容的质量和一致性</strong></p>
<ul>
<li><p><strong>生成内容的质量</strong>：尽管大型语言模型在生成文本方面表现出色，但生成的内容可能存在逻辑错误、不连贯或不准确的问题。</p>
</li>
<li><p><strong>一致性问题</strong>：模型在生成长文本时，可能会出现前后不一致的问题，尤其是在处理复杂的推理任务时。</p>
</li>
</ul>
</li>
<li><p><strong>对上下文的理解有限</strong></p>
<ul>
<li><p><strong>短期记忆</strong>：大型语言模型通常对上下文的理解有限，尤其是在处理长文本时。模型可能会忘记早期的上下文信息，导致生成内容不连贯。</p>
</li>
<li><p><strong>缺乏常识推理</strong>：尽管模型在语言建模方面表现出色，但它们在常识推理和理解复杂语义关系方面仍然存在不足。</p>
</li>
</ul>
</li>
<li><p><strong>难以处理多模态数据</strong></p>
<ul>
<li><p><strong>单一模态</strong>：现有的语言模型主要处理文本数据，难以直接处理多模态数据（如图像、音频和视频）。虽然有一些多模态模型，但它们通常需要额外的训练和调整。</p>
</li>
<li><p><strong>跨模态理解</strong>：在处理多模态数据时，模型需要理解不同模态之间的关联和交互，这仍然是一个挑战。</p>
</li>
</ul>
</li>
<li><p><strong>缺乏长期目标和规划能力</strong></p>
<ul>
<li><strong>短期生成</strong>：大型语言模型通常只能生成短期目标的文本，缺乏长期规划和目标导向的能力。这限制了它们在需要复杂推理和规划的任务中的应用。</li>
</ul>
</li>
<li><p><strong>伦理和法律问题</strong></p>
<ul>
<li><p><strong>滥用风险</strong>：大型语言模型可能被滥用，用于生成虚假信息、恶意内容或进行网络攻击。</p>
</li>
<li><p><strong>法律合规</strong>：在某些国家和地区，使用和部署大型语言模型可能涉及到法律合规问题，尤其是在数据隐私和知识产权方面。</p>
</li>
</ul>
</li>
<li><p><strong>幻觉问题</strong></p>
<p>LLM有时会产生与现实不符的信息，这被称为“幻觉”（Hallucination）。这可以是事实性的错误，或者模型生成的内容与之前对话中的内容不一致。</p>
</li>
</ol>
</li>
</ul>
<p>2. </p>
<h2 id="为什么现在的LLM都是Decoder-only的架构？"><a href="#为什么现在的LLM都是Decoder-only的架构？" class="headerlink" title="为什么现在的LLM都是Decoder only的架构？"></a>为什么现在的LLM都是Decoder only的架构？</h2><ol>
<li>双向attention注意力矩阵容易退化为低秩状态，而causal attention的注意力矩阵是下三角，必然是满秩的，建模能力更强。（有点怀疑）</li>
<li>decoder only + next token prediction预训练，每个位置所能接触的信息比其他架构少，要预测下一个token难度更高，当模型足够大，数据足够多的时候，decoder-only模型学习通用表征的上限更高；</li>
<li>下文学习为decoder-only架构带来的更好的few-shot性能：prompt和demonstration的信息可以视为对模型参数的隐式微调[2]，decoder-only的架构相比encoder-decoder在in-contextlearning+上会更有优势，因为prompt可以更加直接地作用于decoder每一层的参数，微调的信号更强；</li>
<li>causal attention（就是decoder-only的单向attention）具有隐式的位置编码功能，打破了transformer的位置不变性，而带有双向attention的模型，如果不带位置编码，双向attention的部分token可以对换也不改变表示，对语序的区分能力天生较弱。<ul>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.16634">2203.16634] Transformer Language Models without Positional Encodings Still Learn Positional Information</a></li>
</ul>
</li>
<li>decoder-only支持一直复用KV-Cache+，对多轮对话更友好，因为每个token的表示只和它之前的输入有关，而encoder-decoder和PrefixLM就难以做到</li>
<li>轨迹依赖的问题：OpenAl作为开拓者勇于挖坑踩坑，以decoder-only架构为基础摸索出了一套行之有效的训练方法和ScalingLaw，后来者鉴于时间和计算成本，自然不愿意做太多结构上的大改动，继续沿用decoder-only架构。在工程生态上，decoder-only架构也形成了先发优势，Megatron和flash attention+等重要工具对causal attention的支持更好。</li>
</ol>
<h1 id="其他文章"><a href="#其他文章" class="headerlink" title="其他文章"></a>其他文章</h1><ol>
<li><h2 id="我没有大模型经验，可以给个机会吗？"><a href="#我没有大模型经验，可以给个机会吗？" class="headerlink" title="我没有大模型经验，可以给个机会吗？"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/715031517">我没有大模型经验，可以给个机会吗？</a></h2></li>
</ol>
<p>作者：Quokka<br>链接：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/715031517">https://zhuanlan.zhihu.com/p/715031517</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<p>在 DeepSeek 做大模型一年半，经历了无数场面试。</p>
<h3 id="经验"><a href="#经验" class="headerlink" title="经验"></a>经验</h3><p>我最常听到的候选人（尤其是学生）的说辞是：我没有大模型经验，可以给个机会吗？</p>
<p>答案是，我司并不看重候选人的大模型训练经验。这里不是说经验不重要，而是<strong>大部分人的经验没有意义，对我司来说不是加分项</strong>。如果真要看经验，我们看的是其他头部大模型公司的核心骨干，和绝大多数校招和社招人选无关（e.g.: 校招或者实习生常见的简历是微调 LLaMA 7B，社招常见的简历是非头部公司自己的 XX 大模型）。在腰部公司的招聘里，应届生有训练 7B LLaMA 的经验可能是一个加分项，毕竟不是每个实验室都有 8 卡 A100，大部分应届生是没有机会去摸大模型的；但是我们真的不在乎。</p>
<p>事实上，大部分人的大模型经验是扣分项。候选人说自己有大模型训练经验，我会问：你说你有千卡训练 XX B 模型的经验，用的是什么并行配置，DP&#x2F;PP&#x2F;TP 如何划分？很多时候，我得到的回答是：我不知道。甚至有时候，候选人会问我，什么是 DP，我实在是无言以对。做 CV 的候选人还能背两句 DP 和 DDP 区别的八股，做 NLP 的候选人，在最需要并行的研究领域，却完全不知道 DP 是什么。类似地，如果候选人做过大模型训练，却不知道什么是 MFU，不知道 Megatron 启动的命令行参数含义是什么[<a href="#ref_1">1]</a>……都属于负分经历。</p>
<h3 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h3><p>现在不比以前，很多人都有顶会论文。<strong>就像大家日常吐槽的一样，90% 的论文都是废纸</strong>。特别亮眼的文章自然是加分项，例如 PEFT（Parameter-Efficient Fine-Tuning）方向，最近的 LoRA-GA 和 LoRA-pro 都是不错的文章，但大部分文章都是改网络结构讲故事，这些是不加分的。如果你有论文，那么说明你经过了基本的科研训练，仅此而已。MSRA 之前招聘实习生有时甚至还会倾向于招聘没做过科研的白纸，因为怕之前短平快的科研经历把候选人的科研品味带偏了，掰不过来。</p>
<h3 id="除了经验和论文，还能看什么"><a href="#除了经验和论文，还能看什么" class="headerlink" title="除了经验和论文，还能看什么"></a>除了经验和论文，还能看什么</h3><p>用一个词来概括，是潜力。潜力这个词太虚，这里换成两个词来描述：基础、好奇心。</p>
<p><strong>什么是基础？</strong>对于学生来说，首要的自然是学习。学校背景如何、成绩如何、基础知识是否扎实？面试时遇到学生，经常碰到的尴尬场面是：问数学题（高数&#x2F;线代），答曰大一学的忘了；问编程题（leetcode easy&#x2F;medium 难度），答曰没刷题写不了；问模型结构（指 LLaMA，我司面试从不考 DeepSeekMoE），答曰平常都是调 ChatGPT API，不清楚。大部分候选人是答不上来 transformer 模型结构的——一半人承认自己不清楚细节，另一半人里 90% 是自以为自己知道、但实际不知道。</p>
<p>大部分科研人的代码能力孱弱到只会调 ChatGPT API，或者改改 torch.nn.Module，或者调用开源框架跑跑 SFT&#x2F;RLHF。分不清楚进程和线程，操作系统背完就忘；编程语言只会一些最基本的 Python，其他语言一概不通。是的，我知道这不影响你发论文，不影响你毕业，git clone 一下开源代码改两行就能满足你的需求嘛。但是，如果你想做改变世界的研究呢？例如，穿越回 2016 年，你想到了 AlphaGo 的 idea，给你足够的计算资源，你有能力动手实现它吗？</p>
<p><strong>什么是好奇心？</strong>没有大模型经验没关系，但是你愿意主动去了解吗？你会去主动读大模型的论文吗？读论文总不需要显卡吧？可惜很多候选人不去读。甚至别说读论文，有些想转行大模型的人连大模型用都不用一下。ChatGPT 能解决什么问题、不能解决什么问题？它的能力边界在哪里？一问一个不知道。有时候跟一些候选人保持联系了几个月，但是对方对大模型的了解在几个月的时间里没有任何长进，实在是令人惋惜。<strong>如果没机会训练 100B 以上的模型，甚至没有机会训练 7B 的模型，你愿意去下载和分析别人训好的 7B 乃至 1B 的模型，看看里面权重分布的规律吗</strong>？如果有这个细腻的心思，可能你在模型量化方面已经做出了很好的工作。</p>
<p>也有时候，基础和好奇心可以互补。例如模型训练刚开始时的 loss 大约是多少？如果数学基础扎实，那么可以做一些合理的假设推导出来；如果好奇心强，会注意观察每一个细节，也能答对这道题。</p>
<p>最后，再介绍一些比普普通通的大模型训练经验和论文更加分的经历的具体例子：</p>
<p>A. 在两张 2080Ti 上实现和比较过不同的流水算法的性能；</p>
<p>B. 用 Triton 自己实现过一些算子；</p>
<p>C. 能讲出不同的大模型使用的 tokenizer 的差异；</p>
<p>D. 在 Python 以外的语言上有不错的开发能力（例如某些开源项目背书）；</p>
<p>E. 实现过一个效果拔群的五子棋 AI（最好是 RL 算法）。</p>
<h1 id="其他资源"><a href="#其他资源" class="headerlink" title="其他资源"></a>其他资源</h1><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/Hannibal046/Awesome-LLM">Hannibal046&#x2F;Awesome-LLM: Awesome-LLM: a curated list of Large Language Model (github.com)</a></li>
<li></li>
</ol>
<h1 id="面经"><a href="#面经" class="headerlink" title="面经"></a>面经</h1><h2 id="腾讯NLP算法岗面经"><a href="#腾讯NLP算法岗面经" class="headerlink" title="腾讯NLP算法岗面经"></a>腾讯NLP算法岗面经</h2><p>首先来段简单的自我介绍：2021届硕士，硕士期间未有实习经历，本科大三有过一次实习，小公司，可以忽略。本人投递的是腾讯暑期实习：技术研究-<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86">自然语言处理</a>方向。</p>
<blockquote>
<p>腾讯校招开启的比较早，提前批在3.5号就已经开启了，我算是赶上了最早的那一批。本次的算法岗竞争同往年一样，也蛮激烈的，我亲眼看着，从投递最初NLP岗的投录比为3:1，面试时升到了4:1，最后完成流程后变为了5:1，NLP方向相对好些，CV、机器学习等竞争更大，CV 9:1，ML 10:1，听师兄描述，算法岗到后面会到20:1都可能，所以还是要趁早准备。</p>
</blockquote>
<p>本人的面试流程大概如下，3月1日找的内推，收到完善简历的链接，在官网投了简历，当时选择的意向BG是WXG，3月2日上午收到HR小姐姐电话，说她们是PCG部门，问我面不面，我大概询问了PCG的主要业务，得知是社交类的业务居多，然后就拒绝了，说还是优先考虑面WXG那边，于是HR小姐姐便说帮我转投。</p>
<p><strong>这点很诡异，官网的显示是，校招从3.5日开始提前批，但是实际上在这之前应该就开始筛简历并且面试了。</strong></p>
<blockquote>
<p>在腾讯，据说简历如果被捞了，如果同意面试简历就会被锁定，从简历池中提到面试的部门，这样其他的BG就看不到你的简历了，然后走官方流程，当时不知道，主要是考虑WXG当时更符合我的预期吧。</p>
</blockquote>
<p>3月4日，再次收到HR电话，官网当时状态是未发起面试，这次是直接约面试时间，问我啥时候有空，约了3月6日（在此之前大致准备了下算法题，但是没有仔细准备，所以隔了两天，再充分准备下）。3月6日下午，参加了电话面试，3.9，查公众号状态进入复试。期间等待了一段时间，3.15日收到二面电话，并且是当天面试，于是当天晚上8点面试，到9点左右结束。第二天，官网流程变为HR面，17日下午收到面试邀请链接，约20.40的视频面试，20日状态更新为已完成。</p>
<h3 id="一面（技术面）"><a href="#一面（技术面）" class="headerlink" title="一面（技术面）"></a>一面（技术面）</h3><p>一面个人感觉面的不是很好，虽然大部分问题答出来了，但是还是有不清晰的地方，而且有些问题没有答出来，甚至以为会挂，一面时间持续了大概有1小时2分钟。<strong>面试形式：电话面试</strong>。</p>
<ol>
<li>简要的自我介绍。</li>
</ol>
<p>自我介绍之前大致准备了下，就主要介绍个人情况，我主要是按照简历上的内容，大概介绍了下，时间在3分钟左右，但是个人感觉亮点不够突出，这点大家可以再发挥。</p>
<ol start="2">
<li>研究生阶段最有挑战的项目是什么？</li>
</ol>
<p>我觉得这是我的一面面试官比较关注的点，这点聊了很久。因为我们组的主要方向是智能化软件测试，或者说智能软件工程，然后介绍了最近研究的一个项目。大概介绍了这个项目的动机，然后传统的一些研究方法，以及我们准备采用的一些方法（NLP相关）。</p>
<ol start="3">
<li>对于这个项目，传统的方法是怎么样的？</li>
</ol>
<p>介绍了在软件测试领域大致的做法，主要是机器学习相关的内容了，即人工抽取的一些特征，已经使用的方法。</p>
<ol start="4">
<li>列举下这个任务在传统用法的一些典型特征。</li>
</ol>
<p>我大概列举了几个，感觉这里的案例说的不是很清楚，因为那些工作主要不是我在做，我只是对这些工作有过一些了解。</p>
<ol start="5">
<li>看你简历里有QA相关的论文，大概介绍下里面用的方法。</li>
</ol>
<p>主要介绍了这篇论文用的模型，以及方法，细节面试官没有深究。</p>
<ol start="6">
<li>你在这篇论文里用到的是GloVe，为何不用word2vec，或者说word2vec与GloVe有什么区别？</li>
</ol>
<p>我大致说了下他们的区别：word2vec是NNLM的一个较为典型的代表，其利用了DNN的方法来训练获取到词向量，而且词向量是中间产物，这个思路最早是Bengio提出，Google Brain提出的word2vec让词向量火了起来。而GloVe是采用<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1">词频统计</a>信息，利用一些数学方法逐步优化得来，它没有神经网络的结构，所以词向量训练的速度相对更快。（这里当时不记得具体的公式推导了，原论文我倒是看过，但是当时记得不清了，实际上GloVe是词共现矩阵+类SVD的方法）</p>
<ol start="7">
<li>你清楚word2vec吗，大致描述下word2vec的结构以及<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95">训练方法</a>。</li>
</ol>
<p>清楚（好刚的我，面完发现答的有瑕疵），从宏观上描述了DNN的一个结构，从输入（大致带过分词，词表构建，one-hot等过程），到隐层，到输出层。然后详细讲了两种训练结构，即CBoW和Skip-Gram，但是当时这两种方法被我说反了。（当时并无觉察）讲完两种训练方法后，大致介绍了下训练时候词表大小过大，输出层过大的优化方法，即：<strong>hierarchical softmax</strong>和<strong>negative sampling</strong>。</p>
<ol start="8">
<li>现阶段NLP的研究相对CV发展还是很缓慢，你认为是什么原因？</li>
</ol>
<p>自然语言存在变化性，和不确定性，即语义的抽取对神经网络来说是很难的，在英文，人脑可以通过词形来建立词与词之间的关系，但是语义不确定性很强，比如歧义，一词多义，词序等等都会影响语义。而CV的特征相对固定，如图像处理，filter提取的特征一般是某种轮廓或边缘特征，这些特征对于特定的物体都是固定的，所以效果会更好。（说了很多废话，不清楚，其实总结就是<strong>感知智能</strong>和<strong>认知智能</strong>，<strong>感知智能</strong>很容易实现，即CV，而<strong>认知智能</strong>有很多挑战，即NLP）</p>
<ol start="9">
<li>你知道<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B">隐马尔可夫模型</a>吗，大概介绍下。</li>
</ol>
<p>当时HMM的具体理论在准备阶段就大致地看了下，面试官很nice，没有很为难，理论的不记得那来实践的，就接着这个问题，问了HMM的几个要素，即：初始概率，<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E7%8A%B6%E6%80%81%E8%BD%AC%E7%A7%BB%E7%9F%A9%E9%98%B5">状态转移矩阵</a>，发射矩阵，这三个要素，然后我主要讲了下这三个要素的运算过程，提及了一下<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95">维特比算法</a>。（这里当时准备的不充分，说的不是特别清楚，后来我去恶补了一下）</p>
<ol start="10">
<li>维特比算法其实是一种动态<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95">规划算法</a>，动态规划算法通常用来解决什么问题，在HMM里是怎么使用的？</li>
</ol>
<p>大致描述了下动态规划的最优解问题，然后结合HMM的迭代过程说了一些。（后来仔细看了下，感觉面试官应该还是想听到HMM的理论，因为HMM推导会用到它里面的假设，然后得到递推关系，就可以分解为子问题，利用维特比算法求解）</p>
<ol start="11">
<li>快结束时，来道算法题吧，在一个敏感词过滤的场景下，要怎么在一个字符串里找出敏感词的位置？</li>
</ol>
<p>当时懵了，没想出来，只说了一个暴力解。（面试官应该是想听到KMP一类的算法）</p>
<ol start="12">
<li><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E7%AE%97%E6%B3%95%E5%A4%8D%E6%9D%82%E5%BA%A6">算法复杂度</a>是多少？</li>
</ol>
<p>大概算了下，面试官和蔼的说，没想到也没关系。他还曾循循善诱，从多个敏感词的词表简化到一个词，我对不起他。</p>
<ol start="13">
<li>你实习的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E6%97%B6%E9%97%B4%E5%AE%9E%E9%AA%8C%E5%AE%A4">时间实验室</a>有没有什么要求，能实习吗？</li>
</ol>
<p>没要求，当然能。（就这意思）</p>
<ol start="14">
<li>你还有什么问题要问我的吗？</li>
</ol>
<ul>
<li><ul>
<li><p>咱们部门对实习生的预期目标是啥？</p>
<p>面试官很和蔼地介绍了面试的BG是TEG，部门是AI产品部。大致介绍了下实习生的培养方式和考核方式。</p>
</li>
</ul>
</li>
<li><ul>
<li>工作期间的考核情况如何，比如周报、月报、日报这些频繁吗？</li>
</ul>
</li>
</ul>
<p>面试官大概介绍了周会情况、周报情况，并说明疫情期间，线上工作，可能会要求写日报，方便查看工作进度之类，我表示理解。</p>
<p>时长：1 h 2 min，结束，表示感谢，感谢不杀之恩！</p>
<h3 id="二面（技术面）"><a href="#二面（技术面）" class="headerlink" title="二面（技术面）"></a>二面（技术面）</h3><p>二面也是技术，但是和初面不同的是，二面没有那么关注项目了（但是也是从项目开始问），比较注重理论层面。自初面结束之后，有一周时间的空档，我恶补了基础理论知识，首先是对初面的知识查漏补缺，再刷了剑指offer，捡起了李航《统计学习方法》，基本的机器学习算法以及推导。<strong>面试形式：电话面试</strong>。</p>
<blockquote>
<p>个人发现，NLP岗位，很多面试官喜欢问：HMM、CRF、LDA这些知识。</p>
</blockquote>
<ol>
<li>看你的简历上，在做一个相似性评估的项目，大致介绍下。</li>
</ol>
<p>介绍了下我的这个项目的模型，以及用的一些方法。</p>
<ol start="2">
<li>了解，那么获取的词向量你是怎么获取的？</li>
</ol>
<p>从word2vec中获取，然后作为模型的输入（讲了一些实验操作）。</p>
<ol start="3">
<li>你的词向量自己训练过吗？</li>
</ol>
<p>我讲了很多废话。</p>
<ol start="4">
<li>你的词向量自己训练过吗？</li>
</ol>
<p>没有。（面试官心态：早说不就完了）</p>
<ol start="5">
<li>你知道几种词向量的方法？</li>
</ol>
<p>这里说的很详细，我带了一下传统的，如IDF、词袋、LDA，GloVe等偏统计方法，然后具体描述了NNLM下的模型：word2vec（和一面一样，介绍的比较详细），character level CNN Model（英文适用，中文不太适用）；转向RNN结构：传统RNN将序列压缩至向量的过程，LSTM解决RNN长依赖模型，双向语言模型（BiLSTM）；根据双向语言模型，导出了ELMo以及内部细节；主流热门的Transformer系列：Transformer内部细节详细讲了一下，然后转向GPT（Transformer Decoder，单向模型，和之前的双向模型做了个区分），详细说了一下，然后是BERT（Transformer Encoder，双向语言模型，和GPT的对比和ELMo的相同点，以及区别）详解。</p>
<p><strong>安利本人译的一篇综述：</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/109564205">邹智鹏：综述：神经网络语言模型（译）69 赞同 · 5 评论文章<img src="https://pic4.zhimg.com/v2-51ab1d559571ce449cd4580b5a47e697_ipico.jpg" alt="img"></a></p>
<p><strong>如果觉得翻译不好的话，建议论文阅读原文[ ]。</strong></p>
<ol start="6">
<li>n<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E5%85%83%E6%A8%A1%E5%9E%8B">元模型</a>了解吗，如果共现频率为零怎么解决？</li>
</ol>
<p>大致讲了下ngram与n阶马尔可夫。共现为0的解决方案有点忘记了，但是提了一下在GloVe中有提及这个情况的解决方案，但是我也忘记了。</p>
<ol start="7">
<li>你认为为什么BERT能达到这么好的效果？</li>
</ol>
<p>我认为BERT的效果很大程度上取决于Transformer的一个Attention机制，即Self-Attention，正如原文标题《Attention is all you need》，<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6">注意力机制</a>很好地找出了文本序列的语义相关度，在语义层面上，能够提取到更关键的特征，比如理解序列中的指示代词。其次是Transformer的结构优势，没有RNN的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98">梯度消失问题</a>，理论上支持任意长度的文本，用了position embedding（区别说明了下Transformer的三角函数和BERT的position embedding的不同）。</p>
<ol start="8">
<li>注意力机制你了解多少，或者说你了解哪几种？</li>
</ol>
<p>注意力机制最初是在CV领域用的比较多，其次被应用到了NLP领域，并且取得了较好的效果。我主要研究NLP比较多，CV了解不深，所以只了解NLP的两种attention。最早的Attention是在seq2seq中提出（或者说Encoder-Decoder模型），讲了下这个模型注意力机制的细节，以及注意力获取的方式，文献名我忘记了，原始的论文应该是NMT相关场景。然后讲了BERT的Self-Attention细节，比如Q、K、V这些。</p>
<blockquote>
<p>这里到BERT想要继续向后拓展的时候，被面试官打断了。</p>
</blockquote>
<ol start="9">
<li>LSTM你了解吗，它的大致结构是怎么样的？</li>
</ol>
<p>大概描述了下它的三个门。</p>
<ol start="10">
<li>假设输入是$x$维，LSTM的参数量为多少？</li>
</ol>
<p>当时着急了，没怎么算出来，事后算了下，还挺简单的。</p>
<blockquote>
<p>面试官：你下去可以再确认下。</p>
</blockquote>
<ol start="11">
<li>正则化你用过吗，有哪些<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95">正则化方法</a>？</li>
</ol>
<p>用过，我了解的有L1、L2正则化，讲了很多废话。。。</p>
<blockquote>
<p>尝试讲L1、L2的细节的时候，被打断了。</p>
</blockquote>
<ol start="12">
<li>还有其他的吗？</li>
</ol>
<p>有，比如Dropout、Batch Normalization不知道算不算，但是它能达到正则化的效果。</p>
<ol start="13">
<li>文本相似性一般有那几种度量方法？</li>
</ol>
<p><em>之前项目里有过这个相似性的问题，我不知道面试官具体想要的点在哪，就先说了下VSM的基础知识，他又再次深入地问了下，可以用什么模型。</em></p>
<p>然后我大致说了下常用的方法，比如最简单的word2vec加权，然后用VSM，RNN得到固定维度的向量后，用VSM，以及这一系列的方法。然后补充说了，利用深度学习转化为二分类的问题的思路和方法，以及BERT中的使用，因为BERT是有做文本相似性任务的。 深度学习之外，还有很多方法，比如主题模型、LDA之类的，我当时没想到，面试完后想到了。</p>
<ol start="14">
<li>序列标注做过吗？</li>
</ol>
<p>没有。</p>
<ol start="15">
<li>HMM和CRF有什么区别？</li>
</ol>
<p><em>之前恶补了HMM的理论，详细讲了HMM，但是CRF只有大概了解，就没仔细说。</em> HMM的2个假设，正向反向算法（递推式推导），EM算法，维特比算法。CRF打破了两个假设，所以效果更好。（说的比较大概，也和面试官说了这部分不是很了解，只知道个大概）</p>
<ol start="16">
<li>传统机器学习算法了解吗，比如XGBoost和SVM这些？</li>
</ol>
<p>了解。</p>
<ol start="17">
<li>那你讲一下SVM吧。</li>
</ol>
<p>讲了下SVM的推导：hard margin, soft margin, 几何距离、函数距离，最优化，<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95">拉格朗日乘子法</a>，对偶问题，KKT条件，<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">损失函数</a>，惩罚系数等。</p>
<ol start="18">
<li>为什么SVM求解要求解他的对偶问题？</li>
</ol>
<p>为了使原问题更好求解，因为原问题不好求解，引入对偶问题之后就更好求解了。</p>
<blockquote>
<p>面试官：哈哈哈，就是因为这个？好吧。</p>
</blockquote>
<p><strong>其实，是因为对偶问题可以降低原问题的计算复杂度。</strong></p>
<p>好了，结束，没让我问问题，当时是晚上8点开始面，面试官也忙，我表示感谢，他就匆匆挂电话了，时长：50 min。</p>
<h3 id="三面（HR面）"><a href="#三面（HR面）" class="headerlink" title="三面（HR面）"></a>三面（HR面）</h3><p>HR面就不涉及技术了，主要是和HR各种聊，在这之前，看网上很多面经，都说HR面还是很多坑的，即<strong>有套路</strong>，但是好在面我的HR小哥哥比较真实，没有过分的套路我。<strong>面试形式：视频面试（腾讯会议）</strong>。</p>
<ol>
<li>学校的基本情况，导师是谁，组里的研究方向是啥？</li>
<li>根据我的研究方向，问了下我们的工作。</li>
<li>有没有了解行业内你这个领域的一些工作，即你们组的研究方向的落地情况如何？</li>
<li>聊基本情况，实习时间，毕业要求。</li>
<li>家庭情况，籍贯、家里成员、父母工作。</li>
<li>实习地点有没有什么规划？</li>
<li>HR小哥哥开始介绍他们部门的业务情况，业务方向等等。</li>
<li>你有没有什么问题？</li>
<li>那么我这边也没有问题了，整个面试就算通过了，大概一周左右会有通知。（大概意思，算是口头offer）</li>
</ol>
<p>致谢，结束，时长：20-30 min。</p>
<blockquote>
<p>恕我直言，17号面完HR，忐忑无比，开始日常刷流程，直到20日晚才刷到已完成。开始牛客找组织，看有没有已完成，但是没收到offer的同学<em>（没办法，太恐怖了，眼看着3:1到我结束面试，已经5:1了，慌张）</em>。然后开始关注内部状态，⾮非常感谢⽜牛客⽹网的腾讯⼤大⽜牛们帮忙。经历5个⼯工作⽇日 后，内部状态变更更为:实习已录⽤。</p>
</blockquote>
<h3 id="小建议"><a href="#小建议" class="headerlink" title="小建议"></a>小建议</h3><p>技术岗，<strong>刷算法题</strong>！时间来不及就先剑指offer 66道，相对简单，多刷几遍，然后做leetcode。</p>
<p>算法岗，<strong>基础机器学习算法：SVM、Bayes、DT、Clustering、boosting（Adaboost、GBDT）、bagging（RF）、LR。</strong></p>
<p>项目，<strong>一定要熟悉，不熟悉的就别往简历上写</strong>。说不清楚的会被喷的很惨，一定会被diss，甚至会质疑简历真实性。简历上写了的就要再次复盘，要了然于胸，不要忘记了然后讲不明白。</p>
<p>论文，要熟悉所在领域的一些经典论文模型，里面的细节也要清楚，<strong>最好读原文</strong>。</p>
<p><strong>NLP的经典：HMM、CRF、LDA</strong>，我在很多次面试都碰到这些，重点圈出，个人遭遇，视情况准备吧。</p>
<p>HR套路，虽然面我的HR没怎么 套路我，但是HR毕竟还是要尽量选择稳定的人，所以HR面还是要长心眼，对某些敏感问题的态度要坚决，宗旨：<strong>我爱XX公司，我一定去，工作地点不挑，实习时间能满足</strong>。</p>
<p><strong>预祝各位能够斩获理想offer，加油！</strong></p>
<h1 id="简历"><a href="#简历" class="headerlink" title="简历"></a>简历</h1><h2 id="专业技能"><a href="#专业技能" class="headerlink" title="专业技能"></a>专业技能</h2><h3 id="Java版"><a href="#Java版" class="headerlink" title="Java版"></a>Java版</h3><ul>
<li>熟悉<strong>Java</strong>基础以及常用集合；了解Java泛型、反射、代理、IO。</li>
<li>熟悉<strong>Java并发编程</strong>，如锁机制、线程池机制、volatile和synchronized等；了解<strong>JVM</strong>，如类加载、JMM、GC机制。</li>
<li>熟悉<strong>Golang</strong>基础以及goroutine和channel；了解sync包工具；了解<strong>GMP调度模型</strong>、<strong>内存管理机制</strong>。</li>
<li>了解<strong>SpringBoot</strong>、MyBatis等框架的基本使用；了解<strong>IOC、AOP</strong>基本原理、<strong>循环依赖</strong>以及<strong>Bean的生命周期</strong>。</li>
<li>熟悉<strong>MySQL</strong>中表增删查改基本操作，了解 MySQL索引机制、MVCC、锁机制。</li>
<li>熟悉<strong>Redis</strong>基本使用；掌握Redis常用数据类型和实现，了解持久化和常见缓存问题。</li>
<li>熟悉<strong>Git、Docker</strong>等工具的基本使用。</li>
<li>熟悉常见<strong>数据结构与算法</strong>，如链表，数组，哈希，搜索二叉树，大小根堆，字典树等。</li>
<li>熟悉<strong>Linux</strong>常用命令的使用，如ls、ps、netstat、grep、vim等。</li>
<li>熟悉<strong>TCP&#x2F;IP</strong>四层模型以及常见网络协议，如HTTP&#x2F;HTTPS、TCP、DNS。</li>
<li>熟悉<strong>操作系统</strong>相关知识与概念，如进程&#x2F;线程&#x2F;协程、内存管理、文件系统。</li>
</ul>
<h3 id="LLM算法版"><a href="#LLM算法版" class="headerlink" title="LLM算法版"></a>LLM算法版</h3><ul>
<li>掌握深度学习的基本原理与方法，熟悉<strong>Pytorch</strong>基础框架。</li>
<li>熟悉LLM领域常识，如注意力机制、Tokenization、位置编码、Normalization、In-Context-Learning(RAG、CoT)、Pre-Train&#x2F;Post-Train、Inference等。</li>
<li>了解常见开源的模型，如Llama、GPT。</li>
<li>了解常见LLM框架，如Transformers、Langchain、vLLM。</li>
<li>熟悉Python编程语言，也有Java、Golang编程语言开发能力。</li>
<li>熟悉常见<strong>数据结构与算法</strong>，如链表，数组，哈希，搜索二叉树，大小根堆等。</li>
<li>熟悉<strong>操作系统</strong>相关知识与概念、<strong>Linux</strong>常用命令的使用、<strong>TCP&#x2F;IP</strong>四层模型、以及常见网络协议。</li>
</ul>
<h2 id="项目经历"><a href="#项目经历" class="headerlink" title="项目经历"></a>项目经历</h2><h2 id="招聘要求"><a href="#招聘要求" class="headerlink" title="招聘要求"></a>招聘要求</h2><h3 id="aiXcoder大模型算法工程师"><a href="#aiXcoder大模型算法工程师" class="headerlink" title="aiXcoder大模型算法工程师"></a>aiXcoder大模型算法工程师</h3><p>公司介绍：<br>aiXcoder是国内最早成立的大模型创业公司之一，聚焦代码垂类大模型，主要业务场景为代码辅助大模型、以及辅助编程Agent，公司初创团队来自北大，当前算法团队核心成员在大模型领域经验丰富，团队氛围良好。</p>
<p>大模型推理业务应用前景广阔，并且可以和业务场景解耦，可以迁移到其他场景的大模型或者其他类型的模型的应用场景。</p>
<p>岗位职责：<br>1.维护和迭代代码大模型的推理引擎，包括但不限于模型的推理过程构建、量化部署、请求调度与推理速度优化；<br>2.负责大模型推理服务在不同芯片上（例如英伟达GPU、华为Ascend、海光DCU等）的适配、优化，并完成模型效果测试；<br>3.调研大模型推理前沿科研成果，研读论文、复现论文成果，并尝试迁移到现有推理过程；</p>
<p>任职要求：<br>1.计算机或者相关专业在读硕士研究生，熟悉常用的数据结构及经典算法的设计思想；<br>2.熟练掌握python，加分项：了解至少一门编译型语言，例如（C&#x2F;C++）；<br>3.熟练掌握深度学习原理与方法，熟练至少一种深度学习框架，加分项：了解前沿大模型推理框架（vLLM、TensorRT-LLM或者LMdeploy）；<br>4.熟悉常见开源的代码大模型，有开发、使用或部署代码大模型的优先考虑；<br>5.可以尽快入职者优先考虑；</p>
<p>办公地点：北京市海淀区知春路京东科技大厦；<br>薪资面议，可以覆盖房租和日常开销；</p>
<p>有意者请于一周内将简历发送至：<a href="mailto:&#x59;&#111;&#x63;&#x74;&#111;&#x49;&#x6e;&#x63;&#104;&#64;&#103;&#109;&#x61;&#x69;&#108;&#x2e;&#99;&#111;&#109;">&#x59;&#111;&#x63;&#x74;&#111;&#x49;&#x6e;&#x63;&#104;&#64;&#103;&#109;&#x61;&#x69;&#108;&#x2e;&#99;&#111;&#109;</a></p>
<h2 id="别人的简历"><a href="#别人的简历" class="headerlink" title="别人的简历"></a>别人的简历</h2><img src="\llm\image-20240826124910083.png" alt="image-20240826124910083" style="zoom: 67%;" />



<img src="\llm\image-20240826124934100.png" alt="image-20240826124934100" style="zoom: 33%;" />



<img src="\llm\image-20240826124946270.png" alt="image-20240826124946270" style="zoom: 150%;" />







<h1 id="面试题"><a href="#面试题" class="headerlink" title="面试题"></a>面试题</h1><h2 id="来自：LLMs-interview-notes"><a href="#来自：LLMs-interview-notes" class="headerlink" title="来自：LLMs_interview_notes"></a>来自：<a target="_blank" rel="noopener" href="https://github.com/yinhaoxs/LLMs_interview_notes">LLMs_interview_notes</a></h2><p>感觉都是AI写的答案，不靠谱。</p>
<ol>
<li><p>目前 主流的开源模型体系 有哪些？</p>
<p>目前主流的开源LLM（语言模型）模型体系包括以下几个：</p>
<ol>
<li>GPT（Generative Pre-trained Transformer）系列：由OpenAI发布的一系列基于Transformer架构的语言模型，包括GPT、GPT-2、GPT-3等。GPT模型通过在大规模无标签文本上进行预训练，然后在特定任务上进行微调，具有很强的生成能力和语言理解能力。</li>
<li>BERT（Bidirectional Encoder Representations from Transformers）：由Google发布的一种基于Transformer架构的双向预训练语言模型。BERT模型通过在大规模无标签文本上进行预训练，然后在下游任务上进行微调，具有强大的语言理解能力和表征能力。</li>
<li>XLNet：由CMU和Google Brain发布的一种基于Transformer架构的自回归预训练语言模型。XLNet模型通过自回归方式预训练，可以建模全局依赖关系，具有更好的语言建模能力和生成能力。</li>
<li>RoBERTa：由Facebook发布的一种基于Transformer架构的预训练语言模型。RoBERTa模型在BERT的基础上进行了改进，通过更大规模的数据和更长的训练时间，取得了更好的性能。</li>
<li>T5（Text-to-Text Transfer Transformer）：由Google发布的一种基于Transformer架构的多任务预训练语言模型。T5模型通过在大规模数据集上进行预训练，可以用于多种自然语言处理任务，如文本分类、机器翻译、问答等。</li>
</ol>
<p>这些模型在自然语言处理领域取得了显著的成果，并被广泛应用于各种任务和应用中。</p>
</li>
<li><p>prefix LM 和 causal LM 区别是什么？</p>
</li>
</ol>
<p>   确实，从表面上看，前缀语言模型和因果语言模型似乎都在做相似的事情——基于给定的文本生成后续文本。然而，它们之间的关键区别在于如何处理输入数据以及它们的训练目标。</p>
<h3 id="Causal-LM-因果语言模型"><a href="#Causal-LM-因果语言模型" class="headerlink" title="Causal LM (因果语言模型)"></a>Causal LM (因果语言模型)</h3><p>   因果语言模型在训练时采用的是自回归的方式，这意味着在训练过程中，模型每次预测一个位置的输出时，只依赖于该位置之前的所有信息。例如：</p>
<ul>
<li><strong>训练数据</strong>: “我喜欢在周末的时候去公园散步。”</li>
<li><strong>训练目标</strong>: 预测每个位置的下一个词。</li>
</ul>
<p>   具体来说，对于每一个词的位置，模型都会尝试预测下一个词。比如对于“我”后面的一个词，模型会尝试预测出“喜欢”。对于“喜欢”后面的词，模型会尝试预测出“在”。</p>
<p>   在推理阶段，当使用因果语言模型生成文本时，也是基于同样的原理，即在生成每个词时，模型只会考虑之前已经生成的词。</p>
<h3 id="Prefix-LM-前缀语言模型"><a href="#Prefix-LM-前缀语言模型" class="headerlink" title="Prefix LM (前缀语言模型)"></a>Prefix LM (前缀语言模型)</h3><p>   前缀语言模型通常指的是那些在生成文本时，给定了一个固定的前缀，并在此基础上生成后续文本的模型。这种模型在训练时可能会有不同的方法，但在应用上，它的目的是基于一个给定的前缀生成合理的后续文本。</p>
<ul>
<li><strong>前缀</strong>: “我喜欢在周末的时候去公园散步。”</li>
<li><strong>生成的后续文本</strong>: “呼吸新鲜空气，享受大自然的美好。”</li>
</ul>
<p>   在这种情况下，模型不需要逐个预测每个位置的下一个词，而是根据给定的前缀生成整个后续的句子或段落。</p>
<h3 id="总结-12"><a href="#总结-12" class="headerlink" title="总结"></a>总结</h3><ul>
<li><strong>因果语言模型</strong>：每次只预测一个位置的下一个词，严格地从前向后生成文本。</li>
<li><strong>前缀语言模型</strong>：基于给定的前缀生成后续文本，可以是一次性生成多个词或整个句子。</li>
</ul>
<p>   因此，尽管它们最终看起来都是在生成文本，但因果语言模型更侧重于逐个预测词，而前缀语言模型则更侧重于基于前缀生成较长时间跨度的文本。</p>
<ol start="3">
<li><p><strong>涌现能力是啥原因？</strong></p>
<p>大模型的涌现能力主要是由以下几个原因造成的：</p>
<ol>
<li>数据量的增加：随着互联网的发展和数字化信息的爆炸增长，可用于训练模型的数据量大大增加。更多的数据可以提供更丰富、更广泛的语言知识和语境，使得模型能够更好地理解和生成文本。</li>
<li>计算能力的提升：随着计算硬件的发展，特别是图形处理器（GPU）和专用的AI芯片（如TPU）的出现，计算能力大幅提升。这使得训练更大、更复杂的模型成为可能，从而提高了模型的性能和涌现能力。</li>
<li>模型架构的改进：近年来，一些新的模型架构被引入，如Transformer，它在处理序列数据上表现出色。这些新的架构通过引入自注意力机制等技术，使得模型能够更好地捕捉长距离的依赖关系和语言结构，提高了模型的表达能力和生成能力。</li>
<li>预训练和微调的方法：预训练和微调是一种有效的训练策略，可以在大规模无标签数据上进行预训练，然后在特定任务上进行微调。这种方法可以使模型从大规模数据中学习到更丰富的语言知识和语义理解，从而提高模型的涌现能力。</li>
</ol>
<p>综上所述，大模型的涌现能力是由数据量的增加、计算能力的提升、模型架构的改进以及预训练和微调等因素共同作用的结果。这些因素的进步使得大模型能够更好地理解和生成文本，为自然语言处理领域带来了显著的进展。</p>
</li>
<li><p><strong>大模型LLM的架构介绍？</strong></p>
<ol>
<li>Transformer架构：大模型LLM常使用Transformer架构，它是一种基于自注意力机制的序列模型。Transformer架构由多个编码器层和解码器层组成，每个层都包含多头自注意力机制和前馈神经网络。这种架构可以捕捉长距离的依赖关系和语言结构，适用于处理大规模语言数据。</li>
<li>自注意力机制（Self-Attention）：自注意力机制是Transformer架构的核心组件之一。它允许模型在生成每个词时，根据输入序列中的其他词来计算该词的表示。自注意力机制能够动态地为每个词分配不同的权重，从而更好地捕捉上下文信息。</li>
<li>多头注意力（Multi-Head Attention）：多头注意力是自注意力机制的一种扩展形式。它将自注意力机制应用多次，每次使用不同的权重矩阵进行计算，得到多个注意力头。多头注意力可以提供更丰富的上下文表示，增强模型的表达能力。</li>
<li>前馈神经网络（Feed-Forward Network）：在Transformer架构中，每个注意力层后面都有一个前馈神经网络。前馈神经网络由两个全连接层组成，通过非线性激活函数（如ReLU）进行变换。它可以对注意力层输出的表示进行进一步的映射和调整。</li>
<li>预训练和微调：大模型LLM通常采用预训练和微调的方法进行训练。预训练阶段使用大规模无标签数据，通过自监督学习等方法进行训练，使模型学习到丰富的语言知识。微调阶段使用有标签的特定任务数据，如文本生成、机器翻译等，通过有监督学习进行模型的微调和优化。</li>
</ol>
<p>需要注意的是，大模型LLM的具体架构可能会因不同的研究和应用而有所不同。上述介绍的是一种常见的架构，但实际应用中可能会有一些变体或改进。</p>
</li>
<li><p><strong>如果想要在某个模型基础上做全参数微调，究竟需要多少显存？</strong></p>
<p>要确定全参数微调所需的显存量，需要考虑以下几个因素：</p>
</li>
<li><p><strong>为什么SFT之后感觉LLM傻了?</strong></p>
<p>在进行Supervised Fine-Tuning（SFT）之后，有时可能会观察到基座模型（如语言模型）的性能下降或产生一些“傻”的行为。这可能是由于以下原因：</p>
<ol>
<li>数据偏移：SFT过程中使用的微调数据集可能与基座模型在预训练阶段接触到的数据分布有所不同。如果微调数据集与预训练数据集之间存在显著的差异，模型可能会在新任务上表现较差。这种数据偏移可能导致模型在新任务上出现错误的预测或不准确的输出。</li>
<li>非典型标注：微调数据集的标注可能存在错误或不准确的标签。这些错误的标签可能会对模型的性能产生负面影响，导致模型产生“傻”的行为。</li>
<li>过拟合：如果微调数据集相对较小，或者模型的容量（参数数量）较大，模型可能会过拟合微调数据，导致在新的输入上表现不佳。过拟合可能导致模型过于依赖微调数据的特定样本，而无法泛化到更广泛的输入。</li>
<li>缺乏多样性：微调数据集可能缺乏多样性，未能涵盖模型在新任务上可能遇到的各种输入情况。这可能导致模型在面对新的、与微调数据集不同的输入时出现困惑或错误的预测。</li>
</ol>
<p>为了解决这些问题，可以尝试以下方法：</p>
<ul>
<li>收集更多的训练数据，以增加数据的多样性和覆盖范围。</li>
<li>仔细检查微调数据集的标注，确保标签的准确性和一致性。</li>
<li>使用正则化技术（如权重衰减、dropout）来减少过拟合的风险。</li>
<li>进行数据增强，通过对微调数据进行一些变换或扩充来增加多样性。</li>
<li>使用更复杂的模型架构或调整模型的超参数，以提高模型的性能和泛化能力。</li>
</ul>
<p>通过这些方法，可以尽量减少Supervised Fine-Tuning之后模型出现“傻”的情况，并提高模型在新任务上的表现。</p>
</li>
<li><p><strong>SFT 指令微调数据 如何构建?</strong></p>
<p>构建Supervised Fine-Tuning（SFT）的微调数据需要以下步骤：</p>
<ol>
<li>收集原始数据：首先，您需要收集与目标任务相关的原始数据。这可以是对话数据、分类数据、生成任务数据等，具体取决于您的任务类型。确保数据集具有代表性和多样性，以提高模型的泛化能力。</li>
<li>标注数据：对原始数据进行标注，为每个样本提供正确的标签或目标输出。标签的类型取决于您的任务，可以是分类标签、生成文本、对话回复等。确保标注的准确性和一致性。</li>
<li>划分数据集：将标注数据划分为训练集、验证集和测试集。通常，大部分数据用于训练，一小部分用于验证模型的性能和调整超参数，最后一部分用于最终评估模型的泛化能力。</li>
<li>数据预处理：根据任务的要求，对数据进行预处理。这可能包括文本清洗、分词、去除停用词、词干化等处理步骤。确保数据格式和特征表示适合模型的输入要求。</li>
<li>格式转换：将数据转换为适合模型训练的格式。这可能涉及将数据转换为文本文件、JSON格式或其他适合模型输入的格式。</li>
<li>模型微调：使用转换后的数据对基座模型进行微调。根据任务的要求，选择适当的微调方法和超参数进行训练。这可以使用常见的深度学习框架（如PyTorch、TensorFlow）来实现。</li>
<li>模型评估：使用测试集对微调后的模型进行评估，计算模型在任务上的性能指标，如准确率、召回率、生成质量等。根据评估结果对模型进行进一步的优化和调整。</li>
</ol>
<p>通过以上步骤，您可以构建适合Supervised Fine-Tuning的微调数据集，并使用该数据集对基座模型进行微调，以适应特定任务的需求。</p>
</li>
<li><p><strong>领域模型Continue PreTrain 数据选取？</strong></p>
<p>在领域模型的Continue PreTrain过程中，数据选取是一个关键的步骤。以下是一些常见的数据选取方法：</p>
<ol>
<li>领域相关数据：首先，可以收集与目标领域相关的数据。这些数据可以是从互联网上爬取的、来自特定领域的文档或者公司内部的数据等。这样的数据可以提供领域相关的语言和知识，有助于模型在特定领域上的表现。</li>
<li>领域专家标注：如果有领域专家可用，可以请他们对领域相关的数据进行标注。标注可以是分类、命名实体识别、关系抽取等任务，这样可以提供有监督的数据用于模型的训练。</li>
<li>伪标签：如果没有领域专家或者标注数据的成本较高，可以使用一些自动化的方法生成伪标签。例如，可以使用预训练的模型对领域相关的数据进行预测，将预测结果作为伪标签，然后使用这些伪标签进行模型的训练。</li>
<li>数据平衡：在进行数据选取时，需要注意数据的平衡性。如果某个类别的数据样本较少，可以考虑使用数据增强技术或者对该类别进行过采样，以平衡各个类别的数据量。</li>
<li>数据质量控制：在进行数据选取时，需要对数据的质量进行控制。可以使用一些质量评估指标，如数据的准确性、一致性等，来筛选和过滤数据。</li>
<li>数据预处理：在进行数据选取之前，可能需要对数据进行一些预处理，如分词、去除停用词、标准化等，以准备好输入模型进行训练。</li>
</ol>
<p>在数据选取过程中，需要根据具体任务和需求进行适当的调整和定制。选择合适的数据可以提高模型在特定领域上的性能和泛化能力。</p>
</li>
<li><p><strong>领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？</strong></p>
<p>当使用领域数据进行训练后，模型往往会出现遗忘通用能力的问题。以下是一些缓解模型遗忘通用能力的方法：</p>
<ol>
<li>保留通用数据：在进行领域数据训练时，仍然需要保留一部分通用数据用于模型训练。这样可以确保模型仍然能够学习到通用的语言和知识，从而保持一定的通用能力。</li>
<li>增量学习：使用增量学习（Incremental Learning）的方法，将领域数据与通用数据逐步交替进行训练。这样可以在学习新领域的同时，保持对通用知识的记忆。</li>
<li>预训练和微调：在领域数据训练之前，可以使用大规模通用数据进行预训练，获得一个通用的基础模型。然后，在领域数据上进行微调，以适应特定领域的任务。这样可以在保留通用能力的同时，提升领域任务的性能。</li>
<li>强化学习：使用强化学习的方法，通过给模型设置奖励机制，鼓励模型在领域任务上表现好，同时保持一定的通用能力。</li>
<li>领域适应技术：使用领域适应技术，如领域自适应（Domain Adaptation）和领域对抗训练（Domain Adversarial Training），帮助模型在不同领域之间进行迁移学习，从而减少遗忘通用能力的问题。</li>
<li>数据重采样：在进行领域数据训练时，可以使用数据重采样的方法，使得模型在训练过程中能够更多地接触到通用数据，从而缓解遗忘通用能力的问题。</li>
</ol>
<p>综合使用上述方法，可以在一定程度上缓解模型遗忘通用能力的问题，使得模型既能够适应特定领域的任务，又能够保持一定的通用能力。</p>
</li>
<li><p><strong>领域模型Continue PreTrain ，如何 让模型在预训练过程中就学习到更多的知识？</strong></p>
<p>在领域模型的Continue PreTrain过程中，可以采取一些策略来让模型在预训练过程中学习到更多的知识。以下是一些方法：</p>
<ol>
<li>多任务学习：在预训练过程中，可以引入多个任务，使得模型能够学习到更多的知识。这些任务可以是领域相关的任务，也可以是通用的语言理解任务。通过同时训练多个任务，模型可以学习到更多的语言规律和知识。</li>
<li>多领域数据：收集来自不同领域的数据，包括目标领域和其他相关领域的数据。将这些数据混合在一起进行预训练，可以使得模型在不同领域的知识都得到学习和融合。</li>
<li>大规模数据：使用更大规模的数据进行预训练，可以让模型接触到更多的语言和知识。可以从互联网上爬取大量的文本数据，或者利用公开的语料库进行预训练。</li>
<li>数据增强：在预训练过程中，可以采用数据增强的技术，如随机遮挡、词替换、句子重组等，来生成更多的训练样本。这样可以增加模型的训练数据量，使其能够学习到更多的知识和语言规律。</li>
<li>自监督学习：引入自监督学习的方法，通过设计一些自动生成的标签或任务，让模型在无监督的情况下进行预训练。例如，可以设计一个掩码语言模型任务，让模型预测被掩码的词语。这样可以使模型在预训练过程中学习到更多的语言知识。</li>
</ol>
<p>综合使用上述方法，可以让模型在预训练过程中学习到更多的知识和语言规律，提升其在领域任务上的性能。</p>
</li>
<li><p><strong>进行SFT操作的时候，基座模型选用Chat还是Base?</strong></p>
<p>在进行Supervised Fine-Tuning（SFT）操作时，基座模型的选择也可以根据具体情况来决定。与之前的SFT操作不同，这次的目标是在特定的监督任务上进行微调，因此选择基座模型时需要考虑任务的性质和数据集的特点。</p>
<p>如果您的监督任务是对话生成相关的，比如生成对话回复或对话情感分类等，那么选择ChatGPT模型作为基座模型可能更合适。ChatGPT模型在对话生成任务上进行了专门的优化和训练，具有更好的对话交互能力。</p>
<p>然而，如果您的监督任务是单轮文本生成或非对话生成任务，那么选择Base GPT模型作为基座模型可能更合适。Base GPT模型在单轮文本生成和非对话生成任务上表现良好，可以提供更准确的文本生成能力。</p>
<p>总之，基座模型的选择应该根据监督任务的性质和数据集的特点进行权衡。如果任务是对话生成相关的，可以选择ChatGPT模型作为基座模型；如果任务是单轮文本生成或非对话生成，可以选择Base GPT模型作为基座模型。</p>
</li>
<li><p><strong>领域模型微调 指令&amp;数据输入格式 要求？</strong></p>
<p>领域模型微调是指使用预训练的通用语言模型（如BERT、GPT等）对特定领域的数据进行微调，以适应该领域的任务需求。以下是领域模型微调的指令和数据输入格式的要求：</p>
<p>指令：</p>
<ol>
<li>定义任务：明确所需的任务类型，如文本分类、命名实体识别、情感分析等。</li>
<li>选择预训练模型：根据任务需求选择适合的预训练模型，如BERT、GPT等。</li>
<li>准备微调数据：收集和标注与领域任务相关的数据，确保数据集具有代表性和多样性。</li>
<li>数据预处理：根据任务的要求，对数据进行预处理，例如分词、去除停用词、词干化等。</li>
<li>划分数据集：将数据集划分为训练集、验证集和测试集，用于模型的训练、验证和评估。</li>
<li>模型微调：使用预训练模型和微调数据对模型进行微调，调整超参数并进行训练。</li>
<li>模型评估：使用测试集评估微调后的模型的性能，计算适当的评估指标，如准确率、召回率等。</li>
<li>模型应用：将微调后的模型应用于实际任务，在新的输入上进行预测或生成。</li>
</ol>
<p>数据输入格式要求：</p>
<ol>
<li>输入数据应以文本形式提供，每个样本对应一行。</li>
<li>对于分类任务，每个样本应包含文本和标签，可以使用制表符或逗号将文本和标签分隔开。</li>
<li>对于生成任务，每个样本只需包含文本即可。</li>
<li>对于序列标注任务，每个样本应包含文本和对应的标签序列，可以使用制表符或逗号将文本和标签序列分隔开。</li>
<li>数据集应以常见的文件格式（如文本文件、CSV文件、JSON文件等）保存，并确保数据的格式与模型输入的要求一致。</li>
</ol>
<p>根据具体的任务和模型要求，数据输入格式可能会有所不同。在进行领域模型微调之前，建议仔细阅读所使用模型的文档和示例代码，以了解其具体的数据输入格式要求。</p>
</li>
<li><p><strong>领域模型微调 领域评测集 构建？</strong></p>
<p>构建领域评测集的过程可以参考以下步骤：</p>
<ol>
<li>收集数据：首先需要收集与目标领域相关的数据。这可以包括从互联网上爬取文本数据、使用已有的公开数据集或者通过与领域专家合作来获取数据。确保数据集具有代表性和多样性，能够涵盖领域中的各种情况和语境。</li>
<li>标注数据：对收集到的数据进行标注，以便用于评测模型的性能。标注可以根据任务类型来进行，如文本分类、命名实体识别、关系抽取等。标注过程可以由人工标注或者使用自动化工具进行，具体取决于数据集的规模和可行性。</li>
<li>划分数据集：将标注好的数据集划分为训练集、验证集和测试集。通常，训练集用于模型的训练，验证集用于调整超参数和模型选择，测试集用于最终评估模型的性能。划分数据集时要确保每个集合中的样本都具有代表性和多样性。</li>
<li>设计评测指标：根据任务类型和领域需求，选择合适的评测指标来评估模型的性能。例如，对于文本分类任务，可以使用准确率、召回率、F1值等指标来衡量模型的分类性能。</li>
<li>进行评测：使用构建好的评测集对微调后的模型进行评测。将评测集输入模型，获取模型的预测结果，并与标注结果进行比较，计算评测指标。</li>
<li>分析和改进：根据评测结果，分析模型在不同方面的表现，并根据需要进行模型的改进和调整。可以尝试不同的超参数设置、模型架构或优化算法，以提高模型的性能。</li>
</ol>
<p>重复以上步骤，不断优化模型，直到达到满意的评测结果为止。</p>
<p>需要注意的是，构建领域评测集是一个耗时且需要专业知识的过程。在进行领域模型微调之前，建议与领域专家合作，确保评测集的质量和有效性。此外，还可以参考相关研究论文和公开数据集，以获取更多关于领域评测集构建的指导和经验。</p>
</li>
<li><p><strong>领域模型词表扩增是不是有必要的？</strong></p>
<p>领域模型的词表扩增可以有助于提升模型在特定领域任务上的性能，但是否有必要取决于具体的情况。以下是一些考虑因素：</p>
<ol>
<li>领域特定词汇：如果目标领域中存在一些特定的词汇或术语，而这些词汇在通用的预训练模型的词表中没有覆盖到，那么词表扩增就是必要的。通过将这些领域特定的词汇添加到模型的词表中，可以使模型更好地理解和处理这些特定的词汇。</li>
<li>领域特定上下文：在某些领域任务中，词汇的含义可能会受到特定上下文的影响。例如，在医学领域中，同一个词汇在不同的上下文中可能具有不同的含义。如果领域任务中的上下文与通用预训练模型的训练数据中的上下文有较大差异，那么词表扩增可以帮助模型更好地理解和处理领域特定的上下文。</li>
<li>数据稀缺性：如果目标领域的训练数据相对较少，而通用预训练模型的词表较大，那么词表扩增可以帮助模型更好地利用预训练模型的知识，并提升在目标领域任务上的性能。</li>
</ol>
<p>需要注意的是，词表扩增可能会增加模型的计算和存储成本。因此，在决定是否进行词表扩增时，需要综合考虑领域特定词汇的重要性、数据稀缺性以及计算资源的限制等因素。有时候，简单的词表截断或者使用基于规则的方法来处理领域特定词汇也可以取得不错的效果。最佳的词表扩增策略会因特定任务和领域的需求而有所不同，建议根据具体情况进行评估和实验。</p>
</li>
<li><p><strong>如何训练自己的大模型？</strong></p>
<p>训练自己的大模型通常需要以下步骤：</p>
<ol>
<li>数据收集和准备：首先，需要收集与目标任务和领域相关的大规模数据集。这可以包括从互联网上爬取数据、使用公开数据集或者与合作伙伴合作获取数据。然后，对数据进行预处理和清洗，包括去除噪声、处理缺失值、标准化数据等。</li>
<li>模型设计和架构选择：根据任务的特点和目标，选择适合的模型架构。可以基于已有的模型进行修改和调整，或者设计全新的模型。常见的大模型架构包括深度神经网络（如卷积神经网络、循环神经网络、Transformer等）和预训练语言模型（如BERT、GPT等）。</li>
<li>数据划分和预处理：将数据集划分为训练集、验证集和测试集。训练集用于模型的训练，验证集用于调整超参数和模型选择，测试集用于最终评估模型的性能。进行数据预处理，如分词、编码、标记化、特征提取等，以便输入到模型中。</li>
<li>模型训练：使用训练集对模型进行训练。训练过程中，需要选择合适的优化算法、损失函数和学习率等超参数，并进行适当的调整和优化。可以使用GPU或者分布式训练来加速训练过程。</li>
<li>模型调优和验证：使用验证集对训练过程中的模型进行调优和验证。根据验证集的性能指标，调整模型的超参数、网络结构或者其他相关参数，以提升模型的性能。</li>
<li>模型评估和测试：使用测试集对最终训练好的模型进行评估和测试。计算模型的性能指标，如准确率、召回率、F1值等，评估模型的性能和泛化能力。</li>
<li>模型部署和优化：将训练好的模型部署到实际应用中。根据实际需求，对模型进行进一步的优化和调整，以提高模型的效率和性能。</li>
</ol>
<p>需要注意的是，训练自己的大模型通常需要大量的计算资源和时间。可以考虑使用云计算平台或者分布式训练来加速训练过程。此外，对于大模型的训练，还需要仔细选择合适的超参数和进行调优，以避免过拟合或者欠拟合的问题。</p>
</li>
<li><p><strong>训练中文大模型有啥经验？</strong></p>
<p>训练中文大模型时，以下经验可能会有所帮助：</p>
<ol>
<li>数据预处理：对于中文文本，常见的预处理步骤包括分词、去除停用词、词性标注、拼音转换等。分词是中文处理的基本步骤，可以使用成熟的中文分词工具，如jieba、pkuseg等。</li>
<li>数据增强：中文数据集可能相对有限，可以考虑使用数据增强技术来扩充数据集。例如，可以使用同义词替换、随机插入或删除词语、句子重组等方法来生成新的训练样本。</li>
<li>字词级别的表示：中文中既有字级别的表示，也有词级别的表示。对于字级别的表示，可以使用字符嵌入或者字级别的CNN、RNN等模型。对于词级别的表示，可以使用预训练的词向量，如Word2Vec、GloVe等。</li>
<li>预训练模型：可以考虑使用已经在大规模中文语料上预训练好的模型作为初始模型，然后在目标任务上进行微调。例如，可以使用BERT、GPT等预训练语言模型。这样可以利用大规模中文语料的信息，提升模型的表达能力和泛化能力。</li>
<li>中文特定的任务：对于一些中文特定的任务，例如中文分词、命名实体识别、情感分析等，可以使用一些中文特定的工具或者模型来辅助训练。例如，可以使用THULAC、LTP等中文NLP工具包。</li>
<li>计算资源：训练大模型需要大量的计算资源，包括GPU、内存和存储。可以考虑使用云计算平台或者分布式训练来加速训练过程。</li>
<li>超参数调优：对于大模型的训练，超参数的选择和调优非常重要。可以使用网格搜索、随机搜索或者基于优化算法的自动调参方法来寻找最佳的超参数组合。</li>
</ol>
<p>需要注意的是，中文的复杂性和语义特点可能会对模型的训练和性能产生影响。因此，在训练中文大模型时，需要充分理解中文语言的特点，并根据具体任务和需求进行调整和优化。同时，也可以参考相关的中文自然语言处理研究和实践经验，以获取更多的指导和启发。</p>
</li>
<li><p><strong>指令微调的好处？</strong></p>
<p>在大模型训练中进行指令微调（Instruction Fine-tuning）的好处包括：</p>
<ol>
<li>个性化适应：大模型通常是在大规模通用数据上进行训练的，具有强大的语言理解和表示能力。但是，对于某些特定任务或领域，模型可能需要更加个性化的适应。通过指令微调，可以在大模型的基础上，使用特定任务或领域的数据进行微调，使模型更好地适应目标任务的特点。</li>
<li>提升性能：大模型的泛化能力通常很强，但在某些特定任务上可能存在一定的性能瓶颈。通过指令微调，可以针对特定任务的要求，调整模型的参数和结构，以提升性能。例如，在机器翻译任务中，可以通过指令微调来调整注意力机制、解码器结构等，以提高翻译质量。</li>
<li>控制模型行为：大模型通常具有很高的复杂性和参数数量，其行为可能难以解释和控制。通过指令微调，可以引入特定的指令或约束，以约束模型的行为，使其更符合特定任务的需求。例如，在生成式任务中，可以使用基于指令的方法来控制生成结果的风格、长度等。</li>
<li>数据效率：大模型的训练通常需要大量的数据，但在某些任务或领域中，特定数据可能相对稀缺或难以获取。通过指令微调，可以利用大模型在通用数据上的预训练知识，结合少量特定任务数据进行微调，从而在数据有限的情况下获得更好的性能。</li>
<li>提高训练效率：大模型的训练通常需要大量的计算资源和时间。通过指令微调，可以在已经训练好的大模型的基础上进行微调，避免从头开始训练的时间和资源消耗，从而提高训练效率。</li>
</ol>
<p>指令微调的好处在于在大模型的基础上进行个性化调整，以适应特定任务的需求和提升性能，同时还能节省训练时间和资源消耗。</p>
</li>
<li><p><strong>预训练和微调哪个阶段注入知识的？</strong></p>
<p>在大模型训练过程中，知识注入通常是在预训练阶段进行的。具体来说，大模型的训练一般包括两个阶段：预训练和微调。</p>
<p>在预训练阶段，使用大规模的通用数据对模型进行训练，以学习语言知识和表示能力。这一阶段的目标是通过自监督学习或其他无监督学习方法，让模型尽可能地捕捉到数据中的统计规律和语言结构，并生成丰富的语言表示。</p>
<p>在预训练阶段，模型并没有针对特定任务进行优化，因此预训练模型通常是通用的，可以应用于多个不同的任务和领域。</p>
<p>在微调阶段，使用特定任务的数据对预训练模型进行进一步的训练和调整。微调的目标是将预训练模型中学到的通用知识和能力迁移到特定任务上，提升模型在目标任务上的性能。</p>
<p>在微调阶段，可以根据具体任务的需求，调整模型的参数和结构，以更好地适应目标任务的特点。微调通常需要较少的任务数据，因为预训练模型已经具备了一定的语言理解和泛化能力。</p>
<p>因此，知识注入是在预训练阶段进行的，预训练模型通过大规模通用数据的训练，学习到了丰富的语言知识和表示能力，为后续的微调阶段提供了基础。微调阶段则是在预训练模型的基础上，使用特定任务的数据进行进一步训练和调整，以提升性能。</p>
</li>
<li><p><strong>想让模型学习某个领域或行业的知识，是应该预训练还是应该微调？</strong></p>
<p>如果你想让大语言模型学习某个特定领域或行业的知识，通常建议进行微调而不是预训练。</p>
<p>预训练阶段是在大规模通用数据上进行的，旨在为模型提供通用的语言理解和表示能力。预训练模型通常具有较强的泛化能力，可以适用于多个不同的任务和领域。然而，由于预训练模型是在通用数据上进行训练的，其对特定领域的知识和术语可能了解有限。</p>
<p>因此，如果你希望大语言模型能够学习某个特定领域或行业的知识，微调是更合适的选择。在微调阶段，你可以使用特定领域的数据对预训练模型进行进一步训练和调整，以使模型更好地适应目标领域的特点和需求。微调可以帮助模型更深入地理解特定领域的术语、概念和语境，并提升在该领域任务上的性能。</p>
<p>微调通常需要较少的任务数据，因为预训练模型已经具备了一定的语言理解和泛化能力。通过微调，你可以在预训练模型的基础上，利用特定领域的数据进行有针对性的调整，以使模型更好地适应目标领域的需求。</p>
<p>总之，如果你希望大语言模型学习某个特定领域或行业的知识，建议进行微调而不是预训练。微调可以帮助模型更好地适应目标领域的特点和需求，并提升在该领域任务上的性能。</p>
</li>
<li><p><strong>多轮对话任务如何微调模型？</strong></p>
<p>微调大语言模型用于多轮对话任务时，可以采用以下步骤：</p>
<ol>
<li>数据准备：收集或生成与目标对话任务相关的数据集。数据集应包含多轮对话的对话历史、当前对话回合的输入和对应的回答。</li>
<li>模型选择：选择一个合适的预训练模型作为基础模型。例如，可以选择GPT、BERT等大型语言模型作为基础模型。</li>
<li>任务特定层：为了适应多轮对话任务，需要在预训练模型上添加一些任务特定的层。这些层可以用于处理对话历史、上下文理解和生成回答等任务相关的操作。</li>
<li>微调过程：使用多轮对话数据集对预训练模型进行微调。微调的过程类似于监督学习，通过最小化模型在训练集上的损失函数来优化模型参数。可以使用常见的优化算法，如随机梯度下降（SGD）或Adam。</li>
<li>超参数调整：微调过程中需要选择合适的学习率、批次大小、训练轮数等超参数。可以通过交叉验证或其他调参方法来选择最佳的超参数组合。</li>
<li>评估和调优：使用验证集或开发集对微调后的模型进行评估。可以计算模型在多轮对话任务上的指标，如准确率、召回率、F1分数等，以选择最佳模型。</li>
<li>推理和部署：在微调后，可以使用微调后的模型进行推理和部署。将输入的多轮对话输入给模型，模型将生成对应的回答。</li>
</ol>
<p>需要注意的是，微调大语言模型用于多轮对话任务时，数据集的质量和多样性对模型性能至关重要。确保数据集包含各种对话场景和多样的对话历史，以提高模型的泛化能力和适应性。</p>
<p>此外，还可以使用一些技巧来增强模型性能，如数据增强、对抗训练、模型融合等。这些技巧可以进一步提高模型在多轮对话任务上的表现。</p>
</li>
<li><p><strong>微调后的模型出现能力劣化，灾难性遗忘是怎么回事？</strong></p>
<p>灾难性遗忘（Catastrophic Forgetting）是指在模型微调过程中，当模型在新任务上进行训练时，可能会忘记之前学习到的知识，导致在旧任务上的性能下降。这种现象常见于神经网络模型的迁移学习或连续学习场景中。</p>
<p>在微调大语言模型时，灾难性遗忘可能出现的原因包括：</p>
<ol>
<li>数据分布差异：微调过程中使用的新任务数据与预训练数据或旧任务数据的分布存在差异。如果新任务的数据分布与预训练数据差异较大，模型可能会过度调整以适应新任务，导致旧任务上的性能下降。</li>
<li>参数更新冲突：微调过程中，对新任务进行训练时，模型参数可能会被更新，导致之前学习到的知识被覆盖或丢失。新任务的梯度更新可能会与旧任务的梯度更新发生冲突，导致旧任务的知识被遗忘。</li>
</ol>
<p>为了解决灾难性遗忘问题，可以尝试以下方法：</p>
<ol>
<li>重播缓冲区（Replay Buffer）：在微调过程中，使用一个缓冲区来存储旧任务的样本，然后将旧任务的样本与新任务的样本一起用于训练。这样可以保留旧任务的知识，减少灾难性遗忘的发生。</li>
<li>弹性权重共享（Elastic Weight Consolidation）：通过引入正则化项，限制模型参数的变动范围，以保护之前学习到的知识。这种方法可以在微调过程中平衡新任务和旧任务之间的重要性。</li>
<li>增量学习（Incremental Learning）：将微调过程分为多个阶段，每个阶段只微调一小部分参数。这样可以逐步引入新任务，减少参数更新的冲突，降低灾难性遗忘的风险。</li>
<li>多任务学习（Multi-Task Learning）：在微调过程中，同时训练多个相关任务，以提高模型的泛化能力和抗遗忘能力。通过共享模型参数，可以在不同任务之间传递知识，减少灾难性遗忘的影响。</li>
</ol>
<p>综上所述，灾难性遗忘是在模型微调过程中可能出现的问题。通过合适的方法和技术，可以减少灾难性遗忘的发生，保留之前学习到的知识，提高模型的整体性能。</p>
</li>
<li><p><strong>微调模型需要多大显存？</strong></p>
<p>微调大语言模型所需的显存大小取决于多个因素，包括模型的大小、批次大小、序列长度和训练过程中使用的优化算法等。</p>
<p>对于大型语言模型，如GPT-2、GPT-3等，它们通常具有数亿或数十亿个参数，因此需要大量的显存来存储模型参数和梯度。一般来说，微调这些大型语言模型需要至少16GB以上的显存。</p>
<p>此外，批次大小和序列长度也会对显存需求产生影响。较大的批次大小和较长的序列长度会占用更多的显存。如果显存不足以容纳整个批次或序列，可能需要减小批次大小或序列长度，或者使用分布式训练等策略来解决显存不足的问题。</p>
<p>需要注意的是，显存需求还受到训练过程中使用的优化算法的影响。例如，如果使用梯度累积（Gradient Accumulation）来增加批次大小，可能需要更大的显存来存储累积的梯度。</p>
<p>综上所述，微调大语言模型所需的显存大小取决于模型的大小、批次大小、序列长度和训练过程中使用的优化算法等因素。在进行微调之前，需要确保显存足够大以容纳模型和训练过程中的数据。如果显存不足，可以考虑减小批次大小、序列长度或使用分布式训练等策略来解决显存不足的问题。</p>
</li>
<li><p><strong>大模型LLM进行SFT操作的时候在学习什么？</strong></p>
<p>在大语言模型（LLM）进行有监督微调（Supervised Fine-Tuning）时，模型主要学习以下内容：</p>
<ol>
<li>任务特定的标签预测：在有监督微调中，模型会根据给定的任务，学习预测相应的标签或目标。例如，对于文本分类任务，模型会学习将输入文本映射到正确的类别标签。</li>
<li>上下文理解和语言模式：大语言模型在预训练阶段已经学习到了大量的语言知识和模式。在有监督微调中，模型会利用这些学习到的知识来更好地理解任务相关的上下文，并捕捉语言中的各种模式和规律。</li>
<li>特征提取和表示学习：微调过程中，模型会通过学习任务相关的表示来提取有用的特征。这些特征可以帮助模型更好地区分不同的类别或进行其他任务相关的操作。</li>
<li>任务相关的优化：在有监督微调中，模型会通过反向传播和优化算法来调整模型参数，使得模型在给定任务上的性能最优化。模型会学习如何通过梯度下降来最小化损失函数，从而提高任务的准确性或其他性能指标。</li>
</ol>
<p>总的来说，有监督微调阶段主要通过任务特定的标签预测、上下文理解和语言模式、特征提取和表示学习以及任务相关的优化来进行学习。通过这些学习，模型可以适应特定的任务，并在该任务上表现出良好的性能。</p>
</li>
<li><p><strong>预训练和SFT操作有什么不同</strong></p>
<p>大语言模型的预训练和有监督微调（Supervised Fine-Tuning）是两个不同的操作，它们在目标、数据和训练方式等方面存在一些区别。</p>
<ol>
<li><p>目标：预训练的目标是通过无监督学习从大规模的文本语料库中学习语言模型的表示能力和语言知识。预训练的目标通常是通过自我预测任务，例如掩码语言模型（Masked Language Model，MLM）或下一句预测（Next Sentence Prediction，NSP）等，来训练模型。</p>
<p>有监督微调的目标是在特定的任务上进行训练，例如文本分类、命名实体识别等。在有监督微调中，模型会利用预训练阶段学到的语言表示和知识，通过有监督的方式调整模型参数，以适应特定任务的要求。</p>
</li>
<li><p>数据：在预训练阶段，大语言模型通常使用大规模的无标签文本数据进行训练，例如维基百科、网页文本等。这些数据没有特定的标签或任务信息，模型通过自我预测任务来学习语言模型。</p>
<p>在有监督微调中，模型需要使用带有标签的任务相关数据进行训练。这些数据通常是人工标注的，包含了输入文本和对应的标签或目标。模型通过这些标签来进行有监督学习，调整参数以适应特定任务。</p>
</li>
<li><p>训练方式：预训练阶段通常使用无监督的方式进行训练，模型通过最大化预训练任务的目标函数来学习语言模型的表示能力。</p>
<p>有监督微调阶段则使用有监督的方式进行训练，模型通过最小化损失函数来学习任务相关的特征和模式。在微调阶段，通常会使用预训练模型的参数作为初始参数，并在任务相关的数据上进行训练。</p>
</li>
</ol>
<p>总的来说，预训练和有监督微调是大语言模型训练的两个阶段，目标、数据和训练方式等方面存在差异。预训练阶段通过无监督学习从大规模文本数据中学习语言模型，而有监督微调阶段则在特定任务上使用带有标签的数据进行有监督学习，以适应任务要求。</p>
</li>
<li><p><strong>样本量规模增大，训练出现OOM错</strong></p>
<p>当在大语言模型训练过程中，样本量规模增大导致内存不足的情况出现时，可以考虑以下几种解决方案：</p>
<ol>
<li>减少批量大小（Batch Size）：将批量大小减小可以减少每个训练步骤中所需的内存量。较小的批量大小可能会导致训练过程中的梯度估计不稳定，但可以通过增加训练步骤的数量来弥补这一问题。</li>
<li>分布式训练：使用多台机器或多个GPU进行分布式训练可以将训练负载分散到多个设备上，从而减少单个设备上的内存需求。通过分布式训练，可以将模型参数和梯度在多个设备之间进行同步和更新。</li>
<li>内存优化技术：使用一些内存优化技术可以减少模型训练过程中的内存占用。例如，使用混合精度训练（Mixed Precision Training）可以减少模型参数的内存占用；使用梯度累积（Gradient Accumulation）可以减少每个训练步骤中的内存需求。</li>
<li>减少模型规模：如果内存问题仍然存在，可以考虑减少模型的规模，例如减少模型的层数、隐藏单元的数量等。虽然这可能会导致模型性能的一定损失，但可以在一定程度上减少内存需求。</li>
<li>增加硬件资源：如果条件允许，可以考虑增加硬件资源，例如增加内存容量或使用更高内存的设备。这样可以提供更多的内存空间来容纳更大规模的训练数据。</li>
<li>数据处理和加载优化：优化数据处理和加载过程可以减少训练过程中的内存占用。例如，可以使用数据流水线技术来并行加载和处理数据，减少内存中同时存在的数据量。</li>
</ol>
<p>综上所述，当在大语言模型训练中遇到内存不足的问题时，可以通过减小批量大小、分布式训练、内存优化技术、减少模型规模、增加硬件资源或优化数据处理等方式来解决。具体的解决方案需要根据具体情况进行选择和调整。</p>
</li>
<li><p><strong>大模型LLM进行SFT 如何对样本进行优化？</strong></p>
<p>对于大语言模型进行有监督微调（Supervised Fine-Tuning）时，可以采用以下几种方式对样本进行优化：</p>
<ol>
<li>数据清洗和预处理：对于有监督微调的任务，首先需要对样本数据进行清洗和预处理。这包括去除噪声、处理缺失值、进行标准化或归一化等操作，以确保数据的质量和一致性。</li>
<li>数据增强：通过数据增强技术可以扩充训练数据，增加样本的多样性和数量。例如，可以使用数据扩充方法如随机裁剪、旋转、翻转、加噪声等来生成新的训练样本，从而提高模型的泛化能力。</li>
<li>标签平衡：如果样本标签不平衡，即某些类别的样本数量远远多于其他类别，可以采取一些方法来平衡样本标签。例如，可以通过欠采样、过采样或生成合成样本等技术来平衡不同类别的样本数量。</li>
<li>样本选择：在有限的资源和时间下，可以选择一部分具有代表性的样本进行微调训练。可以根据任务的需求和数据分布的特点，选择一些关键样本或难样本进行训练，以提高模型在关键样本上的性能。</li>
<li>样本权重：对于一些重要的样本或困难样本，可以给予更高的权重，以便模型更加关注这些样本的学习。可以通过调整损失函数中样本的权重或采用加权采样的方式来实现。</li>
<li>样本组合和分割：根据任务的特点和数据的结构，可以将多个样本组合成一个样本，或将一个样本分割成多个子样本。这样可以扩展训练数据，提供更多的信息和多样性。</li>
<li>样本筛选和策略：根据任务需求，可以制定一些样本筛选和选择策略。例如，可以根据样本的置信度、难度、多样性等指标进行筛选和选择，以提高模型的性能和泛化能力。</li>
</ol>
<p>总的来说，对大语言模型进行有监督微调时，可以通过数据清洗和预处理、数据增强、标签平衡、样本选择、样本权重、样本组合和分割、样本筛选和策略等方式对样本进行优化。这些优化方法可以提高训练样本的质量、多样性和数量，从而提升模型的性能和泛化能力。具体的优化策略需要根据任务需求和数据特点进行选择和调整。</p>
</li>
<li><p><strong>模型参数迭代实验</strong></p>
<p>模型参数迭代实验是指通过多次迭代更新模型参数，以逐步优化模型性能的过程。在实验中，可以尝试不同的参数更新策略、学习率调整方法、正则化技术等，以找到最佳的参数配置，从而达到更好的模型性能。</p>
<p>下面是一个基本的模型参数迭代实验过程：</p>
<ol>
<li>设定初始参数：首先，需要设定初始的模型参数。可以通过随机初始化或使用预训练模型的参数作为初始值。</li>
<li>选择损失函数：根据任务的特点，选择适当的损失函数作为模型的优化目标。常见的损失函数包括均方误差（MSE）、交叉熵损失等。</li>
<li>选择优化算法：选择适当的优化算法来更新模型参数。常见的优化算法包括随机梯度下降（SGD）、Adam、Adagrad等。可以尝试不同的优化算法，比较它们在模型训练过程中的效果。</li>
<li>划分训练集和验证集：将样本数据划分为训练集和验证集。训练集用于模型参数的更新，验证集用于评估模型性能和调整超参数。</li>
<li>迭代更新参数：通过多次迭代更新模型参数来优化模型。每次迭代中，使用训练集的一批样本进行前向传播和反向传播，计算损失函数并更新参数。可以根据需要调整批量大小、学习率等超参数。</li>
<li>评估模型性能：在每次迭代的过程中，可以使用验证集评估模型的性能。可以计算准确率、精确率、召回率、F1值等指标，以及绘制学习曲线、混淆矩阵等来分析模型的性能。</li>
<li>调整超参数：根据验证集的评估结果，可以调整超参数，如学习率、正则化系数等，以进一步提升模型性能。可以使用网格搜索、随机搜索等方法来寻找最佳的超参数配置。</li>
<li>终止条件：可以设置终止条件，如达到最大迭代次数、模型性能不再提升等。当满足终止条件时，结束模型参数迭代实验。</li>
</ol>
<p>通过模型参数迭代实验，可以逐步优化模型性能，找到最佳的参数配置。在实验过程中，需要注意过拟合和欠拟合等问题，并及时调整模型结构和正则化技术来解决。同时，要进行合理的实验设计和结果分析，以得到可靠的实验结论。</p>
</li>
<li><p><strong>为什么大模型推理时显存涨的那么多还一直占着？</strong></p>
<p>大语言模型进行推理时，显存涨得很多且一直占着显存不释放的原因主要有以下几点：</p>
<ol>
<li>模型参数占用显存：大语言模型通常具有巨大的参数量，这些参数需要存储在显存中以供推理使用。因此，在推理过程中，模型参数会占用相当大的显存空间。</li>
<li>输入数据占用显存：进行推理时，需要将输入数据加载到显存中。对于大语言模型而言，输入数据通常也会占用较大的显存空间，尤其是对于较长的文本输入。</li>
<li>中间计算结果占用显存：在推理过程中，模型会进行一系列的计算操作，生成中间结果。这些中间结果也需要存储在显存中，以便后续计算使用。对于大语言模型而言，中间计算结果可能会占用较多的显存空间。</li>
<li>内存管理策略：某些深度学习框架在推理时采用了一种延迟释放显存的策略，即显存不会立即释放，而是保留一段时间以备后续使用。这种策略可以减少显存的分配和释放频率，提高推理效率，但也会导致显存一直占用的现象。</li>
</ol>
<p>需要注意的是，显存的占用情况可能会受到硬件设备、深度学习框架和模型实现的影响。不同的环境和设置可能会导致显存占用的差异。如果显存占用过多导致资源不足或性能下降，可以考虑调整模型的批量大小、优化显存分配策略或使用更高性能的硬件设备来解决问题。</p>
</li>
<li><p><strong>大模型在gpu和cpu上推理速度如何？</strong></p>
<p>大语言模型在GPU和CPU上进行推理的速度存在显著差异。一般情况下，GPU在进行深度学习推理任务时具有更高的计算性能，因此大语言模型在GPU上的推理速度通常会比在CPU上更快。</p>
<p>以下是GPU和CPU在大语言模型推理速度方面的一些特点：</p>
<ol>
<li>GPU推理速度快：GPU具有大量的并行计算单元，可以同时处理多个计算任务。对于大语言模型而言，GPU可以更高效地执行矩阵运算和神经网络计算，从而加速推理过程。</li>
<li>CPU推理速度相对较慢：相较于GPU，CPU的计算能力较弱，主要用于通用计算任务。虽然CPU也可以执行大语言模型的推理任务，但由于计算能力有限，推理速度通常会较慢。</li>
<li>使用GPU加速推理：为了充分利用GPU的计算能力，通常会使用深度学习框架提供的GPU加速功能，如CUDA或OpenCL。这些加速库可以将计算任务分配给GPU并利用其并行计算能力，从而加快大语言模型的推理速度。</li>
</ol>
<p>需要注意的是，推理速度还受到模型大小、输入数据大小、计算操作的复杂度以及硬件设备的性能等因素的影响。因此，具体的推理速度会因具体情况而异。一般来说，使用GPU进行大语言模型的推理可以获得更快的速度。</p>
</li>
<li><p><strong>推理速度上，int8和fp16比起来怎么样？</strong></p>
<p>在大语言模型的推理速度上，使用INT8（8位整数量化）和FP16（半精度浮点数）相对于FP32（单精度浮点数）可以带来一定的加速效果。这是因为INT8和FP16的数据类型在表示数据时所需的内存和计算资源较少，从而可以加快推理速度。</p>
<p>具体来说，INT8在相同的内存空间下可以存储更多的数据，从而可以在相同的计算资源下进行更多的并行计算。这可以提高每秒推理操作数（Operations Per Second，OPS）的数量，加速推理速度。</p>
<p>FP16在相对较小的数据范围内进行计算，因此在相同的计算资源下可以执行更多的计算操作。虽然FP16的精度相对较低，但对于某些应用场景，如图像处理和语音识别等，FP16的精度已经足够满足需求。</p>
<p>需要注意的是，INT8和FP16的加速效果可能会受到硬件设备的支持程度和具体实现的影响。某些硬件设备可能对INT8和FP16有更好的优化支持，从而进一步提高推理速度。</p>
<p>综上所述，使用INT8和FP16数据类型可以在大语言模型的推理过程中提高推理速度，但需要根据具体场景和硬件设备的支持情况进行评估和选择。</p>
</li>
<li><p><strong>大模型有推理能力吗？</strong></p>
<p>是的，大语言模型具备推理能力。推理是指在训练阶段之后，使用已经训练好的模型对新的输入数据进行预测、生成或分类等任务。大语言模型可以通过输入一段文本或问题，然后生成相应的回答或补全文本。</p>
<p>大语言模型通常基于循环神经网络（RNN）或变种（如长短时记忆网络LSTM或门控循环单元GRU）等结构构建，通过学习大量的文本数据，模型可以捕捉到语言的规律和模式。这使得大语言模型能够对输入的文本进行理解和推理，生成合理的回答或补全。</p>
<p>例如，GPT（Generative Pre-trained Transformer）模型是一种大型的预训练语言模型，它通过预训练的方式学习大规模的文本数据，然后可以在推理阶段生成连贯、合理的文本。这种模型可以用于自然语言处理任务，如文本生成、机器翻译、对话系统等。</p>
<p>需要注意的是，大语言模型的推理能力是基于其训练数据的统计规律和模式，因此在面对新颖、复杂或特殊的输入时，可能会出现推理错误或生成不准确的结果。此外，大语言模型的推理能力也受到模型的大小、训练数据的质量和数量、推理算法等因素的影响。</p>
</li>
<li><p><strong>大模型生成时的参数怎么设置？</strong></p>
<p>在大语言模型进行推理时，参数设置通常包括以下几个方面：</p>
<ol>
<li>模型选择：选择适合推理任务的模型，如循环神经网络（RNN）、长短时记忆网络（LSTM）、门控循环单元（GRU）或变种的Transformer等。不同的模型在推理任务上可能有不同的效果。</li>
<li>模型加载：加载预训练好的模型参数，这些参数可以是在大规模文本数据上进行预训练得到的。预训练模型的选择应根据任务和数据集的特点来确定。</li>
<li>推理算法：选择合适的推理算法，如贪婪搜索、束搜索（beam search）或采样方法等。贪婪搜索只考虑当前最有可能的输出，束搜索会考虑多个候选输出，采样方法会根据概率分布进行随机采样。</li>
<li>温度参数：在生成文本时，可以通过调整温度参数来控制生成的文本的多样性。较高的温度会增加生成文本的随机性和多样性，而较低的温度会使生成文本更加确定和一致。</li>
<li>推理长度：确定生成文本的长度限制，可以设置生成的最大长度或生成的最小长度等。</li>
<li>其他参数：根据具体任务和需求，可能还需要设置其他参数，如生成的起始文本、生成的批次大小等。</li>
</ol>
<p>以上参数设置需要根据具体任务和数据集的特点进行调整和优化。通常情况下，可以通过实验和调参来找到最佳的参数组合，以获得较好的推理效果。同时，还可以通过人工评估和自动评估指标来评估生成文本的质量和准确性，进一步优化参数设置。</p>
</li>
<li><p><strong>有哪些省内存的大语言模型训练&#x2F;微调&#x2F;推理方法？</strong></p>
<p>有一些方法可以帮助省内存的大语言模型训练、微调和推理，以下是一些常见的方法：</p>
<ol>
<li>参数共享（Parameter Sharing）：通过共享模型中的参数，可以减少内存占用。例如，可以在不同的位置共享相同的嵌入层或注意力机制。</li>
<li>梯度累积（Gradient Accumulation）：在训练过程中，将多个小批次的梯度累积起来，然后进行一次参数更新。这样可以减少每个小批次的内存需求，特别适用于GPU内存较小的情况。</li>
<li>梯度裁剪（Gradient Clipping）：通过限制梯度的大小，可以避免梯度爆炸的问题，从而减少内存使用。</li>
<li>分布式训练（Distributed Training）：将训练过程分布到多台机器或多个设备上，可以减少单个设备的内存占用。分布式训练还可以加速训练过程。</li>
<li>量化（Quantization）：将模型参数从高精度表示（如FP32）转换为低精度表示（如INT8或FP16），可以减少内存占用。量化方法可以通过减少参数位数或使用整数表示来实现。</li>
<li>剪枝（Pruning）：通过去除冗余或不重要的模型参数，可以减少模型的内存占用。剪枝方法可以根据参数的重要性进行选择，从而保持模型性能的同时减少内存需求。</li>
<li>蒸馏（Knowledge Distillation）：使用较小的模型（教师模型）来指导训练较大的模型（学生模型），可以从教师模型中提取知识，减少内存占用。</li>
<li>分块处理（Chunking）：将输入数据或模型分成较小的块进行处理，可以减少内存需求。例如，在推理过程中，可以将较长的输入序列分成多个较短的子序列进行处理。</li>
</ol>
<p>这些方法可以结合使用，根据具体场景和需求进行选择和调整。同时，不同的方法可能对不同的模型和任务有不同的效果，因此需要进行实验和评估。</p>
</li>
<li><p><strong>如何让大模型输出合规化</strong></p>
<p>要让大模型输出合规化，可以采取以下方法：</p>
<ol>
<li>数据清理和预处理：在进行模型训练之前，对输入数据进行清理和预处理，以确保数据符合合规要求。这可能包括去除敏感信息、匿名化处理、数据脱敏等操作。</li>
<li>引入合规性约束：在模型训练过程中，可以引入合规性约束，以确保模型输出符合法律和道德要求。例如，可以在训练过程中使用合规性指标或损失函数来约束模型的输出。</li>
<li>限制模型访问权限：对于一些特定的应用场景，可以通过限制模型的访问权限来确保输出的合规性。只允许授权用户或特定角色访问模型，以保护敏感信息和确保合规性。</li>
<li>解释模型决策过程：为了满足合规性要求，可以对模型的决策过程进行解释和解释。通过提供透明的解释，可以使用户或相关方了解模型是如何做出决策的，并评估决策的合规性。</li>
<li>审查和验证模型：在模型训练和部署之前，进行审查和验证以确保模型的输出符合合规要求。这可能涉及到法律专业人士、伦理专家或相关领域的专业人士的参与。</li>
<li>监控和更新模型：持续监控模型的输出，并根据合规要求进行必要的更新和调整。及时发现和解决合规性问题，确保模型的输出一直保持合规。</li>
<li>合规培训和教育：为使用模型的人员提供合规培训和教育，使其了解合规要求，并正确使用模型以确保合规性。</li>
</ol>
<p>需要注意的是，合规性要求因特定领域、应用和地区而异，因此在实施上述方法时，需要根据具体情况进行调整和定制。同时，合规性是一个动态的过程，需要与法律、伦理和社会要求的变化保持同步。</p>
</li>
<li><p><strong>应用模式变更</strong></p>
<p>大语言模型的应用模式变更可以包括以下几个方面：</p>
<ol>
<li>任务定制化：将大语言模型应用于特定的任务或领域，通过对模型进行微调或迁移学习，使其适应特定的应用场景。例如，将大语言模型用于自动文本摘要、机器翻译、对话系统等任务。</li>
<li>个性化交互：将大语言模型应用于个性化交互，通过对用户输入进行理解和生成相应的回复，实现更自然、智能的对话体验。这可以应用于智能助手、在线客服、社交媒体等场景。</li>
<li>内容生成与创作：利用大语言模型的生成能力，将其应用于内容生成和创作领域。例如，自动生成新闻报道、创意文案、诗歌等内容，提供创作灵感和辅助创作过程。</li>
<li>情感分析与情绪识别：通过大语言模型对文本进行情感分析和情绪识别，帮助企业或个人了解用户的情感需求和反馈，以改善产品、服务和用户体验。</li>
<li>知识图谱构建：利用大语言模型的文本理解能力，将其应用于知识图谱的构建和更新。通过对海量文本进行分析和提取，生成结构化的知识表示，为知识图谱的建设提供支持。</li>
<li>法律和合规应用：大语言模型可以用于法律和合规领域，例如自动生成法律文件、合同条款、隐私政策等内容，辅助法律专业人士的工作。</li>
<li>教育和培训应用：将大语言模型应用于教育和培训领域，例如智能辅导系统、在线学习平台等，为学生提供个性化的学习辅助和教学资源。</li>
<li>创新应用场景：探索和创造全新的应用场景，结合大语言模型的能力和创新思维，开拓新的商业模式和服务方式。例如，结合增强现实技术，实现智能导览和语音交互；结合虚拟现实技术，创建沉浸式的交互体验等。</li>
</ol>
<p>应用模式变更需要充分考虑数据安全、用户隐私、道德和法律等因素，确保在合规和可持续发展的前提下进行应用创新。同时，与领域专家和用户进行密切合作，不断优化和改进应用模式，以满足用户需求和市场竞争。</p>
</li>
<li><p><strong>SFT（有监督微调）的数据集格式？</strong></p>
<p>对于大语言模型的训练中，SFT（Supervised Fine-Tuning）的数据集格式可以采用以下方式：</p>
<ol>
<li>输入数据：输入数据是一个文本序列，通常是一个句子或者一个段落。每个样本可以是一个字符串或者是一个tokenized的文本序列。</li>
<li>标签数据：标签数据是与输入数据对应的标签或类别。标签可以是单个类别，也可以是多个类别的集合。对于多分类任务，通常使用one-hot编码或整数编码来表示标签。</li>
<li>数据集划分：数据集通常需要划分为训练集、验证集和测试集。训练集用于模型的训练，验证集用于调整模型的超参数和监控模型的性能，测试集用于评估模型的最终性能。</li>
<li>数据集格式：数据集可以以文本文件（如CSV、JSON等）或数据库的形式存储。每个样本包含输入数据和对应的标签。可以使用表格形式存储数据，每一列代表一个特征或标签。</li>
</ol>
<p>下面是一个示例数据集的格式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input,Label</span><br><span class="line">&quot;This is a sentence.&quot;,1</span><br><span class="line">&quot;Another sentence.&quot;,0</span><br><span class="line">...</span><br></pre></td></tr></table></figure>



<p>在这个示例中，输入数据是一个句子，标签是一个二分类的标签（1代表正例，0代表负例）。每一行代表一个样本，第一列是输入数据，第二列是对应的标签。</p>
<p>需要注意的是，具体的数据集格式可能会因任务类型、数据来源和使用的深度学习框架而有所不同。因此，在进行SFT训练时，建议根据具体任务和框架的要求来定义和处理数据集格式。</p>
</li>
<li><p><strong>RM（奖励模型）的数据格式？</strong></p>
<p>在大语言模型训练中，RM（Reward Model，奖励模型）的数据格式可以采用以下方式：</p>
<ol>
<li>输入数据：输入数据是一个文本序列，通常是一个句子或者一个段落。每个样本可以是一个字符串或者是一个tokenized的文本序列。</li>
<li>奖励数据：奖励数据是与输入数据对应的奖励或评分。奖励可以是一个实数值，表示对输入数据的评价。也可以是一个离散的标签，表示对输入数据的分类。奖励数据可以是人工标注的，也可以是通过其他方式（如人工评估、强化学习等）得到的。</li>
<li>数据集格式：数据集可以以文本文件（如CSV、JSON等）或数据库的形式存储。每个样本包含输入数据和对应的奖励数据。可以使用表格形式存储数据，每一列代表一个特征或标签。</li>
</ol>
<p>下面是一个示例数据集的格式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input,Reward</span><br><span class="line">&quot;This is a sentence.&quot;,0.8</span><br><span class="line">&quot;Another sentence.&quot;,0.2</span><br><span class="line">...</span><br></pre></td></tr></table></figure>



<p>在这个示例中，输入数据是一个句子，奖励数据是一个实数值，表示对输入数据的评价。每一行代表一个样本，第一列是输入数据，第二列是对应的奖励数据。</p>
<p>需要注意的是，具体的数据集格式可能会因任务类型、数据来源和使用的深度学习框架而有所不同。因此，在使用RM进行大语言模型训练时，建议根据具体任务和框架的要求来定义和处理数据集格式。</p>
</li>
<li><p><strong>PPO（强化学习）的数据格式？</strong></p>
<p>在大语言模型训练中，PPO（Proximal Policy Optimization，近端策略优化）是一种常用的强化学习算法。PPO的数据格式可以采用以下方式：</p>
<ol>
<li>输入数据：输入数据是一个文本序列，通常是一个句子或者一个段落。每个样本可以是一个字符串或者是一个tokenized的文本序列。</li>
<li>奖励数据：奖励数据是与输入数据对应的奖励或评分。奖励可以是一个实数值，表示对输入数据的评价。也可以是一个离散的标签，表示对输入数据的分类。奖励数据可以是人工标注的，也可以是通过其他方式（如人工评估、模型评估等）得到的。</li>
<li>动作数据：动作数据是模型在给定输入数据下的输出动作。对于语言模型，动作通常是生成的文本序列。动作数据可以是一个字符串或者是一个tokenized的文本序列。</li>
<li>状态数据：状态数据是模型在给定输入数据和动作数据下的状态信息。对于语言模型，状态数据可以是模型的隐藏状态或其他中间表示。状态数据的具体形式可以根据具体任务和模型结构进行定义。</li>
<li>数据集格式：数据集可以以文本文件（如CSV、JSON等）或数据库的形式存储。每个样本包含输入数据、奖励数据、动作数据和状态数据。可以使用表格形式存储数据，每一列代表一个特征或标签。</li>
</ol>
<p>下面是一个示例数据集的格式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input,Reward,Action,State</span><br><span class="line">&quot;This is a sentence.&quot;,0.8,&quot;This is a generated sentence.&quot;,[0.1, 0.2, 0.3, ...]</span><br><span class="line">&quot;Another sentence.&quot;,0.2,&quot;Another generated sentence.&quot;,[0.4, 0.5, 0.6, ...]</span><br><span class="line">...</span><br></pre></td></tr></table></figure>



<p>在这个示例中，输入数据是一个句子，奖励数据是一个实数值，动作数据是生成的句子，状态数据是模型的隐藏状态。每一行代表一个样本，第一列是输入数据，第二列是对应的奖励数据，第三列是生成的动作数据，第四列是状态数据。</p>
<p>需要注意的是，具体的数据集格式可能会因任务类型、数据来源和使用的深度学习框架而有所不同。因此，在使用PPO进行大语言模型训练时，建议根据具体任务和框架的要求来定义和处理数据集格式。</p>
</li>
<li><p><strong>找数据集哪里找？</strong></p>
<p>在训练自己的大语言模型时，可以从以下几个途径找到合适的数据集：</p>
<ol>
<li>公开数据集：有许多公开可用的数据集可供使用，涵盖了各种领域和任务。例如，Common Crawl、Wikipedia、OpenWebText、BookCorpus等都是常用的大规模文本数据集，可以用于语言模型的训练。</li>
<li>开放数据平台：许多组织和机构提供了开放的数据平台，可以获取各种类型的数据。例如，Kaggle、UCI Machine Learning Repository、Google Dataset Search等平台都提供了丰富的数据集资源。</li>
<li>学术界研究：许多学术研究项目会公开其使用的数据集，可以通过相关论文或项目页面找到这些数据集。例如，NLP领域的一些会议和竞赛（如ACL、EMNLP、CoNLL、GLUE等）提供了公开的数据集供研究使用。</li>
<li>数据收集和爬取：如果没有合适的公开数据集，您可以自己进行数据收集和爬取。这可以通过爬虫技术从互联网上收集相关的文本数据。需要注意的是，在进行数据收集和爬取时，需要遵守法律法规和网站的使用条款，并确保获得数据的合法使用权。</li>
<li>数据增强：如果您已经有了一些初始的数据集，但觉得数量不够，可以考虑使用数据增强技术来扩充数据。数据增强可以通过对原始数据进行一些变换、替换、合成等操作来生成新的样本。</li>
</ol>
<p>无论从哪个途径获取数据集，都需要注意数据的质量、版权和隐私等问题。确保您有合法的使用权，并遵守相关的法律和伦理规范。</p>
</li>
<li><p><strong>微调需要多少条数据？</strong></p>
<p>在大语言模型训练中，微调所需的数据量可以有很大的变化，取决于多个因素，包括模型的规模、任务的复杂性和数据的多样性等。以下是一些常见的微调数据量的指导原则：</p>
<ol>
<li>小规模模型：对于小规模的语言模型，通常需要较少的数据量进行微调。一般来说，几千到几万条数据可能已经足够。这些数据可以包括人工标注的数据、从其他来源收集的数据或者通过数据增强技术生成的数据。</li>
<li>大规模模型：对于大规模的语言模型，通常需要更多的数据量进行微调。数十万到数百万条数据可能是常见的范围。大规模模型的训练需要更多的数据来覆盖更广泛的语言知识和模式。</li>
<li>数据多样性：数据的多样性也是微调所需数据量的一个重要因素。如果任务的数据分布与微调数据不匹配，可能需要更多的数据来进行微调。例如，如果微调的任务是生成新闻标题，但微调数据主要是社交媒体的文本，可能需要更多的数据来覆盖新闻领域的语言模式。</li>
</ol>
<p>需要注意的是，以上只是一些常见的指导原则，并不是绝对的规则。实际上，微调所需的数据量是一个经验性问题，需要根据具体任务、模型和数据情况进行调整。可以通过实验和验证来确定合适的数据量，以达到预期的性能和效果。</p>
</li>
<li><p><strong>有哪些大模型的训练集？</strong></p>
<p>以下是一些常用的大语言模型训练集的示例：</p>
<ol>
<li>Common Crawl：这是一个由互联网上抓取的大规模文本数据集，包含了来自各种网站的文本内容。它是一个常用的数据集，可用于语言模型的训练。</li>
<li>Wikipedia：维基百科是一个包含大量结构化文本的在线百科全书。维基百科的内容丰富多样，涵盖了各种领域的知识，可以作为语言模型训练的数据集。</li>
<li>OpenWebText：这是一个从互联网上抓取的开放文本数据集，类似于Common Crawl。它包含了大量的网页文本，可以作为语言模型的训练数据。</li>
<li>BookCorpus：这是一个包含了大量图书文本的数据集，用于语言模型的训练。它包括了各种类型的图书，涵盖了广泛的主题和领域。</li>
<li>News articles：新闻文章是另一个常用的语言模型训练集。可以通过从新闻网站、新闻API或新闻数据库中收集新闻文章来构建训练集。</li>
<li>其他领域特定数据集：根据具体任务和应用，可以使用特定领域的数据集来训练语言模型。例如，在医学领域，可以使用医学文献或医疗记录作为训练数据；在法律领域，可以使用法律文书或法律条款作为训练数据。</li>
</ol>
<p>需要注意的是，使用这些数据集时，应该遵守数据的版权和使用规定，确保合法的使用权。此外，还可以通过数据增强技术，如数据合成、数据变换等，来扩充训练集的规模和多样性。</p>
</li>
<li><p><strong>进行领域大模型预训练应用哪些数据集比较好？</strong></p>
</li>
</ol>
<pre><code>进行领域大模型预训练时，可以使用以下几种数据集来获得更好的效果：

1. 领域特定文本数据集：收集与目标领域相关的文本数据集，例如专业领域的论文、报告、文档、书籍等。这些数据集可以提供领域内的专业术语、上下文和特定领域的知识。
2. 领域内的网页内容：从目标领域相关的网页抓取文本内容。可以通过爬虫技术从相关网站上获取与目标领域相关的网页文本数据。
3. 领域内的新闻文章：收集与目标领域相关的新闻文章。新闻文章通常包含了领域内的最新信息和事件，可以帮助模型了解领域内的动态和趋势。
4. 行业报告和白皮书：获取与目标领域相关的行业报告、白皮书和研究文献。这些文献通常包含了领域内的专业分析、统计数据和趋势预测，可以帮助模型了解行业背景和发展趋势。
5. 社交媒体数据：收集与目标领域相关的社交媒体数据，如推特、微博、论坛等。社交媒体上的内容通常反映了人们在目标领域中的讨论、观点和问题，可以帮助模型了解领域内的热点和用户需求。
6. 领域内的对话数据：获取与目标领域相关的对话数据，如客服对话、问答平台数据等。这些对话数据可以帮助模型学习领域内的常见问题、解决方案和用户需求。

在选择数据集时，应该确保数据的质量和合法性，并遵守相关的法律和伦理规范。同时，还可以考虑使用数据增强技术，如数据合成、数据变换等，来扩充训练集的规模和多样性。
</code></pre>
<ol start="43">
<li><p><strong>大模型怎么评测？</strong></p>
<p>大语言模型的评测通常涉及以下几个方面：</p>
<ol>
<li>语法和流畅度：评估模型生成的文本是否符合语法规则，并且是否流畅自然。这可以通过人工评估或自动评估指标如困惑度（perplexity）来衡量。</li>
<li>语义准确性：评估模型生成的文本是否准确传达了所需的含义，并且是否避免了歧义或模棱两可的表达。这需要通过人工评估来判断，通常需要领域专家的参与。</li>
<li>上下文一致性：评估模型在生成长篇文本时是否能够保持一致的上下文逻辑和连贯性。这需要通过人工评估来检查模型生成的文本是否与前文和后文相衔接。</li>
<li>信息准确性：评估模型生成的文本中所包含的信息是否准确和可靠。这可以通过人工评估或与已知信息进行对比来判断。</li>
<li>创造性和多样性：评估模型生成的文本是否具有创造性和多样性，是否能够提供不同的观点和表达方式。这需要通过人工评估来判断。</li>
</ol>
<p>评测大语言模型是一个复杂的过程，需要结合人工评估和自动评估指标来进行综合评价。由于大语言模型的规模和复杂性，评测结果往往需要多个评估者的共识，并且需要考虑到评估者的主观因素和评估标准的一致性。</p>
</li>
<li><p><strong>大模型的honest原则是如何实现的？</strong></p>
<p>大语言模型的”honest”原则是指模型在生成文本时应该保持诚实和真实，不应该编造虚假信息或误导用户。实现”honest”原则可以通过以下几种方式：</p>
<ol>
<li>数据训练：使用真实和可靠的数据进行模型的训练，确保模型学习到的知识和信息与真实世界相符。数据的来源和质量对于模型的”honest”性非常重要。</li>
<li>过滤和审查：在训练数据中，可以通过过滤和审查来排除不真实或不可靠的内容。这可以通过人工审核或自动筛选算法来实现，以确保训练数据的可信度。</li>
<li>监督和调整：对模型的生成结果进行监督和调整，及时发现和纠正可能的误导或虚假信息。这可以通过人工审核、用户反馈或者自动监测来实现。</li>
<li>透明度和解释性：提供模型生成文本的解释和可追溯性，使用户能够了解模型生成文本的依据和过程。这可以通过展示模型的输入数据、模型的结构和参数等方式来实现。</li>
<li>遵循道德和法律准则：确保模型的设计和使用符合道德和法律的准则，不违背伦理和法律规定。这需要在模型的开发和应用过程中考虑到社会和伦理的因素。</li>
</ol>
<p>需要注意的是，尽管大语言模型可以尽力遵循”honest”原则，但由于其是基于训练数据进行生成，仍然存在可能生成不准确或误导性的文本。因此，用户在使用大语言模型生成的文本时，仍需保持批判性思维，并结合其他信息和验证渠道进行判断。</p>
</li>
<li><p><strong>模型如何判断回答的知识是训练过的已知的知识，怎么训练这种能力？</strong></p>
</li>
</ol>
<pre><code>大语言模型判断回答的知识是否为训练过的已知知识，通常可以通过以下几种方式来实现：

1. 训练数据：在训练大语言模型时，可以使用包含已知知识的真实数据。这些数据可以来自于可靠的来源，如百科全书、学术文献等。通过训练模型时接触到这些知识，模型可以学习到一定的知识表示和模式。
2. 监督学习：可以使用人工标注的数据来进行监督学习，将已知知识标注为正确答案。在训练模型时，通过最大化与标注答案的匹配程度，模型可以学习到回答问题的知识表示和模式。
3. 开放域知识库：可以利用开放域知识库，如维基百科，作为额外的训练数据。通过将知识库中的信息与模型进行交互，模型可以学习到知识的表示和检索能力。
4. 过滤和筛选：在训练数据中，可以通过过滤和筛选来排除不准确或不可靠的信息。这可以通过人工审核或自动筛选算法来实现，以提高模型对已知知识的准确性。

训练这种能力需要充分的训练数据和有效的训练方法。同时，还需要进行模型的评估和调优，以确保模型能够正确理解和回答已知的知识问题。此外，定期更新训练数据和模型，以跟进新的知识和信息，也是保持模型知识更新和准确性的重要步骤。
</code></pre>
<ol start="46">
<li><p><strong>llama 输入句子长度理论上可以无限长吗？</strong></p>
<p>理论上来说，LLMs（大型语言模型）可以处理任意长度的输入句子，但实际上存在一些限制和挑战。下面是一些相关的考虑因素：</p>
<ol>
<li>计算资源：生成长句子需要更多的计算资源，包括内存和计算时间。由于LLMs通常是基于神经网络的模型，计算长句子可能会导致内存不足或计算时间过长的问题。</li>
<li>模型训练和推理：训练和推理长句子可能会面临一些挑战。在训练阶段，处理长句子可能会导致梯度消失或梯度爆炸的问题，影响模型的收敛性和训练效果。在推理阶段，生成长句子可能会增加模型的错误率和生成时间。</li>
<li>上下文建模：LLMs是基于上下文建模的模型，长句子的上下文可能会更加复杂和深层。模型需要能够捕捉长句子中的语义和语法结构，以生成准确和连贯的文本。</li>
</ol>
<p>尽管存在这些挑战，研究人员和工程师们已经在不断努力改进和优化LLMs，以处理更长的句子。例如，可以采用分块的方式处理长句子，将其分成多个较短的片段进行处理。此外，还可以通过增加计算资源、优化模型结构和参数设置，以及使用更高效的推理算法来提高LLMs处理长句子的能力。</p>
<p>值得注意的是，实际应用中，长句子的处理可能还受到应用场景、任务需求和资源限制等因素的影响。因此，在使用LLMs处理长句子时，需要综合考虑这些因素，并根据具体情况进行选择和调整。</p>
</li>
<li><p><strong>什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？</strong></p>
<p>选择使用哪种大模型，如Bert、LLaMA或ChatGLM，取决于具体的应用场景和需求。下面是一些指导原则：</p>
<ol>
<li>Bert模型：Bert是一种预训练的语言模型，适用于各种自然语言处理任务，如文本分类、命名实体识别、语义相似度计算等。如果你的任务是通用的文本处理任务，而不依赖于特定领域的知识或语言风格，Bert模型通常是一个不错的选择。Bert由一个Transformer编码器组成，更适合于NLU相关的任务。</li>
<li>LLaMA模型：LLaMA（Large Language Model Meta AI）包含从 7B 到 65B 的参数范围，训练使用多达14,000亿tokens语料，具有常识推理、问答、数学推理、代码生成、语言理解等能力。Bert由一个Transformer解码器组成。训练预料主要为以英语为主的拉丁语系，不包含中日韩文。所以适合于英文文本生成的任务。</li>
<li>ChatGLM模型：ChatGLM是一个面向对话生成的语言模型，适用于构建聊天机器人、智能客服等对话系统。如果你的应用场景需要模型能够生成连贯、流畅的对话回复，并且需要处理对话上下文、生成多轮对话等，ChatGLM模型可能是一个较好的选择。ChatGLM的架构为Prefix decoder，训练语料为中英双语，中英文比例为1:1。所以适合于中文和英文文本生成的任务。</li>
</ol>
<p>在选择模型时，还需要考虑以下因素：</p>
<ul>
<li>数据可用性：不同模型可能需要不同类型和规模的数据进行训练。确保你有足够的数据来训练和微调所选择的模型。</li>
<li>计算资源：大模型通常需要更多的计算资源和存储空间。确保你有足够的硬件资源来支持所选择的模型的训练和推理。</li>
<li>预训练和微调：大模型通常需要进行预训练和微调才能适应特定任务和领域。了解所选择模型的预训练和微调过程，并确保你有相应的数据和时间来完成这些步骤。</li>
</ul>
<p>最佳选择取决于具体的应用需求和限制条件。在做出决策之前，建议先进行一些实验和评估，以确定哪种模型最适合你的应用场景。</p>
</li>
<li><p><strong>各个专业领域是否需要各自的大模型来服务？</strong></p>
<p>各个专业领域通常需要各自的大模型来服务，原因如下：</p>
<ol>
<li>领域特定知识：不同领域拥有各自特定的知识和术语，需要针对该领域进行训练的大模型才能更好地理解和处理相关文本。例如，在医学领域，需要训练具有医学知识的大模型，以更准确地理解和生成医学文本。</li>
<li>语言风格和惯用语：各个领域通常有自己独特的语言风格和惯用语，这些特点对于模型的训练和生成都很重要。专门针对某个领域进行训练的大模型可以更好地掌握该领域的语言特点，生成更符合该领域要求的文本。</li>
<li>领域需求的差异：不同领域对于文本处理的需求也有所差异。例如，金融领域可能更关注数字和统计数据的处理，而法律领域可能更关注法律条款和案例的解析。因此，为了更好地满足不同领域的需求，需要专门针对各个领域进行训练的大模型。</li>
<li>数据稀缺性：某些领域的数据可能相对较少，无法充分训练通用的大模型。针对特定领域进行训练的大模型可以更好地利用该领域的数据，提高模型的性能和效果。</li>
</ol>
<p>尽管需要各自的大模型来服务不同领域，但也可以共享一些通用的模型和技术。例如，通用的大模型可以用于处理通用的文本任务，而领域特定的模型可以在通用模型的基础上进行微调和定制，以适应特定领域的需求。这样可以在满足领域需求的同时，减少模型的重复训练和资源消耗。</p>
</li>
<li><p><strong>如何让大模型处理更长的文本？</strong></p>
<p>要让大模型处理更长的文本，可以考虑以下几个方法：</p>
<ol>
<li>分块处理：将长文本分割成较短的片段，然后逐个片段输入模型进行处理。这样可以避免长文本对模型内存和计算资源的压力。在处理分块文本时，可以使用重叠的方式，即将相邻片段的一部分重叠，以保持上下文的连贯性。</li>
<li>层次建模：通过引入层次结构，将长文本划分为更小的单元。例如，可以将文本分为段落、句子或子句等层次，然后逐层输入模型进行处理。这样可以减少每个单元的长度，提高模型处理长文本的能力。</li>
<li>部分生成：如果只需要模型生成文本的一部分，而不是整个文本，可以只输入部分文本作为上下文，然后让模型生成所需的部分。例如，输入前一部分文本，让模型生成后续的内容。</li>
<li>注意力机制：注意力机制可以帮助模型关注输入中的重要部分，可以用于处理长文本时的上下文建模。通过引入注意力机制，模型可以更好地捕捉长文本中的关键信息。</li>
<li>模型结构优化：通过优化模型结构和参数设置，可以提高模型处理长文本的能力。例如，可以增加模型的层数或参数量，以增加模型的表达能力。还可以使用更高效的模型架构，如Transformer等，以提高长文本的处理效率。</li>
</ol>
<p>需要注意的是，处理长文本时还需考虑计算资源和时间的限制。较长的文本可能需要更多的内存和计算时间，因此在实际应用中需要根据具体情况进行权衡和调整。</p>
</li>
<li><p><strong>奖励模型需要和基础模型一致吗？</strong></p>
</li>
</ol>
<pre><code>奖励模型和基础模型在训练过程中可以是一致的，也可以是不同的。这取决于你的任务需求和优化目标。

如果你希望优化一个包含多个子任务的复杂任务，那么你可能需要为每个子任务定义一个奖励模型，然后将这些奖励模型整合到一个统一的奖励函数中。这样，你可以根据任务的具体情况调整每个子任务的权重，以实现更好的性能。

另一方面，如果你的任务是单任务的，那么你可能只需要一个基础模型和一个对应的奖励模型，这两个模型可以共享相同的参数。在这种情况下，你可以通过调整奖励模型的权重来控制任务的优化方向。

总之，奖励模型和基础模型的一致性取决于你的任务需求和优化目标。在实践中，你可能需要尝试不同的模型结构和奖励函数，以找到最适合你任务的解决方案。
</code></pre>
<ol start="51">
<li><p><strong>RLHF 在实践过程中存在哪些不足？</strong></p>
<p>RLHF（Reinforcement Learning from Human Feedback）是一种通过人类反馈进行增强学习的方法，尽管具有一定的优势，但在实践过程中仍然存在以下几个不足之处：</p>
<ol>
<li>人类反馈的代价高昂：获取高质量的人类反馈通常需要大量的人力和时间成本。人类专家需要花费时间来评估模型的行为并提供准确的反馈，这可能限制了RLHF方法的可扩展性和应用范围。</li>
<li>人类反馈的主观性：人类反馈往往是主观的，不同的专家可能会有不同的意见和判断。这可能导致模型在不同专家之间的反馈上存在差异，从而影响模型的训练和性能。</li>
<li>反馈延迟和稀疏性：获取人类反馈可能存在延迟和稀疏性的问题。人类专家不可能实时监控和评估模型的每一个动作，因此模型可能需要等待一段时间才能收到反馈，这可能会导致训练的效率和效果下降。</li>
<li>错误反馈的影响：人类反馈可能存在错误或误导性的情况，这可能会对模型的训练产生负面影响。如果模型在错误的反馈指导下进行训练，可能会导致模型产生错误的行为策略。</li>
<li>缺乏探索与利用的平衡：在RLHF中，人类反馈通常用于指导模型的行为，但可能会导致模型过于依赖人类反馈而缺乏探索的能力。这可能限制了模型发现新策略和优化性能的能力。</li>
</ol>
<p>针对这些不足，研究人员正在探索改进RLHF方法，如设计更高效的人类反馈收集机制、开发更准确的反馈评估方法、结合自适应探索策略等，以提高RLHF方法的实用性和性能。</p>
</li>
<li><p><strong>奖励模型需要和基础模型一致吗？</strong></p>
<p>奖励模型和基础模型在训练过程中可以是一致的，也可以是不同的。这取决于你的任务需求和优化目标。</p>
<p>如果你希望优化一个包含多个子任务的复杂任务，那么你可能需要为每个子任务定义一个奖励模型，然后将这些奖励模型整合到一个统一的奖励函数中。这样，你可以根据任务的具体情况调整每个子任务的权重，以实现更好的性能。</p>
<p>另一方面，如果你的任务是单任务的，那么你可能只需要一个基础模型和一个对应的奖励模型，这两个模型可以共享相同的参数。在这种情况下，你可以通过调整奖励模型的权重来控制任务的优化方向。</p>
<p>总之，奖励模型和基础模型的一致性取决于你的任务需求和优化目标。在实践中，你可能需要尝试不同的模型结构和奖励函数，以找到最适合你任务的解决方案。</p>
</li>
<li><p><strong>RLHF 在实践过程中存在哪些不足？</strong></p>
<p>RLHF（Reinforcement Learning from Human Feedback）是一种通过人类反馈进行增强学习的方法，尽管具有一定的优势，但在实践过程中仍然存在以下几个不足之处：</p>
<ol>
<li>人类反馈的代价高昂：获取高质量的人类反馈通常需要大量的人力和时间成本。人类专家需要花费时间来评估模型的行为并提供准确的反馈，这可能限制了RLHF方法的可扩展性和应用范围。</li>
<li>人类反馈的主观性：人类反馈往往是主观的，不同的专家可能会有不同的意见和判断。这可能导致模型在不同专家之间的反馈上存在差异，从而影响模型的训练和性能。</li>
<li>反馈延迟和稀疏性：获取人类反馈可能存在延迟和稀疏性的问题。人类专家不可能实时监控和评估模型的每一个动作，因此模型可能需要等待一段时间才能收到反馈，这可能会导致训练的效率和效果下降。</li>
<li>错误反馈的影响：人类反馈可能存在错误或误导性的情况，这可能会对模型的训练产生负面影响。如果模型在错误的反馈指导下进行训练，可能会导致模型产生错误的行为策略。</li>
<li>缺乏探索与利用的平衡：在RLHF中，人类反馈通常用于指导模型的行为，但可能会导致模型过于依赖人类反馈而缺乏探索的能力。这可能限制了模型发现新策略和优化性能的能力。</li>
</ol>
<p>针对这些不足，研究人员正在探索改进RLHF方法，如设计更高效的人类反馈收集机制、开发更准确的反馈评估方法、结合自适应探索策略等，以提高RLHF方法的实用性和性能。</p>
</li>
<li><p><strong>如何解决 人工产生的偏好数据集成本较高，很难量产问题？</strong></p>
<p>解决人工产生偏好数据集成本高、难以量产的问题，可以考虑以下几种方法：</p>
<ol>
<li>引入模拟数据：使用模拟数据来代替或辅助人工产生的数据。模拟数据可以通过模拟环境或模型生成，以模拟人类用户的行为和反馈。这样可以降低数据收集的成本和难度，并且可以大规模生成数据。</li>
<li>主动学习：采用主动学习的方法来优化数据收集过程。主动学习是一种主动选择样本的方法，通过选择那些对模型训练最有帮助的样本进行标注，从而减少标注的工作量。可以使用一些算法，如不确定性采样、多样性采样等，来选择最有价值的样本进行人工标注。</li>
<li>在线学习：采用在线学习的方法进行模型训练。在线学习是一种增量学习的方法，可以在模型运行的同时进行训练和优化。这样可以利用实际用户的交互数据来不断改进模型，减少对人工标注数据的依赖。</li>
<li>众包和协作：利用众包平台或协作机制来收集人工产生的偏好数据。通过将任务分发给多个人参与，可以降低每个人的负担，并且可以通过众包平台的规模效应来提高数据收集的效率。</li>
<li>数据增强和迁移学习：通过数据增强技术，如数据合成、数据扩增等，来扩充有限的人工产生数据集。此外，可以利用迁移学习的方法，将从其他相关任务或领域收集的数据应用于当前任务，以减少对人工产生数据的需求。</li>
</ol>
<p>综合运用上述方法，可以有效降低人工产生偏好数据的成本，提高数据的量产能力，并且保证数据的质量和多样性。</p>
</li>
<li><p><strong>如何解决三个阶段的训练（SFT-&gt;RM-&gt;PPO）过程较长，更新迭代较慢问题？</strong></p>
<p>要解决三个阶段训练过程较长、更新迭代较慢的问题，可以考虑以下几种方法：</p>
<ol>
<li>并行化训练：利用多个计算资源进行并行化训练，可以加速整个训练过程。可以通过使用多个CPU核心或GPU来并行处理不同的训练任务，从而提高训练的效率和速度。</li>
<li>分布式训练：将训练任务分发到多台机器或多个节点上进行分布式训练。通过将模型和数据分布在多个节点上，并进行并行计算和通信，可以加快训练的速度和更新的迭代。</li>
<li>优化算法改进：针对每个阶段的训练过程，可以考虑改进优化算法来加速更新迭代。例如，在SFT（Supervised Fine-Tuning）阶段，可以使用更高效的优化算法，如自适应学习率方法（Adaptive Learning Rate）或者剪枝技术来减少模型参数；在RM（Reward Modeling）阶段，可以使用更快速的模型训练算法，如快速梯度法（Fast Gradient Method）等；在PPO（Proximal Policy Optimization）阶段，可以考虑使用更高效的采样和优化方法，如并行采样、多步采样等。</li>
<li>迁移学习和预训练：利用迁移学习和预训练技术，可以利用已有的模型或数据进行初始化或预训练，从而加速训练过程。通过将已有模型的参数或特征迁移到目标模型中，可以减少目标模型的训练时间和样本需求。</li>
<li>参数调优和超参数搜索：对于每个阶段的训练过程，可以进行参数调优和超参数搜索，以找到更好的参数设置和配置。通过系统地尝试不同的参数组合和算法设定，可以找到更快速和高效的训练方式。</li>
</ol>
<p>综合运用上述方法，可以加速三个阶段训练过程，提高更新迭代的速度和效率，从而减少训练时间和资源消耗。</p>
</li>
<li><p><strong>如何解决 PPO 的训练过程同时存在4个模型（2训练，2推理），对计算资源的要求较高 问题？</strong></p>
<p>要解决PPO训练过程中对计算资源要求较高的问题，可以考虑以下几种方法：</p>
<ol>
<li>减少模型规模：通过减少模型的规模和参数量，可以降低对计算资源的需求。可以使用模型压缩技术、剪枝算法等方法来减少模型的参数数量，从而降低计算资源的使用量。</li>
<li>降低训练频率：可以降低PPO训练的频率，减少每个训练周期的次数。例如，可以增加每个训练周期的时间间隔，或者减少每个周期中的训练步数。这样可以减少训练过程中对计算资源的占用。</li>
<li>模型并行化：利用多个计算资源进行模型并行化训练，可以加速PPO的训练过程。可以将模型参数分布到多个GPU上，并进行并行计算和通信，以提高训练的效率和速度。</li>
<li>异步训练：采用异步训练的方式，可以在多个计算资源上同时进行PPO的训练。可以使用异步优化算法，如A3C（Asynchronous Advantage Actor-Critic）等，将训练任务分发到多个线程或进程中进行并行训练，从而提高训练的效率。</li>
<li>云计算和分布式训练：利用云计算平台或分布式系统进行PPO的训练，可以充分利用大规模计算资源。可以将训练任务分发到多个计算节点上进行分布式训练，以加速训练过程。</li>
<li>参数共享和模型缓存：对于有多个模型的情况，可以考虑共享部分参数或缓存已计算的模型输出。通过共享参数和缓存计算结果，可以减少重复计算和存储，从而降低对计算资源的要求。</li>
</ol>
<p>综合运用上述方法，可以有效降低PPO训练过程中对计算资源的要求，提高训练的效率和速度。</p>
</li>
<li><p>Layer Norm 的计算公式写一下？</p>
<p>Layer Norm（层归一化）是一种用于神经网络中的归一化技术，用于提高模型的训练效果和泛化能力。其计算公式如下：</p>
<p>给定输入 x，其维度为 (batch size,seq length,hidden size)，Layer Norm 的计算公式为：</p>
<p>LayerNorm(x)&#x3D;aσ⊙(x−μ)+b</p>
<p>其中， μ 是 x 沿最后一个维度的均值， σ 是 x 沿最后一个维度的标准差， a 和 b 是可学习的缩放因子和偏置项。$\odot$ 表示逐元素相乘。</p>
<p>具体计算过程如下：</p>
<ol>
<li>计算均值 μ： μ&#x3D;mean(x,axis&#x3D;−1,keepdims&#x3D;True)</li>
<li>计算标准差 σ： σ&#x3D;std(x,axis&#x3D;−1,keepdims&#x3D;True)</li>
<li>计算归一化的 x： xnorm&#x3D;x−μσ</li>
<li>计算缩放因子和偏置项： y&#x3D;a⊙xnorm+b</li>
</ol>
<p>其中， a 和 b 是可学习的参数，可以通过反向传播进行训练。</p>
<p>Layer Norm 的作用是将每个样本的特征进行归一化，使得特征在不同样本之间具有相似的分布，有助于提高模型的训练效果和泛化能力。</p>
</li>
<li><p>RMS Norm 的计算公式写一下？</p>
<p>RMS Norm（均方根归一化）是一种用于神经网络中的归一化技术，用于提高模型的训练效果和泛化能力。其计算公式如下：</p>
<p>给定输入 x，其维度为 (batch size,seq length,hidden size)，RMS Norm 的计算公式为：<br>$$<br>\text{RMSNorm}(x) &#x3D; \frac{x}{\sqrt{\text{mean}(x^2, \text{axis}&#x3D;-1, \text{keepdims&#x3D;True}) + \epsilon}}<br>$$<br>其中， ϵ 是一个小的常数，用于避免分母为零。</p>
<p>具体计算过程如下：</p>
<ol>
<li>计算 x 的平方： x^2&#x3D;x⊙x</li>
<li>计算平方的均值： mean(x^2)&#x3D;mean(x^2,axis&#x3D;−1,keepdims&#x3D;True)</li>
<li>计算归一化的 x： xnorm&#x3D;xmean(x^2)+ϵ</li>
</ol>
<p>RMS Norm 的作用是通过计算输入 x 的均方根，将每个样本的特征进行归一化，使得特征在不同样本之间具有相似的尺度，有助于提高模型的训练效果和泛化能力。</p>
</li>
<li><p>RMS Norm 相比于 Layer Norm 有什么特点？</p>
<p>RMS Norm（Root Mean Square Norm）和 Layer Norm 是两种常用的归一化方法，它们在实现上有一些不同之处。</p>
<ol>
<li>计算方式：RMS Norm 是通过计算输入数据的平方均值的平方根来进行归一化，而 Layer Norm 是通过计算输入数据在每个样本中的平均值和方差来进行归一化。</li>
<li>归一化范围：RMS Norm 是对整个输入数据进行归一化，而 Layer Norm 是对每个样本进行归一化。</li>
<li>归一化位置：RMS Norm 通常应用于循环神经网络（RNN）中的隐藏状态，而 Layer Norm 通常应用于卷积神经网络（CNN）或全连接层中。</li>
<li>归一化效果：RMS Norm 在处理长序列数据时可能会出现梯度消失或梯度爆炸的问题，而 Layer Norm 能够更好地处理这些问题。</li>
</ol>
<p>综上所述，RMS Norm 和 Layer Norm 在计算方式、归一化范围、归一化位置和归一化效果等方面存在一些差异，适用于不同的神经网络结构和任务。选择哪种归一化方法应根据具体情况进行评估和选择。</p>
</li>
<li><p>Deep Norm 思路？</p>
<p>Deep Norm 是一种基于归一化的深度学习模型优化方法，其思路是通过在深度神经网络中引入多层归一化操作，以改善模型的训练和泛化性能。</p>
<p>Deep Norm 的主要思想是在网络的每一层之间插入归一化层，以减小输入数据的分布差异，从而加速收敛并提高模型的泛化能力。与传统的批归一化（Batch Normalization）不同，Deep Norm 在每一层都进行归一化，而不是仅在特定层进行。</p>
<p>Deep Norm 的具体步骤如下：</p>
<ol>
<li>输入数据：将输入数据传递给网络的第一层。</li>
<li>归一化层：在网络的每一层之间插入归一化层。归一化层的作用是将每层的输入数据进行归一化，使其均值为0，方差为1。这可以减小数据的分布差异，有助于提高模型的稳定性和泛化性能。</li>
<li>激活函数：在归一化层之后应用激活函数，以引入非线性变换。</li>
<li>下一层：将经过归一化和激活函数处理的数据传递给网络的下一层。</li>
</ol>
<p>通过在每一层引入归一化操作，Deep Norm 可以有效地解决深度神经网络中的梯度消失和梯度爆炸问题，并提高模型的收敛速度和泛化性能。此外，Deep Norm 还可以减少对学习率的敏感性，使得模型更容易优化。</p>
<p>需要注意的是，Deep Norm 需要在训练过程中对每一层的均值和方差进行估计，可以使用滑动平均等方法来更新归一化层的参数。在测试阶段，可以使用训练阶段估计的均值和方差进行归一化。</p>
<p>总而言之，Deep Norm 是一种通过在深度神经网络中引入多层归一化操作来优化模型的方法，可以改善模型的训练和泛化性能。</p>
</li>
<li><p>写一下 Deep Norm 代码实现？</p>
<p>Deep Norm 的代码实现可以基于 PyTorch 框架来完成。以下是一个简单的 Deep Norm 的代码示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">class DeepNorm(nn.Module):</span><br><span class="line">    def __init__(self, input_dim, hidden_dims, output_dim):</span><br><span class="line">        super(DeepNorm, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.layers = nn.ModuleList()</span><br><span class="line">        self.norm_layers = nn.ModuleList()</span><br><span class="line"></span><br><span class="line">        # 添加隐藏层和归一化层</span><br><span class="line">        for i, hidden_dim in enumerate(hidden_dims):</span><br><span class="line">            self.layers.append(nn.Linear(input_dim, hidden_dim))</span><br><span class="line">            self.norm_layers.append(nn.LayerNorm(hidden_dim))</span><br><span class="line">            input_dim = hidden_dim</span><br><span class="line"></span><br><span class="line">        # 添加输出层</span><br><span class="line">        self.output_layer = nn.Linear(input_dim, output_dim)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        for layer, norm_layer in zip(self.layers, self.norm_layers):</span><br><span class="line">            x = layer(x)</span><br><span class="line">            x = norm_layer(x)</span><br><span class="line">            x = torch.relu(x)</span><br><span class="line"></span><br><span class="line">        x = self.output_layer(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"># 创建一个 DeepNorm 模型实例</span><br><span class="line">input_dim = 100</span><br><span class="line">hidden_dims = [64, 32]</span><br><span class="line">output_dim = 10</span><br><span class="line">model = DeepNorm(input_dim, hidden_dims, output_dim)</span><br><span class="line"></span><br><span class="line"># 使用模型进行训练和预测</span><br><span class="line">input_data = torch.randn(32, input_dim)</span><br><span class="line">output = model(input_data)</span><br></pre></td></tr></table></figure>



<p>在这个示例中，我们定义了一个 DeepNorm 类，其中包含了多个隐藏层和归一化层。在 forward 方法中，我们依次对输入数据进行线性变换、归一化和激活函数处理，并通过输出层得到最终的预测结果。</p>
<p>需要注意的是，在实际使用中，可以根据具体任务的需求来调整模型的结构和参数设置。此外，还可以使用其他归一化方法，如 Layer Norm 或 Batch Norm，根据实际情况进行选择和实现。</p>
</li>
<li><p>Deep Norm 有什么优点？</p>
<p>Deep Norm 有以下几个优点：</p>
<ol>
<li>改善梯度传播：Deep Norm 在每一层都引入了归一化操作，可以有效地解决深度神经网络中的梯度消失和梯度爆炸问题。通过减小输入数据的分布差异，Deep Norm 可以使得梯度更加稳定，并加速模型的收敛速度。</li>
<li>提高泛化能力：Deep Norm 的归一化操作有助于提高模型的泛化能力。归一化可以减小数据的分布差异，使得模型更容易学习到数据的共性特征，从而提高模型对未见数据的预测能力。</li>
<li>减少对学习率的敏感性：Deep Norm 的归一化操作可以减少对学习率的敏感性。通过将输入数据归一化到相同的尺度，Deep Norm 可以使得模型的训练更加稳定，减少了对学习率的调整需求。</li>
<li>网络结构更简洁：Deep Norm 可以将归一化操作嵌入到网络的每一层中，而不需要额外的归一化层。这使得网络结构更加简洁，减少了模型参数的数量，降低了计算和存储成本。</li>
<li>提高模型的可解释性：Deep Norm 的归一化操作可以使得模型的输出具有更好的可解释性。通过将输入数据归一化到均值为0，方差为1的范围内，Deep Norm 可以使得模型输出的数值更易于理解和解释。</li>
</ol>
<p>综上所述，Deep Norm 通过引入多层归一化操作，可以改善梯度传播、提高泛化能力、减少对学习率的敏感性，同时还能简化网络结构和提高模型的可解释性。这些优点使得 Deep Norm 成为一种有效的深度学习模型优化方法。</p>
</li>
<li><p>LN 在 LLMs 中的不同位置 有什么区别么？如果有，能介绍一下区别么？</p>
<p> <strong>层归一化 Layer Norm 在 大语言模型 LLMs 中的不同位置 有什么区别么？如果有，能介绍一下区别么？</strong></p>
<p>在大语言模型（Large Language Models）中，Layer Norm（层归一化）可以应用在不同位置，包括输入层、输出层和中间隐藏层。这些位置的归一化有一些区别：</p>
<ol>
<li>输入层归一化：在输入层应用 Layer Norm 可以将输入的特征进行归一化，使得输入数据的分布更加稳定。这有助于减少不同样本之间的分布差异，提高模型的泛化能力。</li>
<li>输出层归一化：在输出层应用 Layer Norm 可以将输出结果进行归一化，使得输出结果的分布更加稳定。这有助于减小输出结果的方差，提高模型的稳定性和预测准确性。</li>
<li>中间隐藏层归一化：在中间隐藏层应用 Layer Norm 可以在每个隐藏层之间进行归一化操作，有助于解决深度神经网络中的梯度消失和梯度爆炸问题。通过减小输入数据的分布差异，Layer Norm 可以使得梯度更加稳定，并加速模型的收敛速度。</li>
</ol>
<p>总的来说，Layer Norm 在大语言模型中的不同位置应用可以解决不同的问题。输入层归一化可以提高模型的泛化能力，输出层归一化可以提高模型的稳定性和预测准确性，而中间隐藏层归一化可以改善梯度传播，加速模型的收敛速度。具体应用 Layer Norm 的位置需要根据具体任务和模型的需求进行选择。</p>
</li>
<li><p>LLMs 各模型分别用了 哪种 Layer normalization？</p>
<p>不同的大语言模型（LLMs）可能会使用不同的层归一化方法，以下是一些常见的层归一化方法在大语言模型中的应用：</p>
<ol>
<li>BERT（Bidirectional Encoder Representations from Transformers）：BERT使用的是Transformer中的层归一化方法，即在每个Transformer编码层中应用Layer Normalization。</li>
<li>GPT（Generative Pre-trained Transformer）：GPT系列模型通常使用的是GPT-Norm，它是一种变种的层归一化方法。GPT-Norm在每个Transformer解码层的每个子层（自注意力、前馈神经网络）之后应用Layer Normalization。</li>
<li>XLNet：XLNet使用的是两种不同的层归一化方法，即Token-wise层归一化和Segment-wise层归一化。Token-wise层归一化是在每个Transformer编码层中应用Layer Normalization，而Segment-wise层归一化是在每个Transformer解码层的自注意力机制之后应用Layer Normalization。</li>
<li>RoBERTa：RoBERTa是对BERT模型的改进，它也使用的是Transformer中的层归一化方法，即在每个Transformer编码层中应用Layer Normalization。</li>
</ol>
<p>需要注意的是，虽然这些大语言模型使用了不同的层归一化方法，但它们的目的都是为了提高模型的训练效果和泛化能力。具体选择哪种层归一化方法取决于模型的设计和任务的需求。</p>
</li>
<li><p>介绍一下 GeLU 计算公式？</p>
<p>GeLU（Gaussian Error Linear Unit）是一种激活函数，常用于神经网络中的非线性变换。它在Transformer模型中广泛应用于FFN（Feed-Forward Network）块。下面是GeLU的计算公式：</p>
<p>假设输入是一个标量 x，GeLU的计算公式如下：<br>$$<br>GeLU(x) &#x3D; 0.5 * x * (1 + tanh(sqrt(2 &#x2F; pi) * (x + 0.044715 * x^3)))<br>$$<br>GeLU函数的特点是在接近零的区域表现得类似于线性函数，而在远离零的区域则表现出非线性的特性。相比于其他常用的激活函数（如ReLU），GeLU函数在某些情况下能够提供更好的性能和更快的收敛速度。</p>
<p>需要注意的是，GeLU函数的计算复杂度较高，可能会增加模型的计算开销。因此，在实际应用中，也可以根据具体情况选择其他的激活函数来代替GeLU函数。</p>
</li>
<li><p>介绍一下 Swish 计算公式？</p>
<p>Swish是一种激活函数，它在深度学习中常用于神经网络的非线性变换。Swish函数的计算公式如下：</p>
<p>Swish(x) &#x3D; x * sigmoid(beta * x)</p>
<p>其中，sigmoid() 是Sigmoid函数，x 是输入，beta 是一个可调节的超参数。</p>
<p>Swish函数的特点是在接近零的区域表现得类似于线性函数，而在远离零的区域则表现出非线性的特性。相比于其他常用的激活函数（如ReLU、tanh等），Swish函数在某些情况下能够提供更好的性能和更快的收敛速度。</p>
<p>Swish函数的设计灵感来自于自动搜索算法，它通过引入一个可调节的超参数来增加非线性程度。当beta为0时，Swish函数退化为线性函数；当beta趋近于无穷大时，Swish函数趋近于ReLU函数。</p>
<p>需要注意的是，Swish函数相对于其他激活函数来说计算开销较大，因为它需要进行Sigmoid运算。因此，在实际应用中，也可以根据具体情况选择其他的激活函数来代替Swish函数。</p>
</li>
<li><p>介绍一下 使用 GeLU 的 GLU 块 计算公式？</p>
<p><strong>介绍一下 使用 GeLU 作为激活函数的 GLU 块 计算公式？</strong></p>
<p>使用GeLU作为激活函数的GLU块的计算公式如下：</p>
<p>GLU(x) &#x3D; x * GeLU(W_1 * x) （1）</p>
<p>其中，GeLU() 是Gaussian Error Linear Unit的激活函数，W_1 是一个可学习的权重矩阵。</p>
<p>在公式（1）中，首先将输入向量 x 通过一个全连接层（线性变换）得到一个与 x 维度相同的向量，然后将该向量作为输入传递给GeLU激活函数进行非线性变换。最后，将GeLU激活函数的输出与输入向量 x 逐元素相乘，得到最终的输出向量。</p>
<p>GeLU激活函数的计算公式如下：</p>
<p>GeLU(x) &#x3D; 0.5 * x * (1 + tanh(sqrt(2&#x2F;pi) * (x + 0.044715 * x^3))) （2）</p>
<p>其中，tanh() 是双曲正切函数，sqrt() 是平方根函数，pi 是圆周率。</p>
<p>在公式（2）中，GeLU函数首先对输入向量 x 进行一个非线性变换，然后通过一系列的数学运算得到最终的输出值。</p>
<p>使用GeLU作为GLU块的激活函数可以增强模型的非线性能力，并在某些情况下提供更好的性能和更快的收敛速度。这种结构常用于Transformer模型中的编码器和解码器，用于对输入向量进行非线性变换和特征提取。</p>
<p>需要注意的是，GLU块和GeLU激活函数是两个不同的概念，它们在计算公式和应用场景上有所区别。在实际应用中，可以根据具体情况选择合适的激活函数来代替GeLU或GLU。</p>
</li>
<li><p>介绍一下 使用 Swish 的 GLU 块 计算公式？</p>
<p>💡 <strong>介绍一下 使用 Swish 作为激活函数的 GLU 块 计算公式？</strong></p>
<p>使用Swish作为激活函数的GLU块的计算公式如下：</p>
<p>GLU(x) &#x3D; x * sigmoid(W_1 * x) （1）</p>
<p>其中，sigmoid() 是Sigmoid函数，W_1 是一个可学习的权重矩阵。</p>
<p>在公式（1）中，首先将输入向量 x 通过一个全连接层（线性变换）得到一个与 x 维度相同的向量，然后将该向量通过Sigmoid函数进行激活。这个Sigmoid函数的输出称为门控向量，用来控制输入向量 x 的元素是否被激活。最后，将门控向量与输入向量 x 逐元素相乘，得到最终的输出向量。</p>
<p>Swish激活函数的计算公式如下：</p>
<p>Swish(x) &#x3D; x * sigmoid(beta * x) （2）</p>
<p>其中，sigmoid() 是Sigmoid函数，beta 是一个可学习的参数。</p>
<p>在公式（2）中，Swish函数首先对输入向量 x 进行一个非线性变换，然后通过Sigmoid函数进行激活，并将该激活结果与输入向量 x 逐元素相乘，得到最终的输出值。</p>
<p>使用Swish作为GLU块的激活函数可以增强模型的非线性能力，并在某些情况下提供更好的性能和更快的收敛速度。GLU块常用于Transformer模型中的编码器和解码器，用于对输入向量进行非线性变换和特征提取。</p>
<p>需要注意的是，GLU块和Swish激活函数是两个不同的概念，它们在计算公式和应用场景上有所区别。在实际应用中，可以根据具体情况选择合适的激活函数来代替Swish或GLU。</p>
</li>
<li></li>
</ol>
<h1 id="CUDA-PTX-ISA"><a href="#CUDA-PTX-ISA" class="headerlink" title="CUDA PTX ISA"></a>CUDA PTX ISA</h1><h2 id="2-1-A-Highly-Multithreaded-Coprocessor"><a href="#2-1-A-Highly-Multithreaded-Coprocessor" class="headerlink" title="2.1. A Highly Multithreaded Coprocessor"></a>2.1. A Highly Multithreaded Coprocessor</h2><p>The GPU is a compute device capable of executing a very large number of threads in parallel. It operates as a coprocessor to the main CPU, or host: In other words, data-parallel, compute-intensive portions of applications running on the host are off-loaded onto the device. Moreprecisely, a portion of anapplication that is executed many imes, but independently on different data, can be isolated into a kernel function that is executed on the GPU as many different threads. To that effect,such a function is compiled to the PTX instruction set and the resulting kernel is translated at install time to the target GPU instruction set. </p>
<blockquote>
<h2 id="2-1-高度多线程的协处理器"><a href="#2-1-高度多线程的协处理器" class="headerlink" title="2.1. 高度多线程的协处理器"></a>2.1. 高度多线程的协处理器</h2><p>GPU 是一种计算设备，能够并行执行大量线程。它作为主 CPU（或主机）的协处理器运行：换句话说，运行在主机上的应用程序中数据并行、计算密集的部分被卸载到设备上。更准确地说，应用程序中被多次执行但独立处理不同数据的部分可以被隔离为一个内核函数，该函数在 GPU 上作为多个不同的线程执行。为此，这样的函数被编译为 PTX 指令集，并在安装时将生成的内核翻译为目标 GPU 指令集。</p>
</blockquote>
<h2 id="2-2-Thread-Hierarchy"><a href="#2-2-Thread-Hierarchy" class="headerlink" title="2.2. Thread Hierarchy"></a>2.2. Thread Hierarchy</h2><p>The batch of threads that executes a kernel is organized as a grid. A grid consists of either cooperative thread arrays or clusters of cooperative thread arrays as described in this section and illustrated in Figure 1 and Figure 2. Cooperative thread arrays (CTAs) implement CUDA thread blocks and clusters implement CUDA thread block clusters. </p>
<blockquote>
<h2 id="2-2-线程层次结构"><a href="#2-2-线程层次结构" class="headerlink" title="2.2. 线程层次结构"></a>2.2. 线程层次结构</h2><p>执行内核的线程批次被组织为一个网格。网格由协作线程数组或协作线程数组集群组成，如本节所述并在图 1 和图 2 中所示。协作线程数组（CTA）实现 CUDA 线程块，而集群实现 CUDA 线程块集群。</p>
</blockquote>
<h3 id="2-2-1-Cooperative-Thread-Arrays"><a href="#2-2-1-Cooperative-Thread-Arrays" class="headerlink" title="2.2.1. Cooperative Thread Arrays"></a>2.2.1. Cooperative Thread Arrays</h3><p>The Parallel Thread Execution (PTX) programming model is explicitly parallel: a PTX program specifies the execution of a given thread of a parallel thread array. A cooperativethreadarray, or CTA, is an array of threads that execute a kernel concurrently or in parallel. Threads within a CTA can communicate with each other. To coordinate the communication of the threads within the CTA, one can specify synchronization points where threads wait until all threads in the CTA have arrived. Each thread has a unique thread identifier within the CTA. Programs use a data parallel decomposi tion to partition inputs, work, and results across the threads of the CTA. Each CTA thread uses its thread identifier to determine its assigned role, assign specific input and output positions, compute addresses, and select work to perform. The thread identifier is a three-element vector tid, (with ele ments tid.x, tid.y, and tid.z) that specifies the thread’s position within a 1D, 2D, or 3D CTA. Each thread identifier component ranges from zero up to the number of thread ids in that CTA dimension. Each CTA has a1D,2D,or3D shape specified by a three-element vector ntid (with elements ntid.x, ntid.y, and ntid.z). The vector ntid specifies the number of threads in each CTA dimension. </p>
<p>Threads within a CTA execute in SIMT (single-instruction, multiple-thread) fashion in groups called warps. A warp is a maximal subset of threads from a single CTA, such that the threads execute the same instructions at the same time. Threads within a warp are sequentially numbered. The warp size is a machine-dependent constant. Typically, a warp has 32 threads. Some applications may be able to maximize performance with knowledge of the warp size, so PTX includes a run-time immediate constant, WARP_SZ, which may be used in any instruction where an immediate operand is allowed. </p>
<blockquote>
<h3 id="2-2-1-协作线程数组"><a href="#2-2-1-协作线程数组" class="headerlink" title="2.2.1. 协作线程数组"></a>2.2.1. 协作线程数组</h3><p>并行线程执行（PTX）编程模型是显式并行的：PTX 程序指定并行线程数组中给定线程的执行。协作线程数组（CTA）是一个线程数组，它们并发或并行地执行内核。CTA 中的线程可以相互通信。为了协调 CTA 内线程的通信，可以指定同步点，线程在此等待，直到 CTA 中的所有线程都到达。每个线程在 CTA 中都有一个唯一的线程标识符。程序使用数据并行分解来跨 CTA 的线程划分输入、工作和结果。每个 CTA 线程使用其线程标识符来确定其分配的角色、指定特定的输入和输出位置、计算地址并选择要执行的工作。线程标识符是一个三元素向量 tid（元素为 tid.x、tid.y 和 tid.z），指定线程在 1D、2D 或 3D CTA 中的位置。每个线程标识符组件的范围从零到该 CTA 维度中的线程 ID 数量。每个 CTA 都有一个 1D、2D 或 3D 形状，由三元素向量 ntid（元素为 ntid.x、ntid.y 和 ntid.z）指定。向量 ntid 指定每个 CTA 维度中的线程数量。</p>
<p>CTA 中的线程以 SIMT（单指令多线程）方式执行，称为 warp。warp 是来自单个 CTA 的最大线程子集，这些线程同时执行相同的指令。warp 中的线程按顺序编号。warp 大小是一个与机器相关的常数。通常，一个 warp 有 32 个线程。一些应用程序可能能够通过了解 warp 大小来最大化性能，因此 PTX 包含一个运行时立即常数 WARP_SZ，可以在允许立即操作数的任何指令中使用。</p>
</blockquote>
<h3 id="2-2-2-Cluster-of-Cooperative-Thread-Arrays"><a href="#2-2-2-Cluster-of-Cooperative-Thread-Arrays" class="headerlink" title="2.2.2. Cluster of Cooperative Thread Arrays"></a>2.2.2. Cluster of Cooperative Thread Arrays</h3><p>Cluster is a group of CTAs that run concurrently or in parallel and can synchronize and communicate with each other via shared memory. The executing CTA has to make sure that the shared memory of the peer CTA exists before communicating with it via shared memory. </p>
<p>Threads within the different CTAs in a cluster can synchronize and communicate with each other via shared memory. Cluster-wide barriers can be used to synchronize all the threads within the cluster. Each CTA in a cluster has a unique CTA identifier within its cluster (cluster_ctaid). Each cluster of CTAs has 1D, 2D or 3D shape specified by the parameter cluster_nctaid. Each CTA in the cluster also has a unique CTA identifier (cluster_ctarank) across all dimensions. The total number of CTAs across all the dimensions in the cluster is specified by cluster_nctarank. Threads may read and use these values through predefined, read-only special registers %cluster_ctaid, %cluster_nctaid, %clus ter_ctarank, %cluster_nctarank. </p>
<p>Cluster level is applicable only on target architecture sm_90 or higher. Specifying cluster level during launch time is optional. If the user specifies the cluster dimensions at launch time then it will be treated as explicit cluster launch, otherwise it will be treated as implicit cluster launch with default dimension 1x1x1. PTX provides read-only special register %is_explicit_cluster to differentiate between explicit and implicit cluster launch. </p>
<blockquote>
<h3 id="2-2-2-协作线程数组集群"><a href="#2-2-2-协作线程数组集群" class="headerlink" title="2.2.2. 协作线程数组集群"></a>2.2.2. 协作线程数组集群</h3><p>集群是一组并发或并行运行的 CTA，它们可以通过共享内存相互同步和通信。执行的 CTA 必须确保对等 CTA 的共享内存存在，然后才能通过共享内存与其通信。</p>
<p>集群中不同 CTA 中的线程可以通过共享内存相互同步和通信。可以使用集群范围的屏障来同步集群中的所有线程。集群中的每个 CTA 在其集群中都有一个唯一的 CTA 标识符（cluster_ctaid）。每个 CTA 集群都有一个 1D、2D 或 3D 形状，由参数 cluster_nctaid 指定。集群中的每个 CTA 在所有维度上也有一个唯一的 CTA 标识符（cluster_ctarank）。集群中所有维度上的 CTA 总数由 cluster_nctarank 指定。线程可以通过预定义的只读特殊寄存器 %cluster_ctaid、%cluster_nctaid、%cluster_ctarank、%cluster_nctarank 读取和使用这些值。</p>
<p>集群级别仅适用于目标架构 sm_90 或更高版本。在启动时指定集群级别是可选的。如果用户在启动时指定了集群维度，则将其视为显式集群启动，否则将视为隐式集群启动，默认维度为 1x1x1。PTX 提供只读特殊寄存器 %is_explicit_cluster 来区分显式和隐式集群启动。</p>
</blockquote>
<h3 id="2-2-3-Grid-of-Clusters"><a href="#2-2-3-Grid-of-Clusters" class="headerlink" title="2.2.3. Grid of Clusters"></a>2.2.3. Grid of Clusters</h3><p>There is a maximum number of threads that a CTA can contain and a maximum number of CTAs that a cluster can contain. However, clusters with CTAs that execute the same kernel can be batched to gether into a grid of clusters, so that the total number of threads that can be launched in a single kernel invocation is very large. This comes at the expense of reduced thread communication and syn chronization, because threads in different clusters cannot communicate and synchronize with each other. </p>
<p>Each cluster has a unique cluster identifier (clusterid) within a grid of clusters. Each grid of clusters has a1D,2D,or3Dshapespecifiedbytheparameternclusterid. Each grid also has a unique temporal grid identifier (gridid). Threads may read and use these values through predefined, read-only special registers %tid, %ntid, %clusterid, %nclusterid, and %gridid.</p>
<p>Each CTA has a unique identifier (ctaid) within a grid. Each grid of CTAs has 1D, 2D, or 3D shape specified by the parameter nctaid. Thread may use and read these values through predefined, read only special registers %ctaid and %nctaid. Each kernel is executed as a batch of threads organized as a grid of clusters consisting of CTAs where cluster is optional level and is applicable only for target architectures sm_90 and higher. Figure1 shows a grid consisting of CTAs and Figure 2 shows a grid consisting of clusters. </p>
<p>Grids may be launched with dependencies between one another- a grid may be a dependent grid and&#x2F;or a prerequisite grid. To understand how grid dependencies may be defined, refer to the section on CUDA Graphs in the Cuda Programming Guide.</p>
<blockquote>
<h3 id="2-2-3-集群网格"><a href="#2-2-3-集群网格" class="headerlink" title="2.2.3. 集群网格"></a>2.2.3. 集群网格</h3><p>CTA 可以包含的最大线程数和集群可以包含的最大 CTA 数是有限的。然而，执行相同内核的 CTA 集群可以被批量组合成一个集群网格，以便在一次内核调用中可以启动的线程总数非常大。这会以减少线程通信和同步为代价，因为不同集群中的线程无法相互通信和同步。</p>
<p>每个集群在集群网格中都有一个唯一的集群标识符（clusterid）。每个集群网格都有一个 1D、2D 或 3D 形状，由参数 nclusterid 指定。每个网格还有一个唯一的临时网格标识符（gridid）。线程可以通过预定义的只读特殊寄存器 %tid、%ntid、%clusterid、%nclusterid 和 %gridid 读取和使用这些值。</p>
<p>每个 CTA 在网格中都有一个唯一的标识符（ctaid）。每个 CTA 网格都有一个 1D、2D 或 3D 形状，由参数 nctaid 指定。线程可以通过预定义的只读特殊寄存器 %ctaid 和 %nctaid 使用和读取这些值。每个内核作为由 CTA 组成的集群网格中的一批线程执行，其中集群是可选级别，仅适用于目标架构 sm_90 及更高版本。图 1 显示了一个由 CTA 组成的网格，图 2 显示了一个由集群组成的网格。</p>
<p>网格可以在彼此之间启动依赖关系——一个网格可以是依赖网格和&#x2F;或先决条件网格。要了解如何定义网格依赖关系，请参阅 CUDA 编程指南中的 CUDA 图表部分。</p>
</blockquote>
<img src="\llm\image-20241026173153579.png" alt="image-20241026173153579" style="zoom: 33%;" />

<img src="\llm\image-20241026173138931.png" alt="image-20241026173138931" style="zoom:33%;" />

<h2 id="2-3-Memory-Hierarchy"><a href="#2-3-Memory-Hierarchy" class="headerlink" title="2.3. Memory Hierarchy"></a>2.3. Memory Hierarchy</h2><p>PTX threads may access data from multiple state spaces during their execution as illustrated by Figure 3 where cluster level is introduced from target architecture sm_90 onwards. Each thread has a private local memory. Each thread block (CTA) has a shared memory visible to all threads of the block and to all active blocks in the cluster and with the same lifetime as the block. Finally, all threads have access to the same global memory. </p>
<p>There are additional state spaces accessible by all threads: the constant, param, texture, and surface state spaces. Constant and texture memory are read-only; surface memory is readable and writable. The global, constant, param, texture, and surface state spaces are optimized for different memory usages. For example, texture memory offers different addressing modes as well as data filtering for specific data formats. Note that texture and surface memory is cached, and within the same kernel call, the cache is not kept coherent with respect to global memory writes and surface memory writes, so any texture fetch or surface read to an address that has been written to via a global or a surface write in the same kernel call returns undefined data. In other words, a thread can safely read some texture or surface memory location only if this memory location has been updated by a previous kernel call or memory copy, but not if it has been previously updated by the same thread or another thread from the same kernel call. </p>
<p>The global, constant, and texture state spaces are persistent across kernel launches by the same application. </p>
<p>Both the host and the device maintain their own local memory, referred to as host memory and device memory,respectively. The device memory maybe mapped and read or written by the host,or,for more efficient transfer, copied from the host memory through optimized API calls that utilize the device’s high-performance Direct Memory Access (DMA) engine.</p>
<blockquote>
<h2 id="2-3-内存层次结构"><a href="#2-3-内存层次结构" class="headerlink" title="2.3. 内存层次结构"></a>2.3. 内存层次结构</h2><p>如图 3 所示，PTX 线程在其执行期间可以从多个状态空间访问数据，其中集群级别从目标架构 sm_90 开始引入。每个线程都有私有本地内存。每个线程块（CTA）都有共享内存，对块中的所有线程以及集群中的所有活动块可见，并且与块具有相同的生命周期。最后，所有线程都可以访问相同的全球内存。</p>
<p>所有线程都可以访问其他状态空间：常量、参数、纹理和表面状态空间。常量和纹理内存是只读的；表面内存是可读写的。全局、常量、参数、纹理和表面状态空间针对不同的内存使用进行了优化。例如，纹理内存提供了不同的寻址模式以及针对特定数据格式的数据过滤。请注意，纹理和表面内存是缓存的，并且在同一内核调用中，缓存不会与全局内存写入和表面内存写入保持一致，因此任何对在同一内核调用中通过全局或表面写入更新的地址进行纹理提取或表面读取都会返回未定义的数据。换句话说，线程只能安全地读取某些纹理或表面内存位置，前提是该内存位置已由先前的内核调用或内存复制更新，但不能在同一内核调用中由同一线程或其他线程更新。</p>
<p>全局、常量和纹理状态空间在同一应用程序的内核启动之间是持久的。</p>
<p>主机和设备都维护自己的本地内存，分别称为主机内存和设备内存。设备内存可以映射并由主机读取或写入，或者通过利用设备的高性能直接内存访问（DMA）引擎的优化 API 调用从主机内存复制，以实现更高效的传输。</p>
</blockquote>
<img src="\llm\image-20241026173019711.png" alt="image-20241026173019711" style="zoom:33%;" />

<h2 id="3-1-A-Set-of-SIMT-Multiprocessors"><a href="#3-1-A-Set-of-SIMT-Multiprocessors" class="headerlink" title="3.1. A Set of SIMT Multiprocessors"></a>3.1. A Set of SIMT Multiprocessors</h2><p>The NVIDIA GPU architecture is built around a scalable array of multithreaded Streaming Multiprocessors (SMs). When a host program invokes a kernel grid, the blocks of the grid are enumerated and distributed to multiprocessors with available execution capacity. The threads of a thread block execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are launched on the vacated multiprocessors.</p>
<p>A multiprocessor consists of multiple Scalar Processor (SP) cores, a multithreaded instruction unit, and on-chip shared memory. The multiprocessor creates, manages, and executes concurrent threads in hardware with zero scheduling overhead. It implements a single-instruction barrier synchroniza tion. Fast barrier synchronization together with lightweight thread creation and zero-overhead thread scheduling efficiently support very fine-grained parallelism, allowing, for example, a low granularity decomposition of problems by assigning one thread to each data element (such as a pixel in an image, a voxel in a volume, a cell in a grid-based computation). </p>
<p>To manage hundreds of threads running several different programs, the multiprocessor employs an architecture we call SIMT (single-instruction, multiple-thread). The multiprocessor maps each thread to one scalar processor core, and each scalar thread executes independently with its own instruction address and register state. The multiprocessor SIMT unit creates, manages, schedules, and executes threads in groupsofparallel threads called warps. (This term originates from weaving, the first parallel thread technology.) Individual threads composing a SIMT warp start together at the same program address but are otherwise free to branch and execute independently. </p>
<p>When a multiprocessor is given one or more thread blocks to execute, it splits them into warps that get scheduled by the SIMT unit. The way a block is split into warps is always the same; each warp contains threads of consecutive, increasing thread IDs with the first warp containing thread 0. </p>
<p>At every instruction issue time, the SIMT unit selects a warp that is ready to execute and issues the next instruction to the active threads of the warp. A warpexecutesonecommoninstructionatatime, so full efficiency is realized when all threads of a warp agree on their execution path. If threads of a warp diverge via a data-dependent conditional branch, the warp serially executes each branch path taken, disabling threads that are not on that path, and when all paths complete, the threads converge back to the same execution path. Branch divergence occurs only within a warp;different warps execute independently regardless of whether they are executing common or disjointed code paths. </p>
<p>SIMT architecture is akin to SIMD (Single Instruction, Multiple Data) vector organizations in that a single instruction controls multiple processing elements. A key difference is that SIMD vector organi zations expose the SIMD width to the software, whereas SIMT instructions specify the execution and branching behavior of a single thread. In contrast with SIMD vector machines, SIMT enables program mers to write thread-level parallel code for independent, scalar threads, as well as data-parallel code for coordinated threads. For the purposes of correctness, the programmer can essentially ignore the  SIMT behavior; however, substantial performance improvements can be realized by taking care that the code seldom requires threads in a warp to diverge. Inpractice,thisisanalogoustotheroleofcache lines in traditional code: Cacheline size can be safely ignored when designing for correctness but must be considered in the code structure when designing for peak performance. Vector architectures, on the other hand, require the software to coalesce loads into vectors and manage divergence manually.</p>
<p>How many blocks a multiprocessor can process at once depends on how many registers per thread and how much shared memory per block are required for a given kernel since the multiprocessor’s registers and shared memory are split among all the threads of the batch of blocks. If there are not enough registers or shared memory available per multiprocessor to process at least one block, the kernel will fail to launch.</p>
<blockquote>
<h2 id="3-1-一组SIMT多处理器"><a href="#3-1-一组SIMT多处理器" class="headerlink" title="3.1. 一组SIMT多处理器"></a>3.1. 一组SIMT多处理器</h2><p>NVIDIA GPU架构围绕一个可扩展的多线程流式多处理器（SM）阵列构建。当主机程序调用内核网格时，网格的块会被枚举并分配给具有可用执行能力的多处理器。线程块的线程在一个多处理器上并发执行。随着线程块的终止，新的块会在空闲的多处理器上启动。</p>
<p>一个多处理器由多个标量处理器（SP）核心、一个多线程指令单元和片上共享内存组成。多处理器在硬件中创建、管理和执行并发线程，零调度开销。它实现了一个单指令屏障同步。快速的屏障同步与轻量级线程创建和零开销线程调度有效地支持非常细粒度的并行性，例如，通过为每个数据元素（如图像中的像素、体积中的体素、基于网格的计算中的单元）分配一个线程来分解问题。</p>
<p>为了管理数百个运行多个不同程序的线程，多处理器采用了一种我们称之为SIMT（单指令多线程）的架构。多处理器将每个线程映射到一个标量处理器核心，每个标量线程独立执行，具有自己的指令地址和寄存器状态。多处理器的SIMT单元在称为warp的并行线程组中创建、管理、调度和执行线程。（这个术语源自纺织，这是第一种并行线程技术。）组成SIMT warp的各个线程从相同的程序地址开始，但其他方面可以自由分支和独立执行。</p>
<p>当多处理器被分配一个或多个线程块来执行时，它将它们拆分为由SIMT单元调度的warp。块被拆分为warp的方式总是相同的；每个warp包含连续递增的线程ID，第一个warp包含线程0。</p>
<p>在每次指令发布时间，SIMT单元选择一个准备执行的warp，并向warp的活动线程发布下一条指令。一个warp一次执行一条公共指令，因此当warp的所有线程在执行路径上达成一致时，效率最高。如果warp的线程通过数据相关的条件分支发散，warp会依次执行每个分支路径，禁用不在该路径上的线程，当所有路径完成后，线程会重新汇聚到相同的执行路径。分支发散仅发生在warp内部；不同的warp无论执行公共还是不相关的代码路径，都独立执行。</p>
<p>SIMT架构类似于SIMD（单指令多数据）向量组织，因为一条指令控制多个处理单元。一个关键区别是，SIMD向量组织向软件暴露SIMD宽度，而SIMT指令指定单个线程的执行和分支行为。与SIMD向量机相比，SIMT使程序员能够为独立的标量线程编写线程级并行代码，以及为协调的线程编写数据并行代码。为了正确性，程序员基本上可以忽略SIMT行为；然而，通过确保代码很少需要warp中的线程发散，可以实现显著的性能提升。在实践中，这类似于传统代码中缓存行的作用：在设计正确性时可以安全地忽略缓存行大小，但在设计峰值性能时必须考虑代码结构中的缓存行大小。另一方面，向量架构要求软件将加载合并到向量中并手动管理发散。</p>
<p>多处理器一次可以处理多少个块取决于给定内核所需的每个线程的寄存器数量和每个块的共享内存数量，因为多处理器的寄存器和共享内存被分配给批处理块的所有线程。如果每个多处理器没有足够的寄存器或共享内存来处理至少一个块，内核将无法启动。</p>
</blockquote>
<img src="\llm\image-20241026174132332.png" alt="image-20241026174132332" style="zoom: 50%;" />



<h1 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h1></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://noexcs.github.io">noexcs</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://noexcs.github.io/llm/">http://noexcs.github.io/llm/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/default/">default</a></div><div class="post-share"><div class="social-share" data-image="https://avatars.githubusercontent.com/u/56532264?v=4" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"></nav></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%80%BB%E7%BB%93"><span class="toc-text">论文阅读总结</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Attention-Is-All-You-Need"><span class="toc-text">Attention Is All You Need.</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9"><span class="toc-text">主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%B1%E5%93%8D%E5%92%8C%E5%BA%94%E7%94%A8"><span class="toc-text">影响和应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%96%87%E7%AB%A0%EF%BC%9A"><span class="toc-text">其他文章：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98"><span class="toc-text">一些问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PagedAttention"><span class="toc-text">PagedAttention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#PagedAttention-%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-text">PagedAttention 的定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PagedAttention-%E7%9A%84%E5%8A%9F%E8%83%BD"><span class="toc-text">PagedAttention 的功能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PagedAttention-%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">PagedAttention 的应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PagedAttention-%E7%9A%84%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82"><span class="toc-text">PagedAttention 的技术细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GroupAttention"><span class="toc-text">GroupAttention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FlashAttention"><span class="toc-text">FlashAttention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-1"><span class="toc-text">主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-text">结论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%B1%E5%93%8D%E5%92%8C%E5%BA%94%E7%94%A8-1"><span class="toc-text">影响和应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scaling-Down-to-Scale-Up-A-Guide-to-Parameter-Efficient-Fine-Tuning"><span class="toc-text">Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-2"><span class="toc-text">主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Additive-methods"><span class="toc-text">Additive methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Selective-methods"><span class="toc-text">Selective methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reparametrization-based-methods"><span class="toc-text">Reparametrization-based methods</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BERT"><span class="toc-text">BERT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-3"><span class="toc-text">主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%B1%E5%93%8D%E5%92%8C%E5%BA%94%E7%94%A8-2"><span class="toc-text">影响和应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-1"><span class="toc-text">结论</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BART"><span class="toc-text">BART</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ViT"><span class="toc-text">ViT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-4"><span class="toc-text">主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%B1%E5%93%8D%E5%92%8C%E5%BA%94%E7%94%A8-3"><span class="toc-text">影响和应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-2"><span class="toc-text">结论</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A-Survey-of-Large-Language-Models"><span class="toc-text">A Survey of Large Language Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-5"><span class="toc-text">主要内容</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ALiBi"><span class="toc-text">ALiBi</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-6"><span class="toc-text">主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%B1%E5%93%8D%E5%92%8C%E5%BA%94%E7%94%A8-4"><span class="toc-text">影响和应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-3"><span class="toc-text">结论</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RoPE"><span class="toc-text">RoPE</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-7"><span class="toc-text">主要内容</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GPT-3%EF%BC%88Language-Models-are-Few-Shot-Learners%EF%BC%89"><span class="toc-text">GPT-3（Language Models are Few-Shot Learners）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-8"><span class="toc-text">主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-4"><span class="toc-text">结论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%B1%E5%93%8D%E5%92%8C%E5%BA%94%E7%94%A8-5"><span class="toc-text">影响和应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CoT"><span class="toc-text">CoT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-9"><span class="toc-text">主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%B1%E5%93%8D%E5%92%8C%E5%BA%94%E7%94%A8-6"><span class="toc-text">影响和应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-5"><span class="toc-text">结论</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RAG"><span class="toc-text">RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-10"><span class="toc-text">主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-6"><span class="toc-text">结论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%B1%E5%93%8D%E5%92%8C%E5%BA%94%E7%94%A8-7"><span class="toc-text">影响和应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Prefix-Tuning"><span class="toc-text">Prefix-Tuning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-11"><span class="toc-text">主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-7"><span class="toc-text">结论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%B1%E5%93%8D%E5%92%8C%E5%BA%94%E7%94%A8-8"><span class="toc-text">影响和应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LoRA"><span class="toc-text">LoRA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-12"><span class="toc-text">主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-8"><span class="toc-text">结论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%B1%E5%93%8D%E5%92%8C%E5%BA%94%E7%94%A8-9"><span class="toc-text">影响和应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Megatron-LM"><span class="toc-text">Megatron-LM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-13"><span class="toc-text">主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MLP%E9%83%A8%E5%88%86%E5%B9%B6%E8%A1%8C"><span class="toc-text">MLP部分并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E9%83%A8%E5%88%86%E5%B9%B6%E8%A1%8C"><span class="toc-text">多头注意力部分并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E5%B1%82"><span class="toc-text">输入输出层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ZeRO"><span class="toc-text">ZeRO</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-14"><span class="toc-text">主要内容</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MoE"><span class="toc-text">MoE</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-15"><span class="toc-text">主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-9"><span class="toc-text">结论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%B1%E5%93%8D%E5%92%8C%E5%BA%94%E7%94%A8-10"><span class="toc-text">影响和应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Clip"><span class="toc-text">Clip</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LongLoRA"><span class="toc-text">LongLoRA</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Tokenizer"><span class="toc-text">Tokenizer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#minbpe"><span class="toc-text">minbpe</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98%EF%BC%88%E6%9D%A5%E8%87%AAAndrej-Karpathy"><span class="toc-text">一些问题（来自Andrej Karpathy)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BPE%EF%BC%88Byte-Pair-Encoding%EF%BC%89"><span class="toc-text">BPE（Byte-Pair Encoding）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tiktoken%E5%BA%93"><span class="toc-text">tiktoken库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#WordPiece"><span class="toc-text">WordPiece</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E7%89%B9%E7%82%B9"><span class="toc-text">主要特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B"><span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SentencePiece"><span class="toc-text">SentencePiece</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Word2Vec"><span class="toc-text">Word2Vec</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BD%95%E4%B8%8D%E9%87%87%E2%BD%A4one-hot%E5%90%91%E9%87%8F"><span class="toc-text">为何不采⽤one-hot向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Word2Vec-%E7%9A%84%E4%BC%98%E7%82%B9"><span class="toc-text">Word2Vec 的优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Word2Vec-%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-text">Word2Vec 的局限性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text"></span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GloVe"><span class="toc-text">GloVe</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%BB%E6%B5%81%E6%9E%B6%E6%9E%84"><span class="toc-text">主流架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Encoder-Decoder"><span class="toc-text">Encoder-Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#T5"><span class="toc-text">T5</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BART-1"><span class="toc-text">BART</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Encoder-Only"><span class="toc-text">Encoder-Only</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BERT-1"><span class="toc-text">BERT</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Decoder-Only"><span class="toc-text">Decoder-Only</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT"><span class="toc-text">GPT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Llama"><span class="toc-text">Llama</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Prefix-Decoder"><span class="toc-text">Prefix Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ChatGLM"><span class="toc-text">ChatGLM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#UNILM"><span class="toc-text">UNILM</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#NanoGPT%E9%A1%B9%E7%9B%AE"><span class="toc-text">NanoGPT项目</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#model"><span class="toc-text">model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5%E5%8C%85"><span class="toc-text">导入包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gelu"><span class="toc-text">gelu</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CausalSelfAttention"><span class="toc-text">CausalSelfAttention</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MultiHead-Attention"><span class="toc-text">MultiHead Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Andrej-Karpathy-%E8%A7%A3%E9%87%8A"><span class="toc-text">Andrej Karpathy 解释</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MLP"><span class="toc-text">MLP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Block"><span class="toc-text">Block</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPTConfig"><span class="toc-text">GPTConfig</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-1"><span class="toc-text">GPT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-init"><span class="toc-text">GPT.__init__</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-forward"><span class="toc-text">GPT.forward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-generate"><span class="toc-text">GPT.generate</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#temperature"><span class="toc-text">temperature</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TopK"><span class="toc-text">TopK</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8top-k%E5%8F%82%E6%95%B0%EF%BC%9F"><span class="toc-text">为什么使用top-k参数？</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TopP"><span class="toc-text">TopP</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8top-p%E5%8F%82%E6%95%B0%EF%BC%9F"><span class="toc-text">为什么使用top-p参数？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#top-p%E9%87%87%E6%A0%B7%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-text">top-p采样的工作原理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-text">例子</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-crop-block-size"><span class="toc-text">GPT.crop_block_size</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-from-pretrained"><span class="toc-text">GPT.from_pretrained</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-configure-optimizers"><span class="toc-text">GPT.configure_optimizers</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Train"><span class="toc-text">Train</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5%E5%8C%85-1"><span class="toc-text">导入包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%BB%98%E8%AE%A4%E9%85%8D%E7%BD%AE"><span class="toc-text">默认配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E5%87%86%E5%A4%87"><span class="toc-text">其他准备</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">模型初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#loss%E8%AF%84%E4%BC%B0"><span class="toc-text">loss评估</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">优化器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-text">学习率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%97%A5%E5%BF%97"><span class="toc-text">日志</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#training-loop"><span class="toc-text">training loop</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#seq2seq%E6%A8%A1%E5%9E%8B"><span class="toc-text">seq2seq模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84"><span class="toc-text">基本结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-text">工作流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Attention-Mechanism%EF%BC%89"><span class="toc-text">注意力机制（Attention Mechanism）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-1"><span class="toc-text">应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%AF%BC%E5%85%A5%E5%BF%85%E8%A6%81%E7%9A%84%E5%BA%93"><span class="toc-text">1. 导入必要的库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%AE%9A%E4%B9%89%E7%BC%96%E7%A0%81%E5%99%A8%EF%BC%88Encoder%EF%BC%89"><span class="toc-text">2. 定义编码器（Encoder）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%AE%9A%E4%B9%89%E8%A7%A3%E7%A0%81%E5%99%A8%EF%BC%88Decoder%EF%BC%89"><span class="toc-text">3. 定义解码器（Decoder）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%AE%9A%E4%B9%89Seq2Seq%E6%A8%A1%E5%9E%8B"><span class="toc-text">4. 定义Seq2Seq模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">5. 初始化模型和优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-text">6. 训练模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E4%BD%BF%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B"><span class="toc-text">7. 使用模型进行预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-text">总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#KV-Cache"><span class="toc-text">KV Cache</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#KV-Cache%E7%BC%BA%E9%99%B7"><span class="toc-text">KV Cache缺陷</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8B"><span class="toc-text">开源模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Llama%E7%B3%BB%E5%88%97"><span class="toc-text">Llama系列</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Llama-1"><span class="toc-text">Llama</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Llama-3-1"><span class="toc-text">Llama 3.1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E6%AD%A5%E9%AA%A4"><span class="toc-text">具体步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-text">优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-2"><span class="toc-text">应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-2"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GPT%E7%B3%BB%E5%88%97"><span class="toc-text">GPT系列</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-1"><span class="toc-text">GPT-1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-2"><span class="toc-text">GPT-2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-3"><span class="toc-text">GPT-3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#InstructGPT"><span class="toc-text">InstructGPT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ChatGPT"><span class="toc-text">ChatGPT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-4"><span class="toc-text">GPT-4</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-4V"><span class="toc-text">GPT-4V</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-4o"><span class="toc-text">GPT-4o</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Qwen"><span class="toc-text">Qwen</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ChatGLM-1"><span class="toc-text">ChatGLM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RoBERTa"><span class="toc-text">RoBERTa</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Fine-Tuning"><span class="toc-text">Fine-Tuning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Prompt-Tuning"><span class="toc-text">Prompt Tuning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adapter-Tuning"><span class="toc-text">Adapter Tuning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LoRA-1"><span class="toc-text">LoRA</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83"><span class="toc-text">分布式训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E6%8C%91%E6%88%98%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-text">分布式训练挑战有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DeepSpeed"><span class="toc-text">DeepSpeed</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Megatron-LM-1"><span class="toc-text">Megatron-LM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E7%BB%86%E8%8A%82"><span class="toc-text">一些细节</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Ring-AllReduce"><span class="toc-text">Ring-AllReduce</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Alignment"><span class="toc-text">Alignment</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#RLHF"><span class="toc-text">RLHF</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98-1"><span class="toc-text">一些问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PPO"><span class="toc-text">PPO</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DPO"><span class="toc-text">DPO</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Agent"><span class="toc-text">Agent</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80-%E6%9D%82%E9%A1%B9"><span class="toc-text">基础&#x2F;杂项</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Optimizer"><span class="toc-text">Optimizer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A8%E9%87%8F%EF%BC%88Momentum%EF%BC%89"><span class="toc-text">动量（Momentum）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSProp"><span class="toc-text">RMSProp</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adam%EF%BC%88Adaptive-momentum-estimation%EF%BC%89"><span class="toc-text">Adam（Adaptive momentum estimation）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AdamW"><span class="toc-text">AdamW</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLU"><span class="toc-text">ReLU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GeLU"><span class="toc-text">GeLU</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GeLU%E5%87%BD%E6%95%B0%E7%9A%84%E7%89%B9%E7%82%B9%EF%BC%9A"><span class="toc-text">GeLU函数的特点：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E7%82%B9%EF%BC%9A"><span class="toc-text">优点：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9%EF%BC%9A"><span class="toc-text">缺点：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%EF%BC%9A"><span class="toc-text">应用场景：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Swish"><span class="toc-text">Swish</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Swish%E5%87%BD%E6%95%B0%E7%9A%84%E7%89%B9%E7%82%B9%EF%BC%9A"><span class="toc-text">Swish函数的特点：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E7%82%B9%EF%BC%9A-1"><span class="toc-text">优点：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9%EF%BC%9A-1"><span class="toc-text">缺点：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SwiGLU"><span class="toc-text">SwiGLU</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#SwiGLU%E7%9A%84%E5%AE%9A%E4%B9%89%EF%BC%9A"><span class="toc-text">SwiGLU的定义：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SwiGLU%E7%9A%84%E7%89%B9%E7%82%B9%EF%BC%9A"><span class="toc-text">SwiGLU的特点：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E7%82%B9%EF%BC%9A-2"><span class="toc-text">优点：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9%EF%BC%9A-2"><span class="toc-text">缺点：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%EF%BC%9A-1"><span class="toc-text">应用场景：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GLU"><span class="toc-text">GLU</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GLU%E7%9A%84%E5%AE%9A%E4%B9%89%EF%BC%9A"><span class="toc-text">GLU的定义：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GLU%E7%9A%84%E7%89%B9%E7%82%B9%EF%BC%9A"><span class="toc-text">GLU的特点：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E7%82%B9%EF%BC%9A-3"><span class="toc-text">优点：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9%EF%BC%9A-3"><span class="toc-text">缺点：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%EF%BC%9A-2"><span class="toc-text">应用场景：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%98%E4%BD%93%EF%BC%9A"><span class="toc-text">变体：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-text">归一化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSNorm"><span class="toc-text">RMSNorm</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-text">基本概念</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%AD%A5%E9%AA%A4"><span class="toc-text">计算步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-1"><span class="toc-text">优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-3"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LayerNorm"><span class="toc-text">LayerNorm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BatchNorm"><span class="toc-text">BatchNorm</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5"><span class="toc-text">训练阶段</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5"><span class="toc-text">推理阶段</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-4"><span class="toc-text">总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%98%B2%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-text">防过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout"><span class="toc-text">Dropout</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5-1"><span class="toc-text">训练阶段</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5-1"><span class="toc-text">推理阶段</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Regularization"><span class="toc-text">Regularization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-Clipping"><span class="toc-text">Gradient Clipping</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Label-Smoothing"><span class="toc-text">Label Smoothing</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83"><span class="toc-text">混合精度训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E6%89%B9%E6%AC%A1-%E6%A2%AF%E5%BA%A6%E7%B4%AF%E7%A7%AF"><span class="toc-text">微批次 &#x2F; 梯度累积</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E7%94%A8%E9%80%94%E5%92%8C%E4%BC%98%E7%82%B9"><span class="toc-text">主要用途和优点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Beam-Search"><span class="toc-text">Beam Search</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%AA%E5%A9%AA%E6%90%9C%E7%B4%A2"><span class="toc-text">贪婪搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%AA%E5%A9%AA%E6%90%9C%E7%B4%A2%E4%BC%9A%E4%BA%A7%E7%94%9F%E9%9D%9E%E6%9C%80%E4%BC%98%E8%BE%93%E5%87%BA%E5%BA%8F%E5%88%97"><span class="toc-text">贪婪搜索会产生非最优输出序列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A9%B7%E4%B8%BE%E6%90%9C%E7%B4%A2"><span class="toc-text">穷举搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%9F%E6%90%9C%E7%B4%A2%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="toc-text">束搜索基本原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-2"><span class="toc-text">优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9"><span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-3"><span class="toc-text">应用场景</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87"><span class="toc-text">评测指标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#F1"><span class="toc-text">F1</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%B2%BE%E7%A1%AE%E7%8E%87%EF%BC%88Precision%EF%BC%89"><span class="toc-text">精确率（Precision）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%AC%E5%9B%9E%E7%8E%87%EF%BC%88Recall%EF%BC%89"><span class="toc-text">召回率（Recall）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A"><span class="toc-text">解释</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-1"><span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Micro-F1-Macro-F1"><span class="toc-text">Micro-F1 &amp; Macro-F1</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8F%E5%B9%B3%E5%9D%87%EF%BC%88Macro-F1%EF%BC%89"><span class="toc-text">宏平均（Macro-F1）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BE%AE%E5%B9%B3%E5%9D%87%EF%BC%88Micro-F1%EF%BC%89"><span class="toc-text">微平均（Micro-F1）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8C%BA%E5%88%AB%E6%80%BB%E7%BB%93"><span class="toc-text">区别总结</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">适用场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-2"><span class="toc-text">示例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8F%E5%B9%B3%E5%9D%87"><span class="toc-text">宏平均</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BE%AE%E5%B9%B3%E5%9D%87"><span class="toc-text">微平均</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-5"><span class="toc-text">总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F"><span class="toc-text">知识蒸馏</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E6%AD%A5%E9%AA%A4"><span class="toc-text">主要步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-3"><span class="toc-text">优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF-4"><span class="toc-text">应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-6"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HMM-Hidden-Markov-Model"><span class="toc-text">HMM (Hidden Markov Model)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CRF-Conditional-Random-Field"><span class="toc-text">CRF (Conditional Random Field)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LDA-Latent-Dirichlet-Allocation"><span class="toc-text">LDA (Latent Dirichlet Allocation)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MinHash"><span class="toc-text">MinHash</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MinHash%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-text">MinHash的工作原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8MinHash%EF%BC%9F"><span class="toc-text">为什么使用MinHash？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-3"><span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6"><span class="toc-text">数学</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#KL%E6%95%A3%E5%BA%A6"><span class="toc-text">KL散度</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89-1"><span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%A7%E8%B4%A8"><span class="toc-text">性质</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90-1"><span class="toc-text">例子</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#JS%E6%95%A3%E5%BA%A6"><span class="toc-text">JS散度</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89-2"><span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%A7%E8%B4%A8-1"><span class="toc-text">性质</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90-2"><span class="toc-text">例子</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hessian-Matrix"><span class="toc-text">Hessian Matrix</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E6%80%A7%E8%B4%A8"><span class="toc-text">重要性质</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90-3"><span class="toc-text">例子</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Inverse-Hessian-Matrix"><span class="toc-text">Inverse Hessian Matrix</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AD%98%E5%9C%A8%E6%80%A7"><span class="toc-text">1. 存在性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%AF%B9%E7%A7%B0%E6%80%A7"><span class="toc-text">2. 对称性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%AD%A3%E5%AE%9A%E6%80%A7"><span class="toc-text">3. 正定性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E4%BC%98%E5%8C%96%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-text">4. 优化中的应用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E8%BF%91%E4%BC%BC%E9%80%86%E6%B5%B7%E6%A3%AE%E7%9F%A9%E9%98%B5"><span class="toc-text">5. 近似逆海森矩阵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E7%89%B9%E5%BE%81%E5%80%BC%E5%92%8C%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F"><span class="toc-text">6. 特征值和特征向量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90-4"><span class="toc-text">例子</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%B8%E5%87%BD%E6%95%B0%EF%BC%88Convex-Function%EF%BC%89"><span class="toc-text">凸函数（Convex Function）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83%EF%BC%88Poisson-distribution%EF%BC%89"><span class="toc-text">泊松分布（Poisson distribution）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83%E7%9A%84%E7%89%B9%E7%82%B9%EF%BC%9A"><span class="toc-text">泊松分布的特点：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83%E7%9A%84%E6%A6%82%E7%8E%87%E8%B4%A8%E9%87%8F%E5%87%BD%E6%95%B0%EF%BC%9A"><span class="toc-text">泊松分布的概率质量函数：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83%E7%9A%84%E6%9C%9F%E6%9C%9B%E5%80%BC%E5%92%8C%E6%96%B9%E5%B7%AE%EF%BC%9A"><span class="toc-text">泊松分布的期望值和方差：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83%E7%9A%84%E5%BA%94%E7%94%A8%EF%BC%9A"><span class="toc-text">泊松分布的应用：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83%E4%B8%8E%E5%85%B6%E4%BB%96%E5%88%86%E5%B8%83%E7%9A%84%E5%85%B3%E7%B3%BB%EF%BC%9A"><span class="toc-text">泊松分布与其他分布的关系：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%EF%BC%88Cross-Entropy-Loss%EF%BC%89"><span class="toc-text">交叉熵损失（Cross-Entropy Loss）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89-3"><span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-text">二分类问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-text">多分类问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%89%B9%E7%82%B9"><span class="toc-text">特点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8"><span class="toc-text">应用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-7"><span class="toc-text">总结</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Qusention-Answer-Generation"><span class="toc-text">Qusention-Answer Generation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96"><span class="toc-text">模型量化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E6%96%87%E7%AB%A0"><span class="toc-text">一些文章</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GPTQ"><span class="toc-text">GPTQ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AQW"><span class="toc-text">AQW</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Quantization-aware-training-QAT"><span class="toc-text">Quantization aware training (QAT)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Post-training-quantization%EF%BC%88PTQ%EF%BC%89"><span class="toc-text">Post training quantization（PTQ）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E6%AD%A5%E9%AA%A4-1"><span class="toc-text">主要步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-4"><span class="toc-text">优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9-1"><span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF-1"><span class="toc-text">适用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-8"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#W8A8"><span class="toc-text">W8A8</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E6%AD%A5%E9%AA%A4-1"><span class="toc-text">具体步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-5"><span class="toc-text">优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9-2"><span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF-2"><span class="toc-text">适用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-9"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Group-wise-quantization"><span class="toc-text">Group-wise quantization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E6%80%9D%E6%83%B3"><span class="toc-text">主要思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E6%AD%A5%E9%AA%A4"><span class="toc-text">实现步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-6"><span class="toc-text">优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9-3"><span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF-3"><span class="toc-text">适用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-10"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Per-channel-scaling"><span class="toc-text">Per-channel scaling</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E6%80%9D%E6%83%B3-1"><span class="toc-text">主要思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E6%AD%A5%E9%AA%A4-1"><span class="toc-text">实现步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-7"><span class="toc-text">优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9-4"><span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF-4"><span class="toc-text">适用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-11"><span class="toc-text">总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6"><span class="toc-text">推理框架</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#vLLM"><span class="toc-text">vLLM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TensorRT-LLM"><span class="toc-text">TensorRT-LLM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LMdeploy"><span class="toc-text">LMdeploy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Triton"><span class="toc-text">Triton</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="toc-text">1. 核心概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E7%A8%8B%E5%BA%8F%E5%9D%97%EF%BC%88Program-Blocks%EF%BC%89"><span class="toc-text">1.1 程序块（Program Blocks）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E7%BA%BF%E7%A8%8B%EF%BC%88Threads%EF%BC%89"><span class="toc-text">1.2 线程（Threads）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%EF%BC%88Shared-Memory%EF%BC%89"><span class="toc-text">1.3 共享内存（Shared Memory）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98%EF%BC%88Global-Memory%EF%BC%89"><span class="toc-text">1.4 全局内存（Global Memory）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%9E%8B"><span class="toc-text">2. 执行模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E5%86%85%E6%A0%B8%E6%89%A7%E8%A1%8C"><span class="toc-text">2.1 内核执行</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E7%BD%91%E6%A0%BC%EF%BC%88Grid%EF%BC%89"><span class="toc-text">2.2 网格（Grid）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-%E5%9D%97%E5%A4%A7%E5%B0%8F%EF%BC%88Block-Size%EF%BC%89"><span class="toc-text">2.3 块大小（Block Size）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%A4%BA%E4%BE%8B%E8%A7%A3%E6%9E%90"><span class="toc-text">3. 示例解析</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-text">模型评估</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#LLM-%E6%B5%8B%E8%AF%84%E9%9B%86"><span class="toc-text">LLM 测评集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#General"><span class="toc-text">General</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MMLU"><span class="toc-text">MMLU</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#MMLU-%E7%9A%84%E4%B8%BB%E8%A6%81%E7%89%B9%E7%82%B9%EF%BC%9A"><span class="toc-text">MMLU 的主要特点：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MMLU-%E6%B5%8B%E8%AF%95%E4%B8%AD%E7%9A%84%E9%A2%98%E5%9E%8B%EF%BC%9A"><span class="toc-text">MMLU 测试中的题型：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MMLU-Pro"><span class="toc-text">MMLU-Pro</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#IFEval"><span class="toc-text">IFEval</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#BBH"><span class="toc-text">BBH</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MuSR"><span class="toc-text">MuSR</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#MuSR-%E7%9A%84%E4%B8%BB%E8%A6%81%E7%89%B9%E7%82%B9%EF%BC%9A"><span class="toc-text">MuSR 的主要特点：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#MuSR-%E7%9A%84%E5%BA%94%E7%94%A8%EF%BC%9A"><span class="toc-text">MuSR 的应用：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Code"><span class="toc-text">Code</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HumanEval"><span class="toc-text">HumanEval</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MBPP-EvalPlus"><span class="toc-text">MBPP EvalPlus</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Math"><span class="toc-text">Math</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GSM8K"><span class="toc-text">GSM8K</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MATH"><span class="toc-text">MATH</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reasoning"><span class="toc-text">Reasoning</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ARC-Challenge"><span class="toc-text">ARC Challenge</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GPQA"><span class="toc-text">GPQA</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tool-use"><span class="toc-text">Tool use</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#BFCL"><span class="toc-text">BFCL</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Nexus"><span class="toc-text">Nexus</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Long-context"><span class="toc-text">Long context</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ZeroSCROLLS-QuALITY"><span class="toc-text">ZeroSCROLLS&#x2F;QuALITY</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#InfiniteBench-En-MC"><span class="toc-text">InfiniteBench&#x2F;En.MC</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NIH-Multi-needle"><span class="toc-text">NIH&#x2F;Multi-needle</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multilingual"><span class="toc-text">Multilingual</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MGSN"><span class="toc-text">MGSN</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-text">传统机器学习算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#XGBoost"><span class="toc-text">XGBoost</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM"><span class="toc-text">SVM</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98-2"><span class="toc-text">一些问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E7%8E%B0%E5%9C%A8%E7%9A%84LLM%E9%83%BD%E6%98%AFDecoder-only%E7%9A%84%E6%9E%B6%E6%9E%84%EF%BC%9F"><span class="toc-text">为什么现在的LLM都是Decoder only的架构？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%96%87%E7%AB%A0"><span class="toc-text">其他文章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%88%91%E6%B2%A1%E6%9C%89%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%8F%E9%AA%8C%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%BB%99%E4%B8%AA%E6%9C%BA%E4%BC%9A%E5%90%97%EF%BC%9F"><span class="toc-text">我没有大模型经验，可以给个机会吗？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%8F%E9%AA%8C"><span class="toc-text">经验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BA%E6%96%87"><span class="toc-text">论文</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%A4%E4%BA%86%E7%BB%8F%E9%AA%8C%E5%92%8C%E8%AE%BA%E6%96%87%EF%BC%8C%E8%BF%98%E8%83%BD%E7%9C%8B%E4%BB%80%E4%B9%88"><span class="toc-text">除了经验和论文，还能看什么</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E8%B5%84%E6%BA%90"><span class="toc-text">其他资源</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9D%A2%E7%BB%8F"><span class="toc-text">面经</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%85%BE%E8%AE%AFNLP%E7%AE%97%E6%B3%95%E5%B2%97%E9%9D%A2%E7%BB%8F"><span class="toc-text">腾讯NLP算法岗面经</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E9%9D%A2%EF%BC%88%E6%8A%80%E6%9C%AF%E9%9D%A2%EF%BC%89"><span class="toc-text">一面（技术面）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E9%9D%A2%EF%BC%88%E6%8A%80%E6%9C%AF%E9%9D%A2%EF%BC%89"><span class="toc-text">二面（技术面）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E9%9D%A2%EF%BC%88HR%E9%9D%A2%EF%BC%89"><span class="toc-text">三面（HR面）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%8F%E5%BB%BA%E8%AE%AE"><span class="toc-text">小建议</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%80%E5%8E%86"><span class="toc-text">简历</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%93%E4%B8%9A%E6%8A%80%E8%83%BD"><span class="toc-text">专业技能</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Java%E7%89%88"><span class="toc-text">Java版</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM%E7%AE%97%E6%B3%95%E7%89%88"><span class="toc-text">LLM算法版</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E7%BB%8F%E5%8E%86"><span class="toc-text">项目经历</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8B%9B%E8%81%98%E8%A6%81%E6%B1%82"><span class="toc-text">招聘要求</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#aiXcoder%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88"><span class="toc-text">aiXcoder大模型算法工程师</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%AB%E4%BA%BA%E7%9A%84%E7%AE%80%E5%8E%86"><span class="toc-text">别人的简历</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9D%A2%E8%AF%95%E9%A2%98"><span class="toc-text">面试题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%A5%E8%87%AA%EF%BC%9ALLMs-interview-notes"><span class="toc-text">来自：LLMs_interview_notes</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Causal-LM-%E5%9B%A0%E6%9E%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-text">Causal LM (因果语言模型)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Prefix-LM-%E5%89%8D%E7%BC%80%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-text">Prefix LM (前缀语言模型)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-12"><span class="toc-text">总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CUDA-PTX-ISA"><span class="toc-text">CUDA PTX ISA</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-A-Highly-Multithreaded-Coprocessor"><span class="toc-text">2.1. A Highly Multithreaded Coprocessor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E9%AB%98%E5%BA%A6%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8"><span class="toc-text">2.1. 高度多线程的协处理器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Thread-Hierarchy"><span class="toc-text">2.2. Thread Hierarchy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E7%BA%BF%E7%A8%8B%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84"><span class="toc-text">2.2. 线程层次结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-Cooperative-Thread-Arrays"><span class="toc-text">2.2.1. Cooperative Thread Arrays</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-%E5%8D%8F%E4%BD%9C%E7%BA%BF%E7%A8%8B%E6%95%B0%E7%BB%84"><span class="toc-text">2.2.1. 协作线程数组</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-Cluster-of-Cooperative-Thread-Arrays"><span class="toc-text">2.2.2. Cluster of Cooperative Thread Arrays</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-%E5%8D%8F%E4%BD%9C%E7%BA%BF%E7%A8%8B%E6%95%B0%E7%BB%84%E9%9B%86%E7%BE%A4"><span class="toc-text">2.2.2. 协作线程数组集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-3-Grid-of-Clusters"><span class="toc-text">2.2.3. Grid of Clusters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-3-%E9%9B%86%E7%BE%A4%E7%BD%91%E6%A0%BC"><span class="toc-text">2.2.3. 集群网格</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-Memory-Hierarchy"><span class="toc-text">2.3. Memory Hierarchy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E5%86%85%E5%AD%98%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84"><span class="toc-text">2.3. 内存层次结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-A-Set-of-SIMT-Multiprocessors"><span class="toc-text">3.1. A Set of SIMT Multiprocessors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E4%B8%80%E7%BB%84SIMT%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8"><span class="toc-text">3.1. 一组SIMT多处理器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E5%B0%BE"><span class="toc-text">结尾</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background: transparent;"><div id="footer-wrap"><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }
      
      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>